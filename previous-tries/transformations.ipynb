{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfaf8685-6d92-4675-bc09-057ff134893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from config import *\n",
    "\n",
    "import os\n",
    "# os.putenv(\"AWS_REGION\",\"us-east-1\") # this does not work\n",
    "os.environ[\"AWS_REGION\"] = \"us-east-1\"\n",
    "import logging\n",
    "\n",
    "# Set environment variable for log4j configuration file\n",
    "os.environ['SPARK_CONF_DIR'] = 'log4j'\n",
    "\n",
    "# Ensure log4j configuration is loaded\n",
    "os.environ['SPARK_SUBMIT_OPTS'] = \"spark.driver.extraJavaOptions=-Dlog4j.configuration=file://log4j.properties\"\n",
    "\n",
    "# logging.basicConfig(level=logging.ERROR, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "# logger = logging.getLogger(\"Connectwise Lakehouse\")\n",
    "\n",
    "# logger.warning(\"starting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99641a37-8c21-4795-80e8-97671a2c3d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: Could not find or load main class spark.driver.extraJavaOptions=-Dlog4j.configuration=file:..log4j.properties\n",
      "Caused by: java.lang.ClassNotFoundException: spark/driver/extraJavaOptions=-Dlog4j/configuration=file://log4j/properties\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m\n\u001b[1;32m      1\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m  \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars.packages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark:spark-sql_2.12:3.5.1,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.hadoop:hadoop-aws:3.3.4,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.hadoop:hadoop-client-runtime:3.3.4,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msoftware.amazon.awssdk:bundle:2.23.19,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;66;43;03m#    // Iceberg Catalog Configuration\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.extensions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use Iceberg with Spark\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcatalog_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.iceberg.spark.SparkCatalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcatalog_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcatalog_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.uri\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrest_endpoint\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcatalog_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.warehouse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3a://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;66;43;03m#  MinIO-Specific Configurations (for both Iceberg and Hadoop)\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcatalog_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.hadoop.fs.s3a.endpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mminio_endpoint\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcatalog_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.s3.endpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mminio_endpoint\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added for Iceberg\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.hadoop.fs.s3a.endpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mminio_endpoint\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;66;43;03m# MinIO Credentials\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcatalog_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.hadoop.fs.s3a.access.key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccess_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcatalog_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.hadoop.fs.s3a.secret.key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecret_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcatalog_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.s3.access-key-id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccess_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcatalog_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.s3.secret-access-key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecret_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.hadoop.fs.s3a.access.key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccess_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.hadoop.fs.s3a.secret.key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecret_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Additional S3 Configurations\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.hadoop.fs.s3a.path.style.access\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.hadoop.fs.s3a.connection.establish.timeout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m15000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.hadoop.fs.s3a.impl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.hadoop.fs.s3a.S3AFileSystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Ensure Iceberg uses the correct S3 FileIO implementat\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;66;43;03m# ion\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[0;32m---> 36\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/bluebird/jun03/Translation/.venv/lib/python3.11/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/Desktop/bluebird/jun03/Translation/.venv/lib/python3.11/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/Desktop/bluebird/jun03/Translation/.venv/lib/python3.11/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/bluebird/jun03/Translation/.venv/lib/python3.11/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/Desktop/bluebird/jun03/Translation/.venv/lib/python3.11/site-packages/pyspark/java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "spark = (\n",
    " SparkSession.builder\n",
    " .master(\"local\")\n",
    " .config(\"spark.jars.packages\",\n",
    "      \"org.apache.spark:spark-sql_2.12:3.5.1,\"\n",
    "      \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "      \"org.apache.hadoop:hadoop-client-runtime:3.3.4,\"\n",
    "      \"software.amazon.awssdk:bundle:2.23.19,\"\n",
    "      \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,\")\n",
    " #    // Iceberg Catalog Configuration\n",
    " .config(\"spark.sql.extensions\",\n",
    "      \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")  # Use Iceberg with Spark\n",
    " .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    " .config(f\"spark.sql.catalog.{catalog_name}.type\", \"rest\")\n",
    " .config(f\"spark.sql.catalog.{catalog_name}.uri\", f\"http://{rest_endpoint}/\")\n",
    " .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", f\"s3a://{bucket}\")\n",
    " #  MinIO-Specific Configurations (for both Iceberg and Hadoop)\n",
    " .config(f\"spark.sql.catalog.{catalog_name}.hadoop.fs.s3a.endpoint\", f\"http://{minio_endpoint}\")\n",
    " .config(f\"spark.sql.catalog.{catalog_name}.s3.endpoint\", f\"http://{minio_endpoint}\")  # Added for Iceberg\n",
    " .config(f\"spark.hadoop.fs.s3a.endpoint\", f\"http://{minio_endpoint}\")\n",
    " # MinIO Credentials\n",
    " .config(f\"spark.sql.catalog.{catalog_name}.hadoop.fs.s3a.access.key\", access_key)\n",
    " .config(f\"spark.sql.catalog.{catalog_name}.hadoop.fs.s3a.secret.key\", secret_key)\n",
    " .config(f\"spark.sql.catalog.{catalog_name}.s3.access-key-id\", access_key)\n",
    " .config(f\"spark.sql.catalog.{catalog_name}.s3.secret-access-key\", secret_key)\n",
    " .config(\"spark.hadoop.fs.s3a.access.key\", access_key)\n",
    " .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key)\n",
    " # Additional S3 Configurations\n",
    " .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    " .config(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"15000\")\n",
    " .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    " # Ensure Iceberg uses the correct S3 FileIO implementat\n",
    " # ion\n",
    "\n",
    " .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b253677f-6f8f-48a0-961e-78f0ee00198d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n",
      "18:09:53.627 [Thread-2] DEBUG org.apache.spark.sql.execution.SparkSqlParser -- Parsing command: USE rest_catalog\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(spark.version)\n",
    "spark.sql(f\"USE {catalog_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f09e1e1-b82f-4186-803b-266fdbfad7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:09:55.548 [Thread-2] DEBUG org.apache.spark.sql.execution.SparkSqlParser -- Parsing command: \n",
      " SELECT *\n",
      " FROM ncentral.customers \n",
      "--    LEFT JOIN ncentral.devices ON ncentral.customers.customer_id = ncentral.devices.customer_id\n",
      "--    LEFT JOIN ncentral.device_statistics ON ncentral.devices.device_id = ncentral.device_statistics.device_id\n",
      "--    GROUP BY  ncentral.customers.customer_id\n",
      " \n",
      "18:09:55.555 [Thread-2] DEBUG org.apache.iceberg.CachingCatalog -- Evicted ncentral.customers from the table cache (EXPIRED)\n",
      "18:09:55.557 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000042 preparing request execution\n",
      "18:09:55.558 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000042 target auth state: UNCHALLENGED\n",
      "18:09:55.558 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000042 proxy auth state: UNCHALLENGED\n",
      "18:09:55.558 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-0000000042 acquiring connection with route {}->http://10.0.0.146:8181\n",
      "18:09:55.558 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000042 acquiring endpoint (3 MINUTES)\n",
      "18:09:55.558 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000042 endpoint lease request (3 MINUTES) [route: {}->http://10.0.0.146:8181][total available: 1; route allocated: 1 of 100; total allocated: 1 of 100]\n",
      "18:09:55.559 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000042 endpoint leased [route: {}->http://10.0.0.146:8181][total available: 0; route allocated: 1 of 100; total allocated: 1 of 100]\n",
      "18:09:55.559 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-15 << \"end of stream\"\n",
      "18:09:55.559 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000042 connection http-outgoing-15 is stale\n",
      "18:09:55.559 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.DefaultManagedHttpClientConnection -- http-outgoing-15 close connection IMMEDIATE\n",
      "18:09:55.559 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000042 acquired ep-0000000042\n",
      "18:09:55.559 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000042 acquired endpoint ep-0000000042\n",
      "18:09:55.559 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-0000000042 opening connection {}->http://10.0.0.146:8181\n",
      "18:09:55.559 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000042 connecting endpoint (null)\n",
      "18:09:55.559 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000042 connecting endpoint to http://10.0.0.146:8181 (3 MINUTES)\n",
      "18:09:55.559 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.DefaultHttpClientConnectionOperator -- 10.0.0.146 resolving remote address\n",
      "18:09:55.559 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.DefaultHttpClientConnectionOperator -- 10.0.0.146 resolved to [/10.0.0.146]\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.DefaultHttpClientConnectionOperator -- 10.0.0.146:8181 connecting null->/10.0.0.146:8181 (3 MINUTES)\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.DefaultManagedHttpClientConnection -- http-outgoing-16 set socket timeout to 3 MINUTES\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.DefaultHttpClientConnectionOperator -- 10.0.0.146:8181 connected null->/10.0.0.146:8181 as http-outgoing-16\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000042 connected http-outgoing-16\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000042 endpoint connected\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000042 executing GET /v1/namespaces/ncentral/tables/customers?snapshots=all HTTP/1.1\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- ex-0000000042 Cookie spec selected: strict\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000042 start execution ex-0000000042\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000042 executing exchange ex-0000000042 over http-outgoing-16\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-16 >> GET /v1/namespaces/ncentral/tables/customers?snapshots=all HTTP/1.1\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-16 >> Accept: application/json\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-16 >> Content-Type: application/json\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-16 >> Accept-Encoding: gzip, x-gzip, deflate\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-16 >> X-Client-Git-Commit-Short: cbb8530\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-16 >> X-Client-Version: Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-16 >> Host: 10.0.0.146:8181\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-16 >> Connection: keep-alive\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-16 >> User-Agent: Apache-HttpClient/5.3.1 (Java/21.0.3)\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 >> \"GET /v1/namespaces/ncentral/tables/customers?snapshots=all HTTP/1.1[\\r][\\n]\"\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 >> \"Accept: application/json[\\r][\\n]\"\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 >> \"Content-Type: application/json[\\r][\\n]\"\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 >> \"Accept-Encoding: gzip, x-gzip, deflate[\\r][\\n]\"\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 >> \"X-Client-Git-Commit-Short: cbb8530[\\r][\\n]\"\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 >> \"X-Client-Version: Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)[\\r][\\n]\"\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 >> \"Host: 10.0.0.146:8181[\\r][\\n]\"\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 >> \"Connection: keep-alive[\\r][\\n]\"\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 >> \"User-Agent: Apache-HttpClient/5.3.1 (Java/21.0.3)[\\r][\\n]\"\n",
      "18:09:55.560 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 >> \"[\\r][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"HTTP/1.1 200 OK[\\r][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"Date: Wed, 26 Jun 2024 22:09:55 GMT[\\r][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"Content-Type: application/json[\\r][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"Vary: Accept-Encoding, User-Agent[\\r][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"Content-Encoding: gzip[\\r][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"Content-Length: 994[\\r][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"Server: Jetty(9.4.51.v20230217)[\\r][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"[\\r][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"[0x1f][0xffffff8b][0x8][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff9d]W[0xffffffdb][0xffffff92][0xffffffa3]8[0xc][0xfffffffd][0x17][0xffffff9e][0xffffffe3][0x4]s1[0xffffff90][0xffffffef][0xffffffd8][0xffffffa7][0xffffffd9][0xffffffea][0xffffffa2][0x1c]#[0x12]o[0x1b][0xffffffcc][0xffffffd8]&[0xffffffbd]=][0xfffffff9][0xfffffff7][0xffffff95]![0xffffff84]\\z2aS[0xffffffa9]J!YGB:[0xffffff96][0xffffff94][0xffffffaf][0xffffffa0][0x1][0xffffffc7]+[0xffffffee]8QZp'u[0x1b]l[0x3][0x1b][0xfffffff3][0xffffffed]f[0xffffffa3][0xfffffff8];[0x1c]toa[0xffffffd3][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"h[0xffffff9d][0xffffffe1]j#z[0xffffffeb]t[0x3][0xffffffc6]n&[0xffffffb3]M[0xffffffe8]?$[0x13]Y[0xffffffce][0xffffffe3]:![0x15][\\r][0xffffff81]$i[0xffffffc6][0xffffffc8][0xffffff8e][0xffffffd2][0xffffff94]TI[0xffffffb2][0x13][0xffffffb4][0xffffffaa]D[0xffffff9a]$[0xffffffeb][0xffffffc9]f[0xfffffffd][0xffffff8f]E7[0xffffffab][0xffffff8b][0xffffffeb]`[0xfffffffb][0x15][0xffffffd4][0xffffffda]4[0xffffffdc][0xffffff91]#b[0xf]1D[0xffffffab][0xffffffc0][0xfffffff1][0xffffff9d][0x2][0xffffffd2][0xfffffff7][0xffffffb2][0xffffffc2][0xffffff90]DUD[0xffffffc0][0xb]J[0xffffffe2][0xffffff84][0xffffffc6]$[0x11]9[0x10]NYL[0xffffffa2]:bB@[0xffffff95][0xffffff86][0x5]C[0xffffffc8]E/[0xffffffe1][0xffffffcf]s[0xffffffeb][0xffffff88][0xffffff85][0xffffff9f]=[0xffffffa0][0xffffff9a][0xffffffb4]}[0xffffffb3][0x3][0x13]l[0xffffffe9]Y[0xffffffd1]w[0x18][0x1e]T[0xffffffa4][0xffffffb1]([0xffffffcb]h[0xffffff81][0xffffffae]Y[0xffffff91][0xffffffa7][0xffffffd1][0xffffffa4][0x17]Z[0xfffffff5]MK|[0xffffff80]Q[0xffffffb6][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"Do[0xc][0xfffffffa] V[0x1c][0xffffffa0][0xffffffe1][0xffffff83]8\\[0x5][0xffffffe3][0x13]\"[0xfffffffc][0xfffffffd][0x15][0xffffffb8][0xffffffcf][0xe]|h[0xffffffce][0xfffffff4][0xffffffc2][0x5][0xffffff93]n:YKP[0xffffffd5]x[0xffffffd0]K[0xffffffd0]K[0xffffffcb][0x1b][0x7f]^H[0xfffffff7][0xffffff89][0xffffffa7][\\r][0xffffffc6])[\\r][0xffffffa0][0xffffffaa][0xffffffe6][0xffffffca][0xffffffc2][0xffffffea][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"N[0xffffffb6][0xfffffffb][0xffffffe0][0xffffffb4][0x1a][0xffffffed][0xffffffa2][0xffffffd9]N[0xffffffb7][0xffffff8e][0xb]WV[0xffffffd0]q[0xffffffe3][0x1a][0xffffff8c][0xffffffed]u[0xffffff94][0xfffffff8][0x1][0x5]#[0xffffff95][0xffffffea]u[0xffffff80][0xffffffe4][0x11][0xffffffe0][0xffffffdf][0x5][0xfffffffe][0xffffffd3][0x7][0xfffffff3]Z[0x1a][0xffffffeb][0x6][0xffffffd9][0xffffffcb] [0xffffffec][0x1][0xffffffc4][0xffffffd7]m[0x19]F[0xfffffff6][0xffffff80][0xffffffd1][0x1d]t[0xb][0xffffffe5][0xffffff99],/[0xffffffe3][0xffffffe4][0xf]8N:[0xffffffb5] [0xffffff90][0xffffffe2][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"[0xffffffa0]G*/[0xffffffa0][0x4][\\r]g[0xffffffdb]3[0xfffffffb]K[0xffffff94][0xffffffff][0xffffffde]^[0xffffffe9]kk[0xfffffffa]h[0xffffffbd],[0xffffff87]t[0xffffffe6]$[0xffffff92][0x0]L[0xffffffcb][0xffffffd5]s[0xffffffff][0xfffffff7][0xfffffff6][0xfffffff1]w[0xfffffff6][0xffffffd1][0x2][0xffffff80][0xffffff99][0xffffff8d][0xffffffd2][0xffffff96][0x16][0xffffffcc]Q[\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"([0xffffffb5][0xffffffd9]?[0xffffff81][0xffffffd8]i[0xffffffad][0xffffff80][0xffffffb7]3Fz[0xffffff83][0xfffffff1]i[0x1d]4K[0xffffffcc]g2[0xffffffe2]Ej[0x1d][0xffffffb4][0x1c]{N[0xfffffff9]![0xffffffdb]J[0x7f][0xffffff94]Uo[0xffffffc6][0xffffffc6][0xfffffff5]jM[0xffffffb2]gh[0xffffffd6][0xffffffe1]}_[0xffffff90][0xffffff9d][0xffffff99][0xffffff9b][0xffffffd8]([0xffffffb0]M[0xffffff94][0xffffffff][0xffffffb7][0xffffffce][0xffffffc5]=[0xffffffd0][0xffffffeb],[0xffffff8b]f[0xffffff8e][0xe][0x17]lA[0xffffffc3][0xffffff9b][0xfffffff9][0xffffffd9]i|uU[\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"]-[0xffffffb1][0xffffff9f][0xffffffd9][0xffffffd9][0x19][0xffffffbd]7[0xffffffbc]Qp[0xffffff84]e[0xc][0xffffff8d]f[0xffffff86][0x1a][0xffffffd8]K[0xffffffd4][0xe][0xffffffe5],[0xffffff9d]~[0xffffff87]gE[0xffffffbd][0xffffff87]I[0xffffff9e][0xffffffc0]`[0x3][0xffffffed][0xffffffa4][0xfffffff9],[0xfffffffd]PZ[0x0]9[0xffffffd3][0x16]s[0xffffffe3][0xffffffa0][0xffffffc4]w<.0gW[0xffffffe6][0x6][0xffffffc0][0xffffff95]t[0xffffff81]qvo[0xfffffffc][0xffffffca][0xffffff9d]};[0xffffffbd][0xffffffad][0xffffff82][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"j[0xffffffde]+[0x1c][0xffffffa6][0x1d][0xffffff88]i@[0xfffffffa])&}>[0x6][0xffffffe9]8)[0xffffffaf][0xfffffff5][0xffffff97][0x1]: [0xc]#z6[0x19]ZhQ\\!k[0xffffffe3][0xffffff88]6[0x15][0xffffff98][0xffffffcb][0xffffffa8][0xffffffbe]HF[0xffffffec][0x1b][0xffffffe5]-8&[0xffffffb1][0x3][0xffffff84][0x6][0xffffffeb][0xffffffb7][0x17][0xfffffffd][0xffffffd1][0xfffffffa][0xffffffbd]!hd#q[0xffffffa6]+[0xffffffdf][0xffffff9a][0xffffff85][0xffffff81]ay[0xffffffe0][0xe][0x15]Q[0x18]%$d$b[0x7f][0xffffffd1]d[0xffffff9b][0xffffffe2]7[[0xffffffa7],[0xffffffa5][0xffffff94][0x15],[0xfffffff9][0xffffff81][0xffffffa7]?[0xffffff8c]t[0xffffffb0][0xffffffc6]hq[0x15]qk[0xffffffa1][0xffffff9b][0xffffffce][0xffffff80][0xfffffff5][0xffffffab][0x10][0xfffffff1]\\[0x16][0x8][0xfffffff1][0xffffffcb][0xffffffba][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"[0x13];o[0x19]-[0xffffffef][0xffffffec]A[0xffffffbb]![0xffffffbe][0xffffffbc]`4[0xffffff8e][0xffffffb2]0bq[0xffffff96][0x16]9[0xb][0xffffffe3][0xffffffd8][0xffffffe7][0xffffffb9][0x1e][0xffffffa2][0xfffffff3][\\r][0xffffffc2][0xffffffff][0xfffffffe][0xffffffd9]bj][0x6][0xffffff9b][0xffffffc9]!8[0xffffffa1][0xffffffb7][0xffffffc9][0xffffffe6][0xffffff9c][0xffffffec]o6[0xffffffa5][0x17]Pe[0x3]H[0xffffffbd][0xffffffa6][0xfffffffb]n[0xffffff91][0xffffffb2]}[0xffffffd3]p[0xffffff9c]e>[0xffffff8b][0xffffff98][0xffffffd1]i{[0xffffffe3]][0x7][0xffffffad][0xffffffbf][0x7f][0x16]s[0xfffffff2][0xffffffbe][0xffffffc6][0xffffffc7][0xfffffff5][0xffffffb0][0x7][0xfffffffa][0x5]O[0xffffff91][0x9]#[0xffffffcf]Y[0xffffff94][0xffffffe3]!^U[0xffffff98][0xffffffe9]a[0xffffff8d][0xffffffad][0xffffffa5][0xfffffff2]5[0x9][0xffffffe8]El@`![0xffffffbd][0xffffff8c][0xffffff85][0xffffffec]\"[0x1d][0xffffffce][0x11]+[0x7f][0xfffffff9][0x17]NS[0xffffff9a][0x15][0xffffffbe]f[0x7][0xffffffde][0xffffffee]Q9[0xffffffb3]f[0x18][0xffffffb5]g8[0xffffffa7][0xffffffb1][0xffffffb7]<[0xffffffc0][0xffffff8d][0xffffffd2]o[0xffffffe1]F[0xffffffd5]CXg1(ppQ[0xffffff84][0x17][0x5][0xfffffff6][0xffffffb0][0xffffffd1][0xfffffff7]x[0xffffffe2]V[0xffffff89][0x5][0xffffffe0][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"[0xffffffd7][0xffffffc1][0x1b]%[0xffffffd6][0xffffffa9][0xffffffe1][0xffffffad][0xffffffac]1[0xffffffcb]Da[0xffffffd3]X[0xffffffb8][0xffffffc2][0xfffffffb][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \"[0xffffff92]o*G([0xffffffe1],[0xffffffad]9[0xffffffd2][0xffffff94][0xffffffe4]yFI[0xffffffc2]!%E[0x1e][0xffffffa5]$[0xf]c.D[0xffffff92][0xffffffb3][0x18][0xffffffea]5?[0x1a]}[0xffffffb7][0xffffffcc][0xfffffffa]+[0xffffffe2][0xffffffbb][\\r][0x6]#[0xffffff87][0xffffff8b][0xfffffffa]vs{[0xffffffef]4[0x17][\\n]\"\n",
      "18:09:55.752 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-16 << \")[0xffffffbd][0x1f][0xfffffff7][0xffffffe4][0xffffffa7][0xffffff8c][0xfffffff9][0x3][0xffffffe1][0xffffffbc][0xffffffef][0xffffffab][0x7f]5[0xfffffffb][0xffffffe1][0xffffffca][0xffffffae][0xfffffffc][0xffffffd6]UK|[0xfffffff8]:[0xffffff9d][0xfffffffe][0x3]+[0xffffffc0][0xffffff86]Q[0xfffffff6][0xc][0x0][0x0]\"\n",
      "18:09:55.753 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-16 << HTTP/1.1 200 OK\n",
      "18:09:55.753 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-16 << Date: Wed, 26 Jun 2024 22:09:55 GMT\n",
      "18:09:55.753 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-16 << Content-Type: application/json\n",
      "18:09:55.753 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-16 << Vary: Accept-Encoding, User-Agent\n",
      "18:09:55.753 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-16 << Content-Encoding: gzip\n",
      "18:09:55.753 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-16 << Content-Length: 994\n",
      "18:09:55.753 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-16 << Server: Jetty(9.4.51.v20230217)\n",
      "18:09:55.753 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000042 connection can be kept alive for 3 MINUTES\n",
      "18:09:55.753 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000042 releasing valid endpoint\n",
      "18:09:55.753 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000042 releasing endpoint\n",
      "18:09:55.753 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000042 connection http-outgoing-16 can be kept alive for 3 MINUTES\n",
      "18:09:55.753 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000042 connection released [route: {}->http://10.0.0.146:8181][total available: 1; route allocated: 1 of 100; total allocated: 1 of 100]\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "join_query = \\\n",
    " f'''\n",
    " SELECT *\n",
    " FROM {namespace}.customers \n",
    "--    LEFT JOIN {namespace}.devices ON {namespace}.customers.customer_id = {namespace}.devices.customer_id\n",
    "--    LEFT JOIN {namespace}.device_statistics ON {namespace}.devices.device_id = {namespace}.device_statistics.device_id\n",
    "--    GROUP BY  {namespace}.customers.customer_id\n",
    " '''\n",
    "\n",
    "joined = spark.sql(join_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fcbee129-d1a3-495d-b73b-a970b42fa79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:14:16.920 [Thread-2] INFO org.apache.iceberg.SnapshotScan -- Scanning table rest_catalog.ncentral.customers snapshot 8961327026375986033 created at 2024-06-26T14:54:58.521+00:00 with filter true\n",
      "18:14:16.925 [Thread-2] DEBUG org.apache.iceberg.aws.s3.S3InputStream -- Seek with new stream for s3a://lakehouse/ncentral/customers/metadata/a65fa511-8871-4ae5-9825-803acc4863ef-m0.avro to offset 0\n",
      "18:14:16.982 [Thread-2] INFO org.apache.iceberg.metrics.LoggingMetricsReporter -- Received metrics report: ScanReport{tableName=rest_catalog.ncentral.customers, snapshotId=8961327026375986033, filter=true, schemaId=0, projectedFieldIds=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27], projectedFieldNames=[city, contact_department, contact_email, contact_ext, contact_firstname, contact_lastname, contact_phone_number, contact_title, country, customer_id, customer_name, external_id, external_id2, is_service_org, is_system, maintenance_window_duration, maintenance_window_start, parent_customer_name, parent_id, phone, postal_code, programlevel_id, registration_token, registration_token_expiry_date, state_prov, street_1, street_2], scanMetrics=ScanMetricsResult{totalPlanningDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.0607235S, count=1}, resultDataFiles=CounterResult{unit=COUNT, value=1}, resultDeleteFiles=CounterResult{unit=COUNT, value=0}, totalDataManifests=CounterResult{unit=COUNT, value=1}, totalDeleteManifests=CounterResult{unit=COUNT, value=0}, scannedDataManifests=CounterResult{unit=COUNT, value=1}, skippedDataManifests=CounterResult{unit=COUNT, value=0}, totalFileSizeInBytes=CounterResult{unit=BYTES, value=55179}, totalDeleteFileSizeInBytes=CounterResult{unit=BYTES, value=0}, skippedDataFiles=CounterResult{unit=COUNT, value=0}, skippedDeleteFiles=CounterResult{unit=COUNT, value=0}, scannedDeleteManifests=CounterResult{unit=COUNT, value=0}, skippedDeleteManifests=CounterResult{unit=COUNT, value=0}, indexedDeleteFiles=CounterResult{unit=COUNT, value=0}, equalityDeleteFiles=CounterResult{unit=COUNT, value=0}, positionalDeleteFiles=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.1, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82), app-id=local-1719428368459, engine-name=spark}}\n",
      "18:14:16.983 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hccustomers_devices-count:  606\n",
      ".client5.http.impl.classic.InternalHttpClient -- ex-0000000048 preparing request execution\n",
      "18:14:16.984 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000048 target auth state: UNCHALLENGED\n",
      "18:14:16.984 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000048 proxy auth state: UNCHALLENGED\n",
      "18:14:16.984 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-0000000048 acquiring connection with route {}->http://10.0.0.146:8181\n",
      "18:14:16.984 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000048 acquiring endpoint (3 MINUTES)\n",
      "18:14:16.984 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000048 endpoint lease request (3 MINUTES) [route: {}->http://10.0.0.146:8181][total available: 1; route allocated: 1 of 100; total allocated: 1 of 100]\n",
      "18:14:16.984 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000048 endpoint leased [route: {}->http://10.0.0.146:8181][total available: 0; route allocated: 1 of 100; total allocated: 1 of 100]\n",
      "18:14:16.984 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-18 << \"end of stream\"\n",
      "18:14:16.984 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000048 connection http-outgoing-18 is stale\n",
      "18:14:16.984 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.DefaultManagedHttpClientConnection -- http-outgoing-18 close connection IMMEDIATE\n",
      "18:14:16.985 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000048 acquired ep-0000000048\n",
      "18:14:16.985 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000048 acquired endpoint ep-0000000048\n",
      "18:14:16.985 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-0000000048 opening connection {}->http://10.0.0.146:8181\n",
      "18:14:16.985 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000048 connecting endpoint (null)\n",
      "18:14:16.985 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000048 connecting endpoint to http://10.0.0.146:8181 (3 MINUTES)\n",
      "18:14:16.985 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.DefaultHttpClientConnectionOperator -- 10.0.0.146 resolving remote address\n",
      "18:14:16.985 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.DefaultHttpClientConnectionOperator -- 10.0.0.146 resolved to [/10.0.0.146]\n",
      "18:14:16.985 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.DefaultHttpClientConnectionOperator -- 10.0.0.146:8181 connecting null->/10.0.0.146:8181 (3 MINUTES)\n",
      "18:14:16.985 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.DefaultManagedHttpClientConnection -- http-outgoing-19 set socket timeout to 3 MINUTES\n",
      "18:14:16.985 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.DefaultHttpClientConnectionOperator -- 10.0.0.146:8181 connected null->/10.0.0.146:8181 as http-outgoing-19\n",
      "18:14:16.985 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000048 connected http-outgoing-19\n",
      "18:14:16.985 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000048 endpoint connected\n",
      "18:14:16.985 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000048 executing POST /v1/namespaces/ncentral/tables/customers/metrics HTTP/1.1\n",
      "18:14:16.985 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- ex-0000000048 Cookie spec selected: strict\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000048 start execution ex-0000000048\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000048 executing exchange ex-0000000048 over http-outgoing-19\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> POST /v1/namespaces/ncentral/tables/customers/metrics HTTP/1.1\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Accept: application/json\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Content-Type: application/json\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Accept-Encoding: gzip, x-gzip, deflate\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> X-Client-Git-Commit-Short: cbb8530\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> X-Client-Version: Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Content-Length: 1780\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Host: 10.0.0.146:8181\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Connection: keep-alive\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> User-Agent: Apache-HttpClient/5.3.1 (Java/21.0.3)\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"POST /v1/namespaces/ncentral/tables/customers/metrics HTTP/1.1[\\r][\\n]\"\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Accept: application/json[\\r][\\n]\"\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Content-Type: application/json[\\r][\\n]\"\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Accept-Encoding: gzip, x-gzip, deflate[\\r][\\n]\"\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"X-Client-Git-Commit-Short: cbb8530[\\r][\\n]\"\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"X-Client-Version: Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)[\\r][\\n]\"\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Content-Length: 1780[\\r][\\n]\"\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Host: 10.0.0.146:8181[\\r][\\n]\"\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Connection: keep-alive[\\r][\\n]\"\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"User-Agent: Apache-HttpClient/5.3.1 (Java/21.0.3)[\\r][\\n]\"\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"[\\r][\\n]\"\n",
      "18:14:16.986 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"{\"report-type\":\"scan-report\",\"table-name\":\"rest_catalog.ncentral.customers\",\"snapshot-id\":8961327026375986033,\"filter\":true,\"schema-id\":0,\"projected-field-ids\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27],\"projected-field-names\":[\"city\",\"contact_department\",\"contact_email\",\"contact_ext\",\"contact_firstname\",\"contact_lastname\",\"contact_phone_number\",\"contact_title\",\"country\",\"customer_id\",\"customer_name\",\"external_id\",\"external_id2\",\"is_service_org\",\"is_system\",\"maintenance_window_duration\",\"maintenance_window_start\",\"parent_customer_name\",\"parent_id\",\"phone\",\"postal_code\",\"programlevel_id\",\"registration_token\",\"registration_token_expiry_date\",\"state_prov\",\"street_1\",\"street_2\"],\"metrics\":{\"total-planning-duration\":{\"count\":1,\"time-unit\":\"nanoseconds\",\"total-duration\":60723500},\"result-data-files\":{\"unit\":\"count\",\"value\":1},\"result-delete-files\":{\"unit\":\"count\",\"value\":0},\"total-data-manifests\":{\"unit\":\"count\",\"value\":1},\"total-delete-manifests\":{\"unit\":\"count\",\"value\":0},\"scanned-data-manifests\":{\"unit\":\"count\",\"value\":1},\"skipped-data-manifests\":{\"unit\":\"count\",\"value\":0},\"total-file-size-in-bytes\":{\"unit\":\"bytes\",\"value\":55179},\"total-delete-file-size-in-bytes\":{\"unit\":\"bytes\",\"value\":0},\"skipped-data-files\":{\"unit\":\"count\",\"value\":0},\"skipped-delete-files\":{\"unit\":\"count\",\"value\":0},\"scanned-delete-manifests\":{\"unit\":\"count\",\"value\":0},\"skipped-delete-manifests\":{\"unit\":\"count\",\"value\":0},\"indexed-delete-files\":{\"unit\":\"count\",\"value\":0},\"equality-delete-files\":{\"unit\":\"count\",\"value\":0},\"positional-delete-files\":{\"unit\":\"count\",\"value\":0}},\"metadata\":{\"engine-version\":\"3.5.1\",\"iceberg-version\":\"Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)\",\"app-id\":\"local-1719428368459\",\"engine-name\":\"spark\"}}\"\n",
      "18:14:17.029 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"HTTP/1.1 200 OK[\\r][\\n]\"\n",
      "18:14:17.029 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Date: Wed, 26 Jun 2024 22:14:17 GMT[\\r][\\n]\"\n",
      "18:14:17.029 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Content-Type: application/json[\\r][\\n]\"\n",
      "18:14:17.029 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Content-Length: 0[\\r][\\n]\"\n",
      "18:14:17.029 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Server: Jetty(9.4.51.v20230217)[\\r][\\n]\"\n",
      "18:14:17.029 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"[\\r][\\n]\"\n",
      "18:14:17.029 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << HTTP/1.1 200 OK\n",
      "18:14:17.029 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Date: Wed, 26 Jun 2024 22:14:17 GMT\n",
      "18:14:17.029 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Content-Type: application/json\n",
      "18:14:17.029 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Content-Length: 0\n",
      "18:14:17.029 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Server: Jetty(9.4.51.v20230217)\n",
      "18:14:17.029 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000048 connection can be kept alive for 3 MINUTES\n",
      "18:14:17.029 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000048 releasing valid endpoint\n",
      "18:14:17.029 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000048 releasing endpoint\n",
      "18:14:17.029 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000048 connection http-outgoing-19 can be kept alive for 3 MINUTES\n",
      "18:14:17.029 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000048 connection released [route: {}->http://10.0.0.146:8181][total available: 1; route allocated: 1 of 100; total allocated: 1 of 100]\n",
      "18:14:17.029 [Thread-2] INFO org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown -- \n",
      "Pushing operators to rest_catalog.ncentral.customers\n",
      "Pushed Aggregate Functions:\n",
      " COUNT(*)\n",
      "Pushed Group by:\n",
      " \n",
      "         \n",
      "18:14:17.035 [Thread-2] DEBUG org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan -- Adaptive execution enabled for plan: HashAggregate(keys=[], functions=[sum(agg_func_0#2687L)], output=[count#2685L])\n",
      "+- HashAggregate(keys=[], functions=[partial_sum(agg_func_0#2687L)], output=[sum#2690L])\n",
      "   +- Project [count(*)#2688L AS agg_func_0#2687L]\n",
      "      +- LocalTableScan [count(*)#2688L]\n",
      "\n",
      "18:14:17.042 [Thread-2] DEBUG org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec -- Materialize query stage ShuffleQueryStageExec: 0\n",
      "18:14:17.050 [Thread-2] DEBUG org.apache.spark.sql.execution.WholeStageCodegenExec -- \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private long hashAgg_bufValue_0;\n",
      "/* 012 */   private scala.collection.Iterator localtablescan_input_0;\n",
      "/* 013 */   private boolean hashAgg_hashAgg_isNull_2_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_4_0;\n",
      "/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[3];\n",
      "/* 016 */\n",
      "/* 017 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 018 */     this.references = references;\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 022 */     partitionIndex = index;\n",
      "/* 023 */     this.inputs = inputs;\n",
      "/* 024 */\n",
      "/* 025 */     localtablescan_input_0 = inputs[0];\n",
      "/* 026 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 027 */     project_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 028 */     project_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 029 */\n",
      "/* 030 */   }\n",
      "/* 031 */\n",
      "/* 032 */   private void hashAgg_doAggregateWithoutKey_0() throws java.io.IOException {\n",
      "/* 033 */     // initialize aggregation buffer\n",
      "/* 034 */     hashAgg_bufIsNull_0 = true;\n",
      "/* 035 */     hashAgg_bufValue_0 = -1L;\n",
      "/* 036 */\n",
      "/* 037 */     while ( localtablescan_input_0.hasNext()) {\n",
      "/* 038 */       InternalRow localtablescan_row_0 = (InternalRow) localtablescan_input_0.next();\n",
      "/* 039 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 040 */       // common sub-expressions\n",
      "/* 041 */\n",
      "/* 042 */       boolean localtablescan_isNull_0 = localtablescan_row_0.isNullAt(0);\n",
      "/* 043 */       long localtablescan_value_0 = localtablescan_isNull_0 ?\n",
      "/* 044 */       -1L : (localtablescan_row_0.getLong(0));\n",
      "/* 045 */\n",
      "/* 046 */       hashAgg_doConsume_0(localtablescan_value_0, localtablescan_isNull_0);\n",
      "/* 047 */       // shouldStop check is eliminated\n",
      "/* 048 */     }\n",
      "/* 049 */\n",
      "/* 050 */   }\n",
      "/* 051 */\n",
      "/* 052 */   private void hashAgg_doConsume_0(long hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0) throws java.io.IOException {\n",
      "/* 053 */     // do aggregate\n",
      "/* 054 */     // common sub-expressions\n",
      "/* 055 */\n",
      "/* 056 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 057 */     hashAgg_doAggregate_sum_0(hashAgg_exprIsNull_0_0, hashAgg_expr_0_0);\n",
      "/* 058 */\n",
      "/* 059 */   }\n",
      "/* 060 */\n",
      "/* 061 */   private void hashAgg_doAggregate_sum_0(boolean hashAgg_exprIsNull_0_0, long hashAgg_expr_0_0) throws java.io.IOException {\n",
      "/* 062 */     hashAgg_hashAgg_isNull_2_0 = true;\n",
      "/* 063 */     long hashAgg_value_2 = -1L;\n",
      "/* 064 */     do {\n",
      "/* 065 */       boolean hashAgg_isNull_3 = true;\n",
      "/* 066 */       long hashAgg_value_3 = -1L;\n",
      "/* 067 */       hashAgg_hashAgg_isNull_4_0 = true;\n",
      "/* 068 */       long hashAgg_value_4 = -1L;\n",
      "/* 069 */       do {\n",
      "/* 070 */         if (!hashAgg_bufIsNull_0) {\n",
      "/* 071 */           hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 072 */           hashAgg_value_4 = hashAgg_bufValue_0;\n",
      "/* 073 */           continue;\n",
      "/* 074 */         }\n",
      "/* 075 */\n",
      "/* 076 */         if (!false) {\n",
      "/* 077 */           hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 078 */           hashAgg_value_4 = 0L;\n",
      "/* 079 */           continue;\n",
      "/* 080 */         }\n",
      "/* 081 */\n",
      "/* 082 */       } while (false);\n",
      "/* 083 */\n",
      "/* 084 */       if (!hashAgg_exprIsNull_0_0) {\n",
      "/* 085 */         hashAgg_isNull_3 = false; // resultCode could change nullability.\n",
      "/* 086 */\n",
      "/* 087 */         hashAgg_value_3 = hashAgg_value_4 + hashAgg_expr_0_0;\n",
      "/* 088 */\n",
      "/* 089 */       }\n",
      "/* 090 */       if (!hashAgg_isNull_3) {\n",
      "/* 091 */         hashAgg_hashAgg_isNull_2_0 = false;\n",
      "/* 092 */         hashAgg_value_2 = hashAgg_value_3;\n",
      "/* 093 */         continue;\n",
      "/* 094 */       }\n",
      "/* 095 */\n",
      "/* 096 */       if (!hashAgg_bufIsNull_0) {\n",
      "/* 097 */         hashAgg_hashAgg_isNull_2_0 = false;\n",
      "/* 098 */         hashAgg_value_2 = hashAgg_bufValue_0;\n",
      "/* 099 */         continue;\n",
      "/* 100 */       }\n",
      "/* 101 */\n",
      "/* 102 */     } while (false);\n",
      "/* 103 */\n",
      "/* 104 */     hashAgg_bufIsNull_0 = hashAgg_hashAgg_isNull_2_0;\n",
      "/* 105 */     hashAgg_bufValue_0 = hashAgg_value_2;\n",
      "/* 106 */   }\n",
      "/* 107 */\n",
      "/* 108 */   protected void processNext() throws java.io.IOException {\n",
      "/* 109 */     while (!hashAgg_initAgg_0) {\n",
      "/* 110 */       hashAgg_initAgg_0 = true;\n",
      "/* 111 */\n",
      "/* 112 */       long hashAgg_beforeAgg_0 = System.nanoTime();\n",
      "/* 113 */       hashAgg_doAggregateWithoutKey_0();\n",
      "/* 114 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* aggTime */).add((System.nanoTime() - hashAgg_beforeAgg_0) / 1000000);\n",
      "/* 115 */\n",
      "/* 116 */       // output the result\n",
      "/* 117 */\n",
      "/* 118 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);\n",
      "/* 119 */       project_mutableStateArray_0[2].reset();\n",
      "/* 120 */\n",
      "/* 121 */       project_mutableStateArray_0[2].zeroOutNullBytes();\n",
      "/* 122 */\n",
      "/* 123 */       if (hashAgg_bufIsNull_0) {\n",
      "/* 124 */         project_mutableStateArray_0[2].setNullAt(0);\n",
      "/* 125 */       } else {\n",
      "/* 126 */         project_mutableStateArray_0[2].write(0, hashAgg_bufValue_0);\n",
      "/* 127 */       }\n",
      "/* 128 */       append((project_mutableStateArray_0[2].getRow()));\n",
      "/* 129 */     }\n",
      "/* 130 */   }\n",
      "/* 131 */\n",
      "/* 132 */ }\n",
      "\n",
      "18:14:17.051 [Thread-2] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection -- code for input[0, bigint, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     long value_0 = isNull_0 ?\n",
      "/* 033 */     -1L : (i.getLong(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "18:14:17.053 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner -- Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "18:14:17.055 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner --  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "18:14:17.057 [Thread-2] DEBUG org.apache.spark.scheduler.DAGScheduler -- eagerlyComputePartitionsForRddAndAncestors for RDD 203 took 0.000146 seconds\n",
      "18:14:17.057 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- Merging stage rdd profiles: Set()\n",
      "18:14:17.057 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Registering RDD 203 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 28\n",
      "18:14:17.057 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Got map stage job 61 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "18:14:17.057 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Final stage: ShuffleMapStage 89 (count at NativeMethodAccessorImpl.java:0)\n",
      "18:14:17.057 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Parents of final stage: List()\n",
      "18:14:17.057 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Missing parents: List()\n",
      "18:14:17.058 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- submitStage(ShuffleMapStage 89 (name=count at NativeMethodAccessorImpl.java:0;jobs=61))\n",
      "18:14:17.058 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- missing: List()\n",
      "18:14:17.058 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Submitting ShuffleMapStage 89 (MapPartitionsRDD[203] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "18:14:17.058 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- submitMissingTasks(ShuffleMapStage 89)\n",
      "18:14:17.060 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_97 stored as values in memory (estimated size 12.6 KiB, free 434.3 MiB)\n",
      "18:14:17.060 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_97 locally took 0 ms\n",
      "18:14:17.060 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_97 without replication took 0 ms\n",
      "18:14:17.061 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_97_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 434.3 MiB)\n",
      "18:14:17.062 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_97_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:17.062 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Added broadcast_97_piece0 in memory on 10.0.0.146:57286 (size: 6.2 KiB, free: 434.4 MiB)\n",
      "18:14:17.062 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_97_piece0\n",
      "18:14:17.062 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_97_piece0\n",
      "18:14:17.062 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_97_piece0 locally took 0 ms\n",
      "18:14:17.062 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_97_piece0 without replication took 0 ms\n",
      "18:14:17.062 [dag-scheduler-event-loop] INFO org.apache.spark.SparkContext -- Created broadcast 97 from broadcast at DAGScheduler.scala:1585\n",
      "18:14:17.062 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Submitting 1 missing tasks from ShuffleMapStage 89 (MapPartitionsRDD[203] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "18:14:17.062 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl -- Adding task set 89.0 with 1 tasks resource profile 0\n",
      "18:14:17.063 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager -- Epoch for TaskSet 89.0: 28\n",
      "18:14:17.063 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager -- Adding pending tasks took 0 ms\n",
      "18:14:17.063 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager -- Valid locality levels for TaskSet 89.0: NO_PREF, ANY\n",
      "18:14:17.063 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl -- parentName: , name: TaskSet_89.0, runningTasks: 0\n",
      "18:14:17.064 [dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager -- Starting task 0.0 in stage 89.0 (TID 61) (10.0.0.146, executor driver, partition 0, PROCESS_LOCAL, 7776 bytes) \n",
      "18:14:17.064 [Executor task launch worker for task 0.0 in stage 89.0 (TID 61)] INFO org.apache.spark.executor.Executor -- Running task 0.0 in stage 89.0 (TID 61)\n",
      "18:14:17.065 [Executor task launch worker for task 0.0 in stage 89.0 (TID 61)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller -- stageTCMP: (89, 0) -> 1\n",
      "18:14:17.066 [Executor task launch worker for task 0.0 in stage 89.0 (TID 61)] DEBUG org.apache.spark.storage.BlockManager -- Getting local block broadcast_97\n",
      "18:14:17.066 [Executor task launch worker for task 0.0 in stage 89.0 (TID 61)] DEBUG org.apache.spark.storage.BlockManager -- Level for block broadcast_97 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "18:14:17.072 [Executor task launch worker for task 0.0 in stage 89.0 (TID 61)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter -- Writing shuffle index file for mapId 61 with length 1\n",
      "18:14:17.072 [Executor task launch worker for task 0.0 in stage 89.0 (TID 61)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver -- Shuffle index for mapId 61: [59]\n",
      "18:14:17.074 [Executor task launch worker for task 0.0 in stage 89.0 (TID 61)] INFO org.apache.spark.executor.Executor -- Finished task 0.0 in stage 89.0 (TID 61). 1840 bytes result sent to driver\n",
      "18:14:17.074 [Executor task launch worker for task 0.0 in stage 89.0 (TID 61)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller -- stageTCMP: (89, 0) -> 0\n",
      "18:14:17.074 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl -- parentName: , name: TaskSet_89.0, runningTasks: 0\n",
      "18:14:17.074 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager -- No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "18:14:17.075 [task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager -- Finished task 0.0 in stage 89.0 (TID 61) in 12 ms on 10.0.0.146 (executor driver) (1/1)\n",
      "18:14:17.075 [task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl -- Removed TaskSet 89.0, whose tasks have all completed, from pool \n",
      "18:14:17.075 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- ShuffleMapTask finished on driver\n",
      "18:14:17.076 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- ShuffleMapStage 89 (count at NativeMethodAccessorImpl.java:0) finished in 0.018 s\n",
      "18:14:17.076 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- looking for newly runnable stages\n",
      "18:14:17.076 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- running: Set()\n",
      "18:14:17.076 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- waiting: Set()\n",
      "18:14:17.076 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- failed: Set()\n",
      "18:14:17.076 [dag-scheduler-event-loop] DEBUG org.apache.spark.MapOutputTrackerMaster -- Increasing epoch to 29\n",
      "18:14:17.076 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- After removal of stage 89, remaining stages = 0\n",
      "18:14:17.081 [Thread-2] DEBUG org.apache.spark.sql.execution.WholeStageCodegenExec -- \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private long hashAgg_bufValue_0;\n",
      "/* 012 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 013 */   private boolean hashAgg_hashAgg_isNull_3_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_5_0;\n",
      "/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 016 */\n",
      "/* 017 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 018 */     this.references = references;\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 022 */     partitionIndex = index;\n",
      "/* 023 */     this.inputs = inputs;\n",
      "/* 024 */\n",
      "/* 025 */     inputadapter_input_0 = inputs[0];\n",
      "/* 026 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 027 */\n",
      "/* 028 */   }\n",
      "/* 029 */\n",
      "/* 030 */   private void hashAgg_doAggregateWithoutKey_0() throws java.io.IOException {\n",
      "/* 031 */     // initialize aggregation buffer\n",
      "/* 032 */     hashAgg_bufIsNull_0 = true;\n",
      "/* 033 */     hashAgg_bufValue_0 = -1L;\n",
      "/* 034 */\n",
      "/* 035 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 036 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 037 */\n",
      "/* 038 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 039 */       long inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 040 */       -1L : (inputadapter_row_0.getLong(0));\n",
      "/* 041 */\n",
      "/* 042 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0);\n",
      "/* 043 */       // shouldStop check is eliminated\n",
      "/* 044 */     }\n",
      "/* 045 */\n",
      "/* 046 */   }\n",
      "/* 047 */\n",
      "/* 048 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, long hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0) throws java.io.IOException {\n",
      "/* 049 */     // do aggregate\n",
      "/* 050 */     // common sub-expressions\n",
      "/* 051 */\n",
      "/* 052 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 053 */     hashAgg_doAggregate_sum_0(hashAgg_exprIsNull_0_0, hashAgg_expr_0_0);\n",
      "/* 054 */\n",
      "/* 055 */   }\n",
      "/* 056 */\n",
      "/* 057 */   private void hashAgg_doAggregate_sum_0(boolean hashAgg_exprIsNull_0_0, long hashAgg_expr_0_0) throws java.io.IOException {\n",
      "/* 058 */     hashAgg_hashAgg_isNull_3_0 = true;\n",
      "/* 059 */     long hashAgg_value_3 = -1L;\n",
      "/* 060 */     do {\n",
      "/* 061 */       boolean hashAgg_isNull_4 = true;\n",
      "/* 062 */       long hashAgg_value_4 = -1L;\n",
      "/* 063 */       hashAgg_hashAgg_isNull_5_0 = true;\n",
      "/* 064 */       long hashAgg_value_5 = -1L;\n",
      "/* 065 */       do {\n",
      "/* 066 */         if (!hashAgg_bufIsNull_0) {\n",
      "/* 067 */           hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 068 */           hashAgg_value_5 = hashAgg_bufValue_0;\n",
      "/* 069 */           continue;\n",
      "/* 070 */         }\n",
      "/* 071 */\n",
      "/* 072 */         if (!false) {\n",
      "/* 073 */           hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 074 */           hashAgg_value_5 = 0L;\n",
      "/* 075 */           continue;\n",
      "/* 076 */         }\n",
      "/* 077 */\n",
      "/* 078 */       } while (false);\n",
      "/* 079 */\n",
      "/* 080 */       if (!hashAgg_exprIsNull_0_0) {\n",
      "/* 081 */         hashAgg_isNull_4 = false; // resultCode could change nullability.\n",
      "/* 082 */\n",
      "/* 083 */         hashAgg_value_4 = hashAgg_value_5 + hashAgg_expr_0_0;\n",
      "/* 084 */\n",
      "/* 085 */       }\n",
      "/* 086 */       if (!hashAgg_isNull_4) {\n",
      "/* 087 */         hashAgg_hashAgg_isNull_3_0 = false;\n",
      "/* 088 */         hashAgg_value_3 = hashAgg_value_4;\n",
      "/* 089 */         continue;\n",
      "/* 090 */       }\n",
      "/* 091 */\n",
      "/* 092 */       if (!hashAgg_bufIsNull_0) {\n",
      "/* 093 */         hashAgg_hashAgg_isNull_3_0 = false;\n",
      "/* 094 */         hashAgg_value_3 = hashAgg_bufValue_0;\n",
      "/* 095 */         continue;\n",
      "/* 096 */       }\n",
      "/* 097 */\n",
      "/* 098 */     } while (false);\n",
      "/* 099 */\n",
      "/* 100 */     hashAgg_bufIsNull_0 = hashAgg_hashAgg_isNull_3_0;\n",
      "/* 101 */     hashAgg_bufValue_0 = hashAgg_value_3;\n",
      "/* 102 */   }\n",
      "/* 103 */\n",
      "/* 104 */   protected void processNext() throws java.io.IOException {\n",
      "/* 105 */     while (!hashAgg_initAgg_0) {\n",
      "/* 106 */       hashAgg_initAgg_0 = true;\n",
      "/* 107 */\n",
      "/* 108 */       long hashAgg_beforeAgg_0 = System.nanoTime();\n",
      "/* 109 */       hashAgg_doAggregateWithoutKey_0();\n",
      "/* 110 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* aggTime */).add((System.nanoTime() - hashAgg_beforeAgg_0) / 1000000);\n",
      "/* 111 */\n",
      "/* 112 */       // output the result\n",
      "/* 113 */\n",
      "/* 114 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 115 */       hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 116 */\n",
      "/* 117 */       hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 118 */\n",
      "/* 119 */       if (hashAgg_bufIsNull_0) {\n",
      "/* 120 */         hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 121 */       } else {\n",
      "/* 122 */         hashAgg_mutableStateArray_0[0].write(0, hashAgg_bufValue_0);\n",
      "/* 123 */       }\n",
      "/* 124 */       append((hashAgg_mutableStateArray_0[0].getRow()));\n",
      "/* 125 */     }\n",
      "/* 126 */   }\n",
      "/* 127 */\n",
      "/* 128 */ }\n",
      "\n",
      "18:14:17.081 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner -- Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "18:14:17.082 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner --  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "18:14:17.083 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner -- Cleaning indylambda closure: $anonfun$collect$2\n",
      "18:14:17.089 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner --  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++\n",
      "18:14:17.090 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner -- Cleaning indylambda closure: $anonfun$runJob$5\n",
      "18:14:17.094 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner --  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "18:14:17.094 [Thread-2] INFO org.apache.spark.SparkContext -- Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "18:14:17.094 [Thread-2] DEBUG org.apache.spark.scheduler.DAGScheduler -- eagerlyComputePartitionsForRddAndAncestors for RDD 206 took 0.000035 seconds\n",
      "18:14:17.094 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- Merging stage rdd profiles: Set()\n",
      "18:14:17.095 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- Merging stage rdd profiles: Set()\n",
      "18:14:17.095 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Got job 62 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "18:14:17.095 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Final stage: ResultStage 91 (count at NativeMethodAccessorImpl.java:0)\n",
      "18:14:17.095 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Parents of final stage: List(ShuffleMapStage 90)\n",
      "18:14:17.095 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Missing parents: List()\n",
      "18:14:17.095 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- submitStage(ResultStage 91 (name=count at NativeMethodAccessorImpl.java:0;jobs=62))\n",
      "18:14:17.095 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- missing: List()\n",
      "18:14:17.095 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Submitting ResultStage 91 (MapPartitionsRDD[206] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "18:14:17.095 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- submitMissingTasks(ResultStage 91)\n",
      "18:14:17.096 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_98 stored as values in memory (estimated size 13.7 KiB, free 434.2 MiB)\n",
      "18:14:17.096 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_98 locally took 0 ms\n",
      "18:14:17.096 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_98 without replication took 0 ms\n",
      "18:14:17.097 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_98_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.2 MiB)\n",
      "18:14:17.098 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_98_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:17.098 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Added broadcast_98_piece0 in memory on 10.0.0.146:57286 (size: 6.4 KiB, free: 434.4 MiB)\n",
      "18:14:17.098 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_98_piece0\n",
      "18:14:17.098 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_98_piece0\n",
      "18:14:17.098 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_98_piece0 locally took 0 ms\n",
      "18:14:17.098 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_98_piece0 without replication took 0 ms\n",
      "18:14:17.098 [dag-scheduler-event-loop] INFO org.apache.spark.SparkContext -- Created broadcast 98 from broadcast at DAGScheduler.scala:1585\n",
      "18:14:17.098 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Submitting 1 missing tasks from ResultStage 91 (MapPartitionsRDD[206] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "18:14:17.098 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl -- Adding task set 91.0 with 1 tasks resource profile 0\n",
      "18:14:17.098 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager -- Epoch for TaskSet 91.0: 29\n",
      "18:14:17.098 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager -- Adding pending tasks took 0 ms\n",
      "18:14:17.099 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager -- Valid locality levels for TaskSet 91.0: NODE_LOCAL, ANY\n",
      "18:14:17.099 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl -- parentName: , name: TaskSet_91.0, runningTasks: 0\n",
      "18:14:17.099 [dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager -- Starting task 0.0 in stage 91.0 (TID 62) (10.0.0.146, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "18:14:17.100 [Executor task launch worker for task 0.0 in stage 91.0 (TID 62)] INFO org.apache.spark.executor.Executor -- Running task 0.0 in stage 91.0 (TID 62)\n",
      "18:14:17.100 [Executor task launch worker for task 0.0 in stage 91.0 (TID 62)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller -- stageTCMP: (91, 0) -> 1\n",
      "18:14:17.101 [Executor task launch worker for task 0.0 in stage 91.0 (TID 62)] DEBUG org.apache.spark.storage.BlockManager -- Getting local block broadcast_98\n",
      "18:14:17.101 [Executor task launch worker for task 0.0 in stage 91.0 (TID 62)] DEBUG org.apache.spark.storage.BlockManager -- Level for block broadcast_98 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "18:14:17.102 [Executor task launch worker for task 0.0 in stage 91.0 (TID 62)] DEBUG org.apache.spark.MapOutputTrackerMaster -- Fetching outputs for shuffle 28\n",
      "18:14:17.102 [Executor task launch worker for task 0.0 in stage 91.0 (TID 62)] DEBUG org.apache.spark.MapOutputTrackerMaster -- Convert map statuses for shuffle 28, mappers 0-1, partitions 0-1\n",
      "18:14:17.103 [Executor task launch worker for task 0.0 in stage 91.0 (TID 62)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator -- maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647\n",
      "18:14:17.103 [Executor task launch worker for task 0.0 in stage 91.0 (TID 62)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator -- Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "18:14:17.103 [Executor task launch worker for task 0.0 in stage 91.0 (TID 62)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator -- Started 0 remote fetches in 0 ms\n",
      "18:14:17.103 [Executor task launch worker for task 0.0 in stage 91.0 (TID 62)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator -- Start fetching local blocks: (shuffle_28_61_0,0)\n",
      "18:14:17.103 [Executor task launch worker for task 0.0 in stage 91.0 (TID 62)] DEBUG org.apache.spark.storage.BlockManager -- Getting local shuffle block shuffle_28_61_0\n",
      "18:14:17.103 [Executor task launch worker for task 0.0 in stage 91.0 (TID 62)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator -- Got local blocks in 0 ms\n",
      "18:14:17.105 [Executor task launch worker for task 0.0 in stage 91.0 (TID 62)] INFO org.apache.spark.executor.Executor -- Finished task 0.0 in stage 91.0 (TID 62). 3995 bytes result sent to driver\n",
      "18:14:17.105 [Executor task launch worker for task 0.0 in stage 91.0 (TID 62)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller -- stageTCMP: (91, 0) -> 0\n",
      "18:14:17.105 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl -- parentName: , name: TaskSet_91.0, runningTasks: 0\n",
      "18:14:17.105 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager -- No tasks for locality level NODE_LOCAL, so moving to locality level ANY\n",
      "18:14:17.105 [task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager -- Finished task 0.0 in stage 91.0 (TID 62) in 6 ms on 10.0.0.146 (executor driver) (1/1)\n",
      "18:14:17.105 [task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl -- Removed TaskSet 91.0, whose tasks have all completed, from pool \n",
      "18:14:17.106 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- ResultStage 91 (count at NativeMethodAccessorImpl.java:0) finished in 0.011 s\n",
      "18:14:17.106 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- After removal of stage 91, remaining stages = 1\n",
      "18:14:17.106 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- After removal of stage 90, remaining stages = 0\n",
      "18:14:17.106 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Job 62 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "18:14:17.106 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl -- Killing all running tasks in stage 91: Stage finished\n",
      "18:14:17.106 [Thread-2] INFO org.apache.spark.scheduler.DAGScheduler -- Job 62 finished: count at NativeMethodAccessorImpl.java:0, took 0.011676 s\n",
      "18:14:17.107 [Thread-2] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec -- Final plan:\n",
      "*(2) HashAggregate(keys=[], functions=[sum(agg_func_0#2687L)], output=[count#2685L])\n",
      "+- ShuffleQueryStage 0\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=1224]\n",
      "      +- *(1) HashAggregate(keys=[], functions=[partial_sum(agg_func_0#2687L)], output=[sum#2690L])\n",
      "         +- *(1) Project [count(*)#2688L AS agg_func_0#2687L]\n",
      "            +- *(1) LocalTableScan [count(*)#2688L]\n",
      "\n",
      "18:14:25.965 [executor-heartbeater] DEBUG org.apache.spark.executor.ExecutorMetricsPoller -- removing (91, 0) from stageTCMP\n",
      "18:14:25.965 [executor-heartbeater] DEBUG org.apache.spark.executor.ExecutorMetricsPoller -- removing (89, 0) from stageTCMP\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import logging\n",
    "\n",
    "# Suppress all logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# If you want to be specific, you can configure the py4j logger as well\n",
    "logging.getLogger('py4j').setLevel(logging.ERROR)\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "print(\"customers_devices-count: \", joined.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0a9c9ef-48f1-471b-a09c-70b9c75a7a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:14:32.309 [Thread-2] DEBUG org.apache.spark.sql.catalyst.analysis.ResolveReferencesInAggregate -- Resolving 'ncentral.customers.customer_id to customer_id#2267L\n",
      "18:14:32.310 [Thread-2] DEBUG org.apache.spark.sql.catalyst.analysis.ResolveReferencesInAggregate -- Resolving 'ncentral.customers.customer_id to customer_id#2267L\n",
      "18:14:32.336 [Thread-2] INFO org.apache.iceberg.spark.source.SparkScanBuilder -- Skipping aggregate pushdown: group by aggregation push down is not supported\n",
      "18:14:32.340 [Thread-2] INFO org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown -- \n",
      "Output: customer_id#2267L\n",
      "         \n",
      "18:14:32.341 [Thread-2] INFO org.apache.iceberg.SnapshotScan -- Scanning table rest_catalog.ncentral.customers snapshot 8961327026375986033 created at 2024-06-26T14:54:58.521+00:00 with filter true\n",
      "18:14:32.344 [Thread-2] INFO org.apache.iceberg.BaseDistributedDataScan -- Planning file tasks locally for table rest_catalog.ncentral.customers\n",
      "18:14:32.351 [Thread-2] DEBUG org.apache.iceberg.aws.s3.S3InputStream -- Seek with new stream for s3a://lakehouse/ncentral/customers/metadata/a65fa511-8871-4ae5-9825-803acc4863ef-m0.avro to offset 0\n",
      "18:14:32.376 [Thread-2] INFO org.apache.iceberg.metrics.LoggingMetricsReporter -- Received metrics report: ScanReport{tableName=rest_catalog.ncentral.customers, snapshotId=8961327026375986033, filter=true, schemaId=0, projectedFieldIds=[10], projectedFieldNames=[customer_id], scanMetrics=ScanMetricsResult{totalPlanningDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.033980958S, count=1}, resultDataFiles=CounterResult{unit=COUNT, value=1}, resultDeleteFiles=CounterResult{unit=COUNT, value=0}, totalDataManifests=CounterResult{unit=COUNT, value=1}, totalDeleteManifests=CounterResult{unit=COUNT, value=0}, scannedDataManifests=CounterResult{unit=COUNT, value=1}, skippedDataManifests=CounterResult{unit=COUNT, value=0}, totalFileSizeInBytes=CounterResult{unit=BYTES, value=55179}, totalDeleteFileSizeInBytes=CounterResult{unit=BYTES, value=0}, skippedDataFiles=CounterResult{unit=COUNT, value=0}, skippedDeleteFiles=CounterResult{unit=COUNT, value=0}, scannedDeleteManifests=CounterResult{unit=COUNT, value=0}, skippedDeleteManifests=CounterResult{unit=COUNT, value=0}, indexedDeleteFiles=CounterResult{unit=COUNT, value=0}, equalityDeleteFiles=CounterResult{unit=COUNT, value=0}, positionalDeleteFiles=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.1, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82), app-id=local-1719428368459, engine-name=spark}}\n",
      "18:14:32.377 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000049 preparing request execution\n",
      "18:14:32.377 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000049 target auth state: UNCHALLENGED\n",
      "18:14:32.377 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000049 proxy auth state: UNCHALLENGED\n",
      "18:14:32.377 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-0000000049 acquiring connection with route {}->http://10.0.0.146:8181\n",
      "18:14:32.377 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000049 acquiring endpoint (3 MINUTES)\n",
      "18:14:32.378 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000049 endpoint lease request (3 MINUTES) [route: {}->http://10.0.0.146:8181][total available: 1; route allocated: 1 of 100; total allocated: 1 of 100]\n",
      "18:14:32.378 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000049 endpoint leased [route: {}->http://10.0.0.146:8181][total available: 0; route allocated: 1 of 100; total allocated: 1 of 100]\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"[read] I/O error: Read timed out\"\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000049 acquired ep-0000000049\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000049 acquired endpoint ep-0000000049\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000049 executing POST /v1/namespaces/ncentral/tables/customers/metrics HTTP/1.1\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- ex-0000000049 Cookie spec selected: strict\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000049 start execution ex-0000000049\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000049 executing exchange ex-0000000049 over http-outgoing-19\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> POST /v1/namespaces/ncentral/tables/customers/metrics HTTP/1.1\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Accept: application/json\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Content-Type: application/json\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Accept-Encoding: gzip, x-gzip, deflate\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> X-Client-Git-Commit-Short: cbb8530\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> X-Client-Version: Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Content-Length: 1270\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Host: 10.0.0.146:8181\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Connection: keep-alive\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> User-Agent: Apache-HttpClient/5.3.1 (Java/21.0.3)\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"POST /v1/namespaces/ncentral/tables/customers/metrics HTTP/1.1[\\r][\\n]\"\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Accept: application/json[\\r][\\n]\"\n",
      "18:14:32.379 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Content-Type: application/json[\\r][\\n]\"\n",
      "18:14:32.380 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Accept-Encoding: gzip, x-gzip, deflate[\\r][\\n]\"\n",
      "18:14:32.380 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"X-Client-Git-Commit-Short: cbb8530[\\r][\\n]\"\n",
      "18:14:32.380 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"X-Client-Version: Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)[\\r][\\n]\"\n",
      "18:14:32.380 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Content-Length: 1270[\\r][\\n]\"\n",
      "18:14:32.380 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Host: 10.0.0.146:8181[\\r][\\n]\"\n",
      "18:14:32.380 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Connection: keep-alive[\\r][\\n]\"\n",
      "18:14:32.380 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"User-Agent: Apache-HttpClient/5.3.1 (Java/21.0.3)[\\r][\\n]\"\n",
      "18:14:32.380 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"[\\r][\\n]\"\n",
      "18:14:32.380 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"{\"report-type\":\"scan-report\",\"table-name\":\"rest_catalog.ncentral.customers\",\"snapshot-id\":8961327026375986033,\"filter\":true,\"schema-id\":0,\"projected-field-ids\":[10],\"projected-field-names\":[\"customer_id\"],\"metrics\":{\"total-planning-duration\":{\"count\":1,\"time-unit\":\"nanoseconds\",\"total-duration\":33980958},\"result-data-files\":{\"unit\":\"count\",\"value\":1},\"result-delete-files\":{\"unit\":\"count\",\"value\":0},\"total-data-manifests\":{\"unit\":\"count\",\"value\":1},\"total-delete-manifests\":{\"unit\":\"count\",\"value\":0},\"scanned-data-manifests\":{\"unit\":\"count\",\"value\":1},\"skipped-data-manifests\":{\"unit\":\"count\",\"value\":0},\"total-file-size-in-bytes\":{\"unit\":\"bytes\",\"value\":55179},\"total-delete-file-size-in-bytes\":{\"unit\":\"bytes\",\"value\":0},\"skipped-data-files\":{\"unit\":\"count\",\"value\":0},\"skipped-delete-files\":{\"unit\":\"count\",\"value\":0},\"scanned-delete-manifests\":{\"unit\":\"count\",\"value\":0},\"skipped-delete-manifests\":{\"unit\":\"count\",\"value\":0},\"indexed-delete-files\":{\"unit\":\"count\",\"value\":0},\"equality-delete-files\":{\"unit\":\"count\",\"value\":0},\"positional-delete-files\":{\"unit\":\"count\",\"value\":0}},\"metadata\":{\"engine-version\":\"3.5.1\",\"iceberg-version\":\"Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)\",\"app-id\":\"local-1719428368459\",\"engine-name\":\"spark\"}}\"\n",
      "18:14:32.411 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"HTTP/1.1 200 OK[\\r][\\n]\"\n",
      "18:14:32.411 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Date: Wed, 26 Jun 2024 22:14:32 GMT[\\r][\\n]\"\n",
      "18:14:32.411 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Content-Type: application/json[\\r][\\n]\"\n",
      "18:14:32.411 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Content-Length: 0[\\r][\\n]\"\n",
      "18:14:32.411 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Server: Jetty(9.4.51.v20230217)[\\r][\\n]\"\n",
      "18:14:32.411 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"[\\r][\\n]\"\n",
      "18:14:32.412 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << HTTP/1.1 200 OK\n",
      "18:14:32.412 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Date: Wed, 26 Jun 2024 22:14:32 GMT\n",
      "18:14:32.412 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Content-Type: application/json\n",
      "18:14:32.412 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Content-Length: 0\n",
      "18:14:32.412 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Server: Jetty(9.4.51.v20230217)\n",
      "18:14:32.412 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000049 connection can be kept alive for 3 MINUTES\n",
      "18:14:32.412 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000049 releasing valid endpoint\n",
      "18:14:32.412 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000049 releasing endpoint\n",
      "18:14:32.412 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000049 connection http-outgoing-19 can be kept alive for 3 MINUTES\n",
      "18:14:32.412 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000049 connection released [route: {}->http://10.0.0.146:8181][total available: 1; route allocated: 1 of 100; total allocated: 1 of 100]\n",
      "18:14:32.412 [Thread-2] DEBUG org.apache.iceberg.spark.source.SparkPartitioningAwareScan -- Planned 1 task group(s) without data grouping for table rest_catalog.ncentral.customers\n",
      "18:14:32.412 [Thread-2] INFO org.apache.iceberg.spark.source.SparkPartitioningAwareScan -- Reporting UnknownPartitioning with 1 partition(s) for table rest_catalog.ncentral.customers\n",
      "18:14:32.417 [Thread-2] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_99 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)\n",
      "18:14:32.417 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_99 locally took 0 ms\n",
      "18:14:32.417 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_99 without replication took 0 ms\n",
      "18:14:32.419 [Thread-2] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_99_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.2 MiB)\n",
      "18:14:32.420 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_99_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:32.420 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Added broadcast_99_piece0 in memory on 10.0.0.146:57286 (size: 3.8 KiB, free: 434.4 MiB)\n",
      "18:14:32.420 [Thread-2] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_99_piece0\n",
      "18:14:32.420 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_99_piece0\n",
      "18:14:32.420 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_99_piece0 locally took 1 ms\n",
      "18:14:32.420 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_99_piece0 without replication took 1 ms\n",
      "18:14:32.421 [Thread-2] INFO org.apache.spark.SparkContext -- Created broadcast 99 from broadcast at SparkBatch.java:79\n",
      "18:14:32.422 [Thread-2] DEBUG org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan -- Adaptive execution enabled for plan: CollectLimit 11\n",
      "+- HashAggregate(keys=[customer_id#2267L], functions=[count(1)], output=[toprettystring(customer_id)#2725, toprettystring(count)#2726])\n",
      "   +- HashAggregate(keys=[customer_id#2267L], functions=[partial_count(1)], output=[customer_id#2267L, count#2731L])\n",
      "      +- BatchScan rest_catalog.ncentral.customers[customer_id#2267L] rest_catalog.ncentral.customers (branch=null) [filters=, groupedBy=] RuntimeFilters: []\n",
      "\n",
      "18:14:32.428 [Thread-2] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_100 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)\n",
      "18:14:32.429 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_100 locally took 0 ms\n",
      "18:14:32.429 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_100 without replication took 0 ms\n",
      "18:14:32.430 [Thread-2] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_100_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.2 MiB)\n",
      "18:14:32.430 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_100_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:32.430 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Added broadcast_100_piece0 in memory on 10.0.0.146:57286 (size: 3.8 KiB, free: 434.4 MiB)\n",
      "18:14:32.430 [Thread-2] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_100_piece0\n",
      "18:14:32.430 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_100_piece0\n",
      "18:14:32.430 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_100_piece0 locally took 0 ms\n",
      "18:14:32.430 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_100_piece0 without replication took 0 ms\n",
      "18:14:32.431 [Thread-2] INFO org.apache.spark.SparkContext -- Created broadcast 100 from broadcast at SparkBatch.java:79\n",
      "18:14:32.432 [Thread-2] DEBUG org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec -- Materialize query stage ShuffleQueryStageExec: 0\n",
      "18:14:32.437 [Thread-2] DEBUG org.apache.spark.sql.execution.WholeStageCodegenExec -- \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private long hashAgg_bufValue_0;\n",
      "/* 012 */   private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> hashAgg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 017 */   private int columnartorow_batchIdx_0;\n",
      "/* 018 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] columnartorow_mutableStateArray_3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[4];\n",
      "/* 019 */   private org.apache.spark.sql.vectorized.ColumnarBatch[] columnartorow_mutableStateArray_1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];\n",
      "/* 020 */   private scala.collection.Iterator[] columnartorow_mutableStateArray_0 = new scala.collection.Iterator[1];\n",
      "/* 021 */   private org.apache.spark.sql.vectorized.ColumnVector[] columnartorow_mutableStateArray_2 = new org.apache.spark.sql.vectorized.ColumnVector[1];\n",
      "/* 022 */\n",
      "/* 023 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 024 */     this.references = references;\n",
      "/* 025 */   }\n",
      "/* 026 */\n",
      "/* 027 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 028 */     partitionIndex = index;\n",
      "/* 029 */     this.inputs = inputs;\n",
      "/* 030 */\n",
      "/* 031 */     columnartorow_mutableStateArray_0[0] = inputs[0];\n",
      "/* 032 */     columnartorow_mutableStateArray_3[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 033 */     columnartorow_mutableStateArray_3[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 034 */     columnartorow_mutableStateArray_3[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 035 */     columnartorow_mutableStateArray_3[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 036 */\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */   public class hashAgg_FastHashMap_0 {\n",
      "/* 040 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 041 */     private int[] buckets;\n",
      "/* 042 */     private int capacity = 1 << 16;\n",
      "/* 043 */     private double loadFactor = 0.5;\n",
      "/* 044 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 045 */     private int maxSteps = 2;\n",
      "/* 046 */     private int numRows = 0;\n",
      "/* 047 */     private Object emptyVBase;\n",
      "/* 048 */     private long emptyVOff;\n",
      "/* 049 */     private int emptyVLen;\n",
      "/* 050 */     private boolean isBatchFull = false;\n",
      "/* 051 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 052 */\n",
      "/* 053 */     public hashAgg_FastHashMap_0(\n",
      "/* 054 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 055 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 056 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 057 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 058 */\n",
      "/* 059 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 060 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 061 */\n",
      "/* 062 */       emptyVBase = emptyBuffer;\n",
      "/* 063 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 064 */       emptyVLen = emptyBuffer.length;\n",
      "/* 065 */\n",
      "/* 066 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 067 */         1, 0);\n",
      "/* 068 */\n",
      "/* 069 */       buckets = new int[numBuckets];\n",
      "/* 070 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 071 */     }\n",
      "/* 072 */\n",
      "/* 073 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(long hashAgg_key_0) {\n",
      "/* 074 */       long h = hash(hashAgg_key_0);\n",
      "/* 075 */       int step = 0;\n",
      "/* 076 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 077 */       while (step < maxSteps) {\n",
      "/* 078 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 079 */         if (buckets[idx] == -1) {\n",
      "/* 080 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 081 */             agg_rowWriter.reset();\n",
      "/* 082 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 083 */             agg_rowWriter.write(0, hashAgg_key_0);\n",
      "/* 084 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 085 */             = agg_rowWriter.getRow();\n",
      "/* 086 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 087 */             long koff = agg_result.getBaseOffset();\n",
      "/* 088 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 089 */\n",
      "/* 090 */             UnsafeRow vRow\n",
      "/* 091 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 092 */             if (vRow == null) {\n",
      "/* 093 */               isBatchFull = true;\n",
      "/* 094 */             } else {\n",
      "/* 095 */               buckets[idx] = numRows++;\n",
      "/* 096 */             }\n",
      "/* 097 */             return vRow;\n",
      "/* 098 */           } else {\n",
      "/* 099 */             // No more space\n",
      "/* 100 */             return null;\n",
      "/* 101 */           }\n",
      "/* 102 */         } else if (equals(idx, hashAgg_key_0)) {\n",
      "/* 103 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 104 */         }\n",
      "/* 105 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 106 */         step++;\n",
      "/* 107 */       }\n",
      "/* 108 */       // Didn't find it\n",
      "/* 109 */       return null;\n",
      "/* 110 */     }\n",
      "/* 111 */\n",
      "/* 112 */     private boolean equals(int idx, long hashAgg_key_0) {\n",
      "/* 113 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 114 */       return (row.getLong(0) == hashAgg_key_0);\n",
      "/* 115 */     }\n",
      "/* 116 */\n",
      "/* 117 */     private long hash(long hashAgg_key_0) {\n",
      "/* 118 */       long hashAgg_hash_0 = 0;\n",
      "/* 119 */\n",
      "/* 120 */       long hashAgg_result_0 = hashAgg_key_0;\n",
      "/* 121 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 122 */\n",
      "/* 123 */       return hashAgg_hash_0;\n",
      "/* 124 */     }\n",
      "/* 125 */\n",
      "/* 126 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 127 */       return batch.rowIterator();\n",
      "/* 128 */     }\n",
      "/* 129 */\n",
      "/* 130 */     public void close() {\n",
      "/* 131 */       batch.close();\n",
      "/* 132 */     }\n",
      "/* 133 */\n",
      "/* 134 */   }\n",
      "/* 135 */\n",
      "/* 136 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 137 */     if (columnartorow_mutableStateArray_1[0] == null) {\n",
      "/* 138 */       columnartorow_nextBatch_0();\n",
      "/* 139 */     }\n",
      "/* 140 */     while ( columnartorow_mutableStateArray_1[0] != null) {\n",
      "/* 141 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();\n",
      "/* 142 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;\n",
      "/* 143 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {\n",
      "/* 144 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;\n",
      "/* 145 */         boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 146 */         long columnartorow_value_0 = columnartorow_isNull_0 ? -1L : (columnartorow_mutableStateArray_2[0].getLong(columnartorow_rowIdx_0));\n",
      "/* 147 */\n",
      "/* 148 */         hashAgg_doConsume_0(columnartorow_value_0, columnartorow_isNull_0);\n",
      "/* 149 */         // shouldStop check is eliminated\n",
      "/* 150 */       }\n",
      "/* 151 */       columnartorow_batchIdx_0 = columnartorow_numRows_0;\n",
      "/* 152 */       columnartorow_mutableStateArray_1[0] = null;\n",
      "/* 153 */       columnartorow_nextBatch_0();\n",
      "/* 154 */     }\n",
      "/* 155 */\n",
      "/* 156 */     hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator();\n",
      "/* 157 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 158 */\n",
      "/* 159 */   }\n",
      "/* 160 */\n",
      "/* 161 */   private void hashAgg_doConsume_0(long hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0) throws java.io.IOException {\n",
      "/* 162 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 163 */     UnsafeRow hashAgg_fastAggBuffer_0 = null;\n",
      "/* 164 */\n",
      "/* 165 */     if (!hashAgg_exprIsNull_0_0) {\n",
      "/* 166 */       hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert(\n",
      "/* 167 */         hashAgg_expr_0_0);\n",
      "/* 168 */     }\n",
      "/* 169 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 170 */     if (hashAgg_fastAggBuffer_0 == null) {\n",
      "/* 171 */       // generate grouping key\n",
      "/* 172 */       columnartorow_mutableStateArray_3[2].reset();\n",
      "/* 173 */\n",
      "/* 174 */       columnartorow_mutableStateArray_3[2].zeroOutNullBytes();\n",
      "/* 175 */\n",
      "/* 176 */       if (hashAgg_exprIsNull_0_0) {\n",
      "/* 177 */         columnartorow_mutableStateArray_3[2].setNullAt(0);\n",
      "/* 178 */       } else {\n",
      "/* 179 */         columnartorow_mutableStateArray_3[2].write(0, hashAgg_expr_0_0);\n",
      "/* 180 */       }\n",
      "/* 181 */       int hashAgg_unsafeRowKeyHash_0 = (columnartorow_mutableStateArray_3[2].getRow()).hashCode();\n",
      "/* 182 */       if (true) {\n",
      "/* 183 */         // try to get the buffer from hash map\n",
      "/* 184 */         hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 185 */         hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((columnartorow_mutableStateArray_3[2].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 186 */       }\n",
      "/* 187 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 188 */       // aggregation after processing all input rows.\n",
      "/* 189 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 190 */         if (hashAgg_sorter_0 == null) {\n",
      "/* 191 */           hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 192 */         } else {\n",
      "/* 193 */           hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 194 */         }\n",
      "/* 195 */\n",
      "/* 196 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 197 */         // try to allocate buffer again.\n",
      "/* 198 */         hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 199 */           (columnartorow_mutableStateArray_3[2].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 200 */         if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 201 */           // failed to allocate the first page\n",
      "/* 202 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 203 */         }\n",
      "/* 204 */       }\n",
      "/* 205 */\n",
      "/* 206 */     }\n",
      "/* 207 */\n",
      "/* 208 */     // Updates the proper row buffer\n",
      "/* 209 */     if (hashAgg_fastAggBuffer_0 != null) {\n",
      "/* 210 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0;\n",
      "/* 211 */     }\n",
      "/* 212 */\n",
      "/* 213 */     // common sub-expressions\n",
      "/* 214 */\n",
      "/* 215 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 216 */\n",
      "/* 217 */     long hashAgg_value_6 = hashAgg_unsafeRowAggBuffer_0.getLong(0);\n",
      "/* 218 */\n",
      "/* 219 */     long hashAgg_value_5 = -1L;\n",
      "/* 220 */\n",
      "/* 221 */     hashAgg_value_5 = hashAgg_value_6 + 1L;\n",
      "/* 222 */\n",
      "/* 223 */     hashAgg_unsafeRowAggBuffer_0.setLong(0, hashAgg_value_5);\n",
      "/* 224 */\n",
      "/* 225 */   }\n",
      "/* 226 */\n",
      "/* 227 */   private void columnartorow_nextBatch_0() throws java.io.IOException {\n",
      "/* 228 */     if (columnartorow_mutableStateArray_0[0].hasNext()) {\n",
      "/* 229 */       columnartorow_mutableStateArray_1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)columnartorow_mutableStateArray_0[0].next();\n",
      "/* 230 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[8] /* numInputBatches */).add(1);\n",
      "/* 231 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* numOutputRows */).add(columnartorow_mutableStateArray_1[0].numRows());\n",
      "/* 232 */       columnartorow_batchIdx_0 = 0;\n",
      "/* 233 */       columnartorow_mutableStateArray_2[0] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(0);\n",
      "/* 234 */\n",
      "/* 235 */     }\n",
      "/* 236 */   }\n",
      "/* 237 */\n",
      "/* 238 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 239 */   throws java.io.IOException {\n",
      "/* 240 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[9] /* numOutputRows */).add(1);\n",
      "/* 241 */\n",
      "/* 242 */     boolean hashAgg_isNull_7 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 243 */     long hashAgg_value_8 = hashAgg_isNull_7 ?\n",
      "/* 244 */     -1L : (hashAgg_keyTerm_0.getLong(0));\n",
      "/* 245 */     long hashAgg_value_9 = hashAgg_bufferTerm_0.getLong(0);\n",
      "/* 246 */\n",
      "/* 247 */     columnartorow_mutableStateArray_3[3].reset();\n",
      "/* 248 */\n",
      "/* 249 */     columnartorow_mutableStateArray_3[3].zeroOutNullBytes();\n",
      "/* 250 */\n",
      "/* 251 */     if (hashAgg_isNull_7) {\n",
      "/* 252 */       columnartorow_mutableStateArray_3[3].setNullAt(0);\n",
      "/* 253 */     } else {\n",
      "/* 254 */       columnartorow_mutableStateArray_3[3].write(0, hashAgg_value_8);\n",
      "/* 255 */     }\n",
      "/* 256 */\n",
      "/* 257 */     columnartorow_mutableStateArray_3[3].write(1, hashAgg_value_9);\n",
      "/* 258 */     append((columnartorow_mutableStateArray_3[3].getRow()));\n",
      "/* 259 */\n",
      "/* 260 */   }\n",
      "/* 261 */\n",
      "/* 262 */   protected void processNext() throws java.io.IOException {\n",
      "/* 263 */     if (!hashAgg_initAgg_0) {\n",
      "/* 264 */       hashAgg_initAgg_0 = true;\n",
      "/* 265 */       hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 266 */\n",
      "/* 267 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 268 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 269 */           @Override\n",
      "/* 270 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 271 */             hashAgg_fastHashMap_0.close();\n",
      "/* 272 */           }\n",
      "/* 273 */         });\n",
      "/* 274 */\n",
      "/* 275 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 276 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 277 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 278 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[10] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 279 */     }\n",
      "/* 280 */     // output the result\n",
      "/* 281 */\n",
      "/* 282 */     while ( hashAgg_fastHashMapIter_0.next()) {\n",
      "/* 283 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey();\n",
      "/* 284 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue();\n",
      "/* 285 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 286 */\n",
      "/* 287 */       if (shouldStop()) return;\n",
      "/* 288 */     }\n",
      "/* 289 */     hashAgg_fastHashMap_0.close();\n",
      "/* 290 */\n",
      "/* 291 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 292 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 293 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 294 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 295 */       if (shouldStop()) return;\n",
      "/* 296 */     }\n",
      "/* 297 */     hashAgg_mapIter_0.close();\n",
      "/* 298 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 299 */       hashAgg_hashMap_0.free();\n",
      "/* 300 */     }\n",
      "/* 301 */   }\n",
      "/* 302 */\n",
      "/* 303 */ }\n",
      "\n",
      "18:14:32.438 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner -- Cleaning indylambda closure: $anonfun$doExecuteColumnar$1\n",
      "18:14:32.440 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner --  +++ indylambda closure ($anonfun$doExecuteColumnar$1) is now cleaned +++\n",
      "18:14:32.441 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner -- Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "18:14:32.443 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner --  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "18:14:32.447 [Thread-2] DEBUG org.apache.spark.scheduler.DAGScheduler -- eagerlyComputePartitionsForRddAndAncestors for RDD 210 took 0.000148 seconds\n",
      "18:14:32.447 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- Merging stage rdd profiles: Set()\n",
      "18:14:32.448 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Registering RDD 210 (showString at <unknown>:0) as input to shuffle 29\n",
      "18:14:32.449 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Got map stage job 63 (showString at <unknown>:0) with 1 output partitions\n",
      "18:14:32.449 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Final stage: ShuffleMapStage 92 (showString at <unknown>:0)\n",
      "18:14:32.449 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Parents of final stage: List()\n",
      "18:14:32.449 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Missing parents: List()\n",
      "18:14:32.450 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- submitStage(ShuffleMapStage 92 (name=showString at <unknown>:0;jobs=63))\n",
      "18:14:32.450 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- missing: List()\n",
      "18:14:32.450 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Submitting ShuffleMapStage 92 (MapPartitionsRDD[210] at showString at <unknown>:0), which has no missing parents\n",
      "18:14:32.450 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- submitMissingTasks(ShuffleMapStage 92)\n",
      "18:14:32.454 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_101 stored as values in memory (estimated size 38.3 KiB, free 434.1 MiB)\n",
      "18:14:32.454 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_101 locally took 0 ms\n",
      "18:14:32.454 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_101 without replication took 0 ms\n",
      "18:14:32.455 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_101_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 434.1 MiB)\n",
      "18:14:32.455 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_101_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:32.455 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Added broadcast_101_piece0 in memory on 10.0.0.146:57286 (size: 17.0 KiB, free: 434.3 MiB)\n",
      "18:14:32.455 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_101_piece0\n",
      "18:14:32.455 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_101_piece0\n",
      "18:14:32.455 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_101_piece0 locally took 0 ms\n",
      "18:14:32.455 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_101_piece0 without replication took 0 ms\n",
      "18:14:32.455 [dag-scheduler-event-loop] INFO org.apache.spark.SparkContext -- Created broadcast 101 from broadcast at DAGScheduler.scala:1585\n",
      "18:14:32.456 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Submitting 1 missing tasks from ShuffleMapStage 92 (MapPartitionsRDD[210] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "18:14:32.456 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl -- Adding task set 92.0 with 1 tasks resource profile 0\n",
      "18:14:32.456 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager -- Epoch for TaskSet 92.0: 29\n",
      "18:14:32.456 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager -- Adding pending tasks took 0 ms\n",
      "18:14:32.456 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager -- Valid locality levels for TaskSet 92.0: NO_PREF, ANY\n",
      "18:14:32.457 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl -- parentName: , name: TaskSet_92.0, runningTasks: 0\n",
      "18:14:32.457 [dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager -- Starting task 0.0 in stage 92.0 (TID 63) (10.0.0.146, executor driver, partition 0, PROCESS_LOCAL, 13375 bytes) \n",
      "18:14:32.458 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] INFO org.apache.spark.executor.Executor -- Running task 0.0 in stage 92.0 (TID 63)\n",
      "18:14:32.460 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller -- stageTCMP: (92, 0) -> 1\n",
      "18:14:32.461 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.storage.BlockManager -- Getting local block broadcast_101\n",
      "18:14:32.461 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.storage.BlockManager -- Level for block broadcast_101 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "18:14:32.466 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection -- code for pmod(hash(input[0, bigint, true], 42), 50):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = false;\n",
      "/* 032 */     int value_0 = -1;\n",
      "/* 033 */     if (50 == 0) {\n",
      "/* 034 */       isNull_0 = true;\n",
      "/* 035 */     } else {\n",
      "/* 036 */       int value_1 = 42;\n",
      "/* 037 */       boolean isNull_2 = i.isNullAt(0);\n",
      "/* 038 */       long value_2 = isNull_2 ?\n",
      "/* 039 */       -1L : (i.getLong(0));\n",
      "/* 040 */       if (!isNull_2) {\n",
      "/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashLong(value_2, value_1);\n",
      "/* 042 */       }\n",
      "/* 043 */\n",
      "/* 044 */       int remainder_0 = value_1 % 50;\n",
      "/* 045 */       if (remainder_0 < 0) {\n",
      "/* 046 */         value_0=(remainder_0 + 50) % 50;\n",
      "/* 047 */       } else {\n",
      "/* 048 */         value_0=remainder_0;\n",
      "/* 049 */       }\n",
      "/* 050 */\n",
      "/* 051 */     }\n",
      "/* 052 */     if (isNull_0) {\n",
      "/* 053 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 054 */     } else {\n",
      "/* 055 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 056 */     }\n",
      "/* 057 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "18:14:32.467 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection -- code for 0:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     mutableStateArray_0[0].write(0, 0L);\n",
      "/* 033 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 034 */   }\n",
      "/* 035 */\n",
      "/* 036 */\n",
      "/* 037 */ }\n",
      "\n",
      "18:14:32.467 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.memory.TaskMemoryManager -- Task 63 acquired 16.0 MiB for org.apache.spark.sql.catalyst.expressions.FixedLengthRowBasedKeyValueBatch@50b2aff8\n",
      "18:14:32.467 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection -- code for input[0, bigint, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     long value_0 = isNull_0 ?\n",
      "/* 033 */     -1L : (i.getLong(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "18:14:32.468 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection -- code for 0:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     mutableStateArray_0[0].write(0, 0L);\n",
      "/* 033 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 034 */   }\n",
      "/* 035 */\n",
      "/* 036 */\n",
      "/* 037 */ }\n",
      "\n",
      "18:14:32.468 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection -- code for input[0, bigint, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     long value_0 = isNull_0 ?\n",
      "/* 033 */     -1L : (i.getLong(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "18:14:32.468 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.memory.TaskMemoryManager -- Task 63 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@1d5218a7\n",
      "18:14:32.468 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection -- code for input[0, bigint, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     long value_0 = isNull_0 ?\n",
      "/* 033 */     -1L : (i.getLong(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "18:14:32.469 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.storage.BlockManager -- Getting local block broadcast_100\n",
      "18:14:32.469 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.storage.BlockManager -- Level for block broadcast_100 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "18:14:32.470 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.spark.source.BatchDataReader -- Reading 1 file split(s) for table rest_catalog.ncentral.customers\n",
      "18:14:32.470 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.spark.source.BatchDataReader -- Opening data file s3a://lakehouse/ncentral/customers/data/00000-23-a07f6ea7-815e-4199-8ac8-f2c1fa439666-0-00001.parquet\n",
      "18:14:32.470 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader -- File length 55179\n",
      "18:14:32.470 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader -- reading footer index at 55171\n",
      "18:14:32.470 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.aws.s3.S3InputStream -- Seek with new stream for s3a://lakehouse/ncentral/customers/data/00000-23-a07f6ea7-815e-4199-8ac8-f2c1fa439666-0-00001.parquet to offset 55171\n",
      "18:14:32.480 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader -- read footer length: 5631, footer index: 49540\n",
      "18:14:32.480 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.aws.s3.S3InputStream -- Seek with new stream for s3a://lakehouse/ncentral/customers/data/00000-23-a07f6ea7-815e-4199-8ac8-f2c1fa439666-0-00001.parquet to offset 49540\n",
      "18:14:32.493 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader -- Finished to read all footer bytes.\n",
      "18:14:32.495 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.format.converter.ParquetMetadataConverter -- FileMetaData(version:1, schema:[SchemaElement(name:table, num_children:27), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:city, converted_type:UTF8, field_id:1, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:contact_department, converted_type:UTF8, field_id:2, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:contact_email, converted_type:UTF8, field_id:3, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:contact_ext, converted_type:UTF8, field_id:4, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:contact_firstname, converted_type:UTF8, field_id:5, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:contact_lastname, converted_type:UTF8, field_id:6, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:contact_phone_number, converted_type:UTF8, field_id:7, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:contact_title, converted_type:UTF8, field_id:8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:country, converted_type:UTF8, field_id:9, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:INT64, repetition_type:OPTIONAL, name:customer_id, field_id:10), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:customer_name, converted_type:UTF8, field_id:11, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:external_id, converted_type:UTF8, field_id:12, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:external_id2, converted_type:UTF8, field_id:13, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BOOLEAN, repetition_type:OPTIONAL, name:is_service_org, field_id:14), SchemaElement(type:BOOLEAN, repetition_type:OPTIONAL, name:is_system, field_id:15), SchemaElement(type:INT64, repetition_type:OPTIONAL, name:maintenance_window_duration, field_id:16), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:maintenance_window_start, converted_type:UTF8, field_id:17, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:parent_customer_name, converted_type:UTF8, field_id:18, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:INT64, repetition_type:OPTIONAL, name:parent_id, field_id:19), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:phone, converted_type:UTF8, field_id:20, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:postal_code, converted_type:UTF8, field_id:21, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:programlevel_id, converted_type:UTF8, field_id:22, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:registration_token, converted_type:UTF8, field_id:23, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:registration_token_expiry_date, converted_type:UTF8, field_id:24, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:state_prov, converted_type:UTF8, field_id:25, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:street_1, converted_type:UTF8, field_id:26, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:street_2, converted_type:UTF8, field_id:27, logicalType:<LogicalType STRING:StringType()>)], num_rows:606, row_groups:[RowGroup(columns:[ColumnChunk(file_offset:880, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[city], codec:ZSTD, num_values:606, total_uncompressed_size:2055, total_compressed_size:1495, data_page_offset:880, dictionary_page_offset:4, statistics:Statistics(null_count:49, max_value:76 69 6E 65 6C 61 6E 64, min_value:41 6A 61 78), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:49202, offset_index_length:12, column_index_offset:48235, column_index_length:27), ColumnChunk(file_offset:1564, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[contact_department], codec:ZSTD, num_values:606, total_uncompressed_size:109, total_compressed_size:127, data_page_offset:1564, dictionary_page_offset:1499, statistics:Statistics(null_count:601, max_value:4D 61 72 6B 65 74 69 6E 67, min_value:43 6C 69 6E 69 63), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:49214, offset_index_length:11, column_index_offset:48262, column_index_length:31), ColumnChunk(file_offset:1626, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[contact_email], codec:ZSTD, num_values:606, total_uncompressed_size:14656, total_compressed_size:6495, data_page_offset:1626, statistics:Statistics(null_count:55, max_value:7A 6F 68 72 61 6F 6E 65 68 65 61 6C 74 68 40 67 6D 61 69 6C 2E 63 6F 6D, min_value:32 70 73 6F 6E 65 69 6C 6C 40 72 6F 67 65 72 73 2E 63 6F 6D), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:49225, offset_index_length:12, column_index_offset:48293, column_index_length:59), ColumnChunk(file_offset:8121, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[contact_ext], codec:ZSTD, num_values:606, total_uncompressed_size:274, total_compressed_size:200, data_page_offset:8121, statistics:Statistics(null_count:579, max_value:78 20 35, min_value:31), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:49237, offset_index_length:12, column_index_offset:48352, column_index_length:20), ColumnChunk(file_offset:10543, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[contact_firstname], codec:ZSTD, num_values:606, total_uncompressed_size:5026, total_compressed_size:2980, data_page_offset:10543, dictionary_page_offset:8321, statistics:Statistics(null_count:50, max_value:69 6E 66 6F, min_value:41 61 72 6F 6E), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:49249, offset_index_length:13, column_index_offset:48372, column_index_length:24), ColumnChunk(file_offset:11301, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[contact_lastname], codec:ZSTD, num_values:606, total_uncompressed_size:5925, total_compressed_size:3161, data_page_offset:11301, statistics:Statistics(null_count:53, max_value:5A 68 75, min_value:41 62 62 6F 74 74), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:49262, offset_index_length:13, column_index_offset:48396, column_index_length:24), ColumnChunk(file_offset:14462, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[contact_phone_number], codec:ZSTD, num_values:606, total_uncompressed_size:8114, total_compressed_size:3088, data_page_offset:14462, statistics:Statistics(null_count:53, max_value:39 31 37 37 38 39 32 35 36 34, min_value:28 32 32 36 29 20 33 37 38 2D 36 35 31 35), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:49275, offset_index_length:13, column_index_offset:48420, column_index_length:39), ColumnChunk(file_offset:18371, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[contact_title], codec:ZSTD, num_values:606, total_uncompressed_size:2119, total_compressed_size:1361, data_page_offset:18371, dictionary_page_offset:17550, statistics:Statistics(null_count:148, max_value:57 69 66 65, min_value:41 64 6D 69 6E), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:49288, offset_index_length:13, column_index_offset:48459, column_index_length:25), ColumnChunk(file_offset:18945, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[country], codec:ZSTD, num_values:606, total_uncompressed_size:142, total_compressed_size:160, data_page_offset:18945, dictionary_page_offset:18911, statistics:Statistics(max:43 41, min:43 41, null_count:47, max_value:43 41, min_value:43 41), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:49301, offset_index_length:13, column_index_offset:48484, column_index_length:19), ColumnChunk(file_offset:19071, meta_data:ColumnMetaData(type:INT64, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[customer_id], codec:ZSTD, num_values:606, total_uncompressed_size:4881, total_compressed_size:1021, data_page_offset:19071, statistics:Statistics(max:D0 03 00 00 00 00 00 00, min:64 00 00 00 00 00 00 00, null_count:0, max_value:D0 03 00 00 00 00 00 00, min_value:64 00 00 00 00 00 00 00), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:49314, offset_index_length:13, column_index_offset:48503, column_index_length:31), ColumnChunk(file_offset:20092, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[customer_name], codec:ZSTD, num_values:606, total_uncompressed_size:13701, total_compressed_size:6087, data_page_offset:20092, statistics:Statistics(null_count:0, max_value:59 6F 75 72 20 54 6F 74 61 6C 20 48 65 61 6C 74 68 20 43 65 6E 74 72 65, min_value:31 2E 20 4D 69 67 72 61 74 69 6F 6E), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:49327, offset_index_length:13, column_index_offset:48534, column_index_length:51), ColumnChunk(file_offset:26179, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[external_id], codec:ZSTD, num_values:606, total_uncompressed_size:10420, total_compressed_size:5199, data_page_offset:26179, statistics:Statistics(null_count:46, max_value:79 6F 72 6B 72 65 67 69 6F 6E 64 65 6E 74 75 72 65, min_value:31 31 30 30 46 48 4E), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:49340, offset_index_length:13, column_index_offset:48585, column_index_length:39), ColumnChunk(file_offset:31378, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[external_id2], codec:ZSTD, num_values:606, total_uncompressed_size:31, total_compressed_size:40, data_page_offset:31378, statistics:Statistics(null_count:606), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:49353, offset_index_length:12, column_index_offset:48624, column_index_length:16), ColumnChunk(file_offset:31418, meta_data:ColumnMetaData(type:BOOLEAN, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[is_service_org], codec:ZSTD, num_values:606, total_uncompressed_size:108, total_compressed_size:48, data_page_offset:31418, statistics:Statistics(max:00, min:00, null_count:0, max_value:00, min_value:00), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:49365, offset_index_length:12, column_index_offset:48640, column_index_length:17), ColumnChunk(file_offset:31466, meta_data:ColumnMetaData(type:BOOLEAN, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[is_system], codec:ZSTD, num_values:606, total_uncompressed_size:108, total_compressed_size:48, data_page_offset:31466, statistics:Statistics(max:00, min:00, null_count:0, max_value:00, min_value:00), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:49377, offset_index_length:12, column_index_offset:48657, column_index_length:17), ColumnChunk(file_offset:31549, meta_data:ColumnMetaData(type:INT64, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[maintenance_window_duration], codec:ZSTD, num_values:606, total_uncompressed_size:60, total_compressed_size:78, data_page_offset:31549, dictionary_page_offset:31514, statistics:Statistics(max:00 00 00 00 00 00 00 00, min:00 00 00 00 00 00 00 00, null_count:0, max_value:00 00 00 00 00 00 00 00, min_value:00 00 00 00 00 00 00 00), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:49389, offset_index_length:12, column_index_offset:48674, column_index_length:31), ColumnChunk(file_offset:31592, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[maintenance_window_start], codec:ZSTD, num_values:606, total_uncompressed_size:31, total_compressed_size:40, data_page_offset:31592, statistics:Statistics(null_count:606), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:49401, offset_index_length:12, column_index_offset:48705, column_index_length:16), ColumnChunk(file_offset:31685, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[parent_customer_name], codec:ZSTD, num_values:606, total_uncompressed_size:78, total_compressed_size:96, data_page_offset:31685, dictionary_page_offset:31632, statistics:Statistics(max:42 6C 75 65 42 69 72 64 20 49 54 20 53 6F 6C 75 74 69 6F 6E 73, min:42 6C 75 65 42 69 72 64 20 49 54 20 53 6F 6C 75 74 69 6F 6E 73, null_count:0, max_value:42 6C 75 65 42 69 72 64 20 49 54 20 53 6F 6C 75 74 69 6F 6E 73, min_value:42 6C 75 65 42 69 72 64 20 49 54 20 53 6F 6C 75 74 69 6F 6E 73), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:49413, offset_index_length:12, column_index_offset:48721, column_index_length:57), ColumnChunk(file_offset:31764, meta_data:ColumnMetaData(type:INT64, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[parent_id], codec:ZSTD, num_values:606, total_uncompressed_size:61, total_compressed_size:79, data_page_offset:31764, dictionary_page_offset:31728, statistics:Statistics(max:32 00 00 00 00 00 00 00, min:32 00 00 00 00 00 00 00, null_count:0, max_value:32 00 00 00 00 00 00 00, min_value:32 00 00 00 00 00 00 00), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:49425, offset_index_length:12, column_index_offset:48778, column_index_length:31), ColumnChunk(file_offset:31807, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[phone], codec:ZSTD, num_values:606, total_uncompressed_size:7629, total_compressed_size:2841, data_page_offset:31807, statistics:Statistics(null_count:73, max_value:39 30 35 39 39 37 32 38 30 35, min_value:28 32 38 39 29 20 32 39 36 2D 34 37 31 32), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:49437, offset_index_length:13, column_index_offset:48809, column_index_length:40), ColumnChunk(file_offset:36717, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[postal_code], codec:ZSTD, num_values:606, total_uncompressed_size:5519, total_compressed_size:2823, data_page_offset:36717, dictionary_page_offset:34648, statistics:Statistics(null_count:54, max_value:56 39 50 20 32 48 31, min_value:42 34 56 20 33 4E 32), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:49450, offset_index_length:13, column_index_offset:48849, column_index_length:29), ColumnChunk(file_offset:37471, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[programlevel_id], codec:ZSTD, num_values:606, total_uncompressed_size:31, total_compressed_size:40, data_page_offset:37471, statistics:Statistics(null_count:606), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:49463, offset_index_length:12, column_index_offset:48878, column_index_length:16), ColumnChunk(file_offset:37511, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[registration_token], codec:ZSTD, num_values:606, total_uncompressed_size:5357, total_compressed_size:2795, data_page_offset:37511, statistics:Statistics(null_count:475, max_value:66 63 61 38 63 64 33 30 2D 35 64 62 38 2D 31 63 33 37 2D 66 65 34 62 2D 34 33 35 64 35 30 33 63 62 63 32 32, min_value:30 31 32 35 61 39 39 66 2D 65 33 66 35 2D 34 61 39 64 2D 31 63 31 39 2D 37 63 33 32 34 37 34 61 65 38 35 38), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:49475, offset_index_length:13, column_index_offset:48894, column_index_length:88), ColumnChunk(file_offset:40445, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[registration_token_expiry_date], codec:ZSTD, num_values:606, total_uncompressed_size:984, total_compressed_size:352, data_page_offset:40445, dictionary_page_offset:40306, statistics:Statistics(null_count:475, max_value:32 30 32 34 2D 30 37 2D 32 34 20 32 33 3A 35 39 3A 30 30 2E 30 30 30 20 2D 30 34 30 30, min_value:32 30 32 34 2D 30 36 2D 32 35 20 32 33 3A 35 39 3A 30 30 2E 30 30 30 20 2D 30 34 30 30), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:49488, offset_index_length:13, column_index_offset:48982, column_index_length:74), ColumnChunk(file_offset:40722, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[state_prov], codec:ZSTD, num_values:606, total_uncompressed_size:329, total_compressed_size:288, data_page_offset:40722, dictionary_page_offset:40658, statistics:Statistics(null_count:66, max_value:4F 4E, min_value:41 42), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:49501, offset_index_length:13, column_index_offset:49056, column_index_length:20), ColumnChunk(file_offset:45754, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[street_1], codec:ZSTD, num_values:606, total_uncompressed_size:11585, total_compressed_size:5567, data_page_offset:45754, dictionary_page_offset:40946, statistics:Statistics(null_count:49, max_value:53 75 6E 6E 79 62 72 6F 6F 6B 20 48 65 61 6C 74 68 20 53 63 69 65 6E 63 65 73 20 43 65 6E 74 72 65, min_value:31 20 2D 20 31 31 30 30 31 20 32 30 74 68 20 41 76 65 6E 75 65), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:49514, offset_index_length:13, column_index_offset:49076, column_index_length:69), ColumnChunk(file_offset:47678, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[street_2], codec:ZSTD, num_values:606, total_uncompressed_size:3789, total_compressed_size:1722, data_page_offset:47678, dictionary_page_offset:46513, statistics:Statistics(null_count:189, max_value:57 68 69 74 62 79 20 4D 65 64 69 63 61 6C 20 41 72 74 73 20 43 65 6E 74 72 65 2C 20 53 75 69 74 65 20 32 30 33, min_value:23 32 30 31), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:49527, offset_index_length:13, column_index_offset:49145, column_index_length:57)], total_byte_size:103122, num_rows:606, file_offset:4, total_compressed_size:48231, ordinal:0)], key_value_metadata:[KeyValue(key:iceberg.schema, value:{\"type\":\"struct\",\"schema-id\":0,\"fields\":[{\"id\":1,\"name\":\"city\",\"required\":false,\"type\":\"string\"},{\"id\":2,\"name\":\"contact_department\",\"required\":false,\"type\":\"string\"},{\"id\":3,\"name\":\"contact_email\",\"required\":false,\"type\":\"string\"},{\"id\":4,\"name\":\"contact_ext\",\"required\":false,\"type\":\"string\"},{\"id\":5,\"name\":\"contact_firstname\",\"required\":false,\"type\":\"string\"},{\"id\":6,\"name\":\"contact_lastname\",\"required\":false,\"type\":\"string\"},{\"id\":7,\"name\":\"contact_phone_number\",\"required\":false,\"type\":\"string\"},{\"id\":8,\"name\":\"contact_title\",\"required\":false,\"type\":\"string\"},{\"id\":9,\"name\":\"country\",\"required\":false,\"type\":\"string\"},{\"id\":10,\"name\":\"customer_id\",\"required\":false,\"type\":\"long\"},{\"id\":11,\"name\":\"customer_name\",\"required\":false,\"type\":\"string\"},{\"id\":12,\"name\":\"external_id\",\"required\":false,\"type\":\"string\"},{\"id\":13,\"name\":\"external_id2\",\"required\":false,\"type\":\"string\"},{\"id\":14,\"name\":\"is_service_org\",\"required\":false,\"type\":\"boolean\"},{\"id\":15,\"name\":\"is_system\",\"required\":false,\"type\":\"boolean\"},{\"id\":16,\"name\":\"maintenance_window_duration\",\"required\":false,\"type\":\"long\"},{\"id\":17,\"name\":\"maintenance_window_start\",\"required\":false,\"type\":\"string\"},{\"id\":18,\"name\":\"parent_customer_name\",\"required\":false,\"type\":\"string\"},{\"id\":19,\"name\":\"parent_id\",\"required\":false,\"type\":\"long\"},{\"id\":20,\"name\":\"phone\",\"required\":false,\"type\":\"string\"},{\"id\":21,\"name\":\"postal_code\",\"required\":false,\"type\":\"string\"},{\"id\":22,\"name\":\"programlevel_id\",\"required\":false,\"type\":\"string\"},{\"id\":23,\"name\":\"registration_token\",\"required\":false,\"type\":\"string\"},{\"id\":24,\"name\":\"registration_token_expiry_date\",\"required\":false,\"type\":\"string\"},{\"id\":25,\"name\":\"state_prov\",\"required\":false,\"type\":\"string\"},{\"id\":26,\"name\":\"street_1\",\"required\":false,\"type\":\"string\"},{\"id\":27,\"name\":\"street_2\",\"required\":false,\"type\":\"string\"}]})], created_by:parquet-mr version 1.13.1 (build db4183109d5b734ec5930d870cdae161e408ddba), column_orders:[<ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>])\n",
      "18:14:32.506 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.format.converter.ParquetMetadataConverter -- {\n",
      "  \"fileMetaData\" : {\n",
      "    \"schema\" : {\n",
      "      \"name\" : \"table\",\n",
      "      \"repetition\" : \"REPEATED\",\n",
      "      \"logicalTypeAnnotation\" : null,\n",
      "      \"id\" : null,\n",
      "      \"fields\" : [ {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 1\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"contact_department\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 2\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"contact_email\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 3\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"contact_ext\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 4\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"contact_firstname\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 5\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"contact_lastname\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 6\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"contact_phone_number\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 7\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"contact_title\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 8\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"country\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 9\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"customer_id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 10\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"INT64\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }, {\n",
      "        \"name\" : \"customer_name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 11\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"external_id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 12\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"external_id2\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 13\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"is_service_org\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 14\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BOOLEAN\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }, {\n",
      "        \"name\" : \"is_system\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 15\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BOOLEAN\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }, {\n",
      "        \"name\" : \"maintenance_window_duration\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 16\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"INT64\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }, {\n",
      "        \"name\" : \"maintenance_window_start\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 17\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"parent_customer_name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 18\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"parent_id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 19\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"INT64\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }, {\n",
      "        \"name\" : \"phone\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 20\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"postal_code\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 21\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"programlevel_id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 22\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"registration_token\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 23\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"registration_token_expiry_date\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 24\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"state_prov\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 25\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"street_1\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 26\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"street_2\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 27\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      } ],\n",
      "      \"paths\" : [ [ \"city\" ], [ \"contact_department\" ], [ \"contact_email\" ], [ \"contact_ext\" ], [ \"contact_firstname\" ], [ \"contact_lastname\" ], [ \"contact_phone_number\" ], [ \"contact_title\" ], [ \"country\" ], [ \"customer_id\" ], [ \"customer_name\" ], [ \"external_id\" ], [ \"external_id2\" ], [ \"is_service_org\" ], [ \"is_system\" ], [ \"maintenance_window_duration\" ], [ \"maintenance_window_start\" ], [ \"parent_customer_name\" ], [ \"parent_id\" ], [ \"phone\" ], [ \"postal_code\" ], [ \"programlevel_id\" ], [ \"registration_token\" ], [ \"registration_token_expiry_date\" ], [ \"state_prov\" ], [ \"street_1\" ], [ \"street_2\" ] ],\n",
      "      \"columns\" : [ {\n",
      "        \"path\" : [ \"city\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"city\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 1\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"contact_department\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"contact_department\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 2\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"contact_email\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"contact_email\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 3\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"contact_ext\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"contact_ext\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 4\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"contact_firstname\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"contact_firstname\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 5\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"contact_lastname\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"contact_lastname\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 6\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"contact_phone_number\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"contact_phone_number\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 7\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"contact_title\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"contact_title\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 8\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"country\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"country\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 9\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"customer_id\" ],\n",
      "        \"type\" : \"INT64\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"customer_id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : null,\n",
      "          \"id\" : {\n",
      "            \"id\" : 10\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"INT64\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : null\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"customer_name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"customer_name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 11\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"external_id\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"external_id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 12\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"external_id2\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"external_id2\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 13\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"is_service_org\" ],\n",
      "        \"type\" : \"BOOLEAN\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"is_service_org\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : null,\n",
      "          \"id\" : {\n",
      "            \"id\" : 14\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BOOLEAN\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : null\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"is_system\" ],\n",
      "        \"type\" : \"BOOLEAN\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"is_system\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : null,\n",
      "          \"id\" : {\n",
      "            \"id\" : 15\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BOOLEAN\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : null\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"maintenance_window_duration\" ],\n",
      "        \"type\" : \"INT64\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"maintenance_window_duration\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : null,\n",
      "          \"id\" : {\n",
      "            \"id\" : 16\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"INT64\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : null\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"maintenance_window_start\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"maintenance_window_start\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 17\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"parent_customer_name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"parent_customer_name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 18\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"parent_id\" ],\n",
      "        \"type\" : \"INT64\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"parent_id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : null,\n",
      "          \"id\" : {\n",
      "            \"id\" : 19\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"INT64\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : null\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"phone\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"phone\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 20\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"postal_code\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"postal_code\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 21\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"programlevel_id\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"programlevel_id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 22\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"registration_token\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"registration_token\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 23\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"registration_token_expiry_date\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"registration_token_expiry_date\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 24\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"state_prov\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"state_prov\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 25\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"street_1\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"street_1\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 26\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"street_2\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"street_2\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 27\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      } ],\n",
      "      \"primitive\" : false,\n",
      "      \"fieldCount\" : 27,\n",
      "      \"originalType\" : null\n",
      "    },\n",
      "    \"keyValueMetaData\" : {\n",
      "      \"iceberg.schema\" : \"{\\\"type\\\":\\\"struct\\\",\\\"schema-id\\\":0,\\\"fields\\\":[{\\\"id\\\":1,\\\"name\\\":\\\"city\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":2,\\\"name\\\":\\\"contact_department\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":3,\\\"name\\\":\\\"contact_email\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":4,\\\"name\\\":\\\"contact_ext\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":5,\\\"name\\\":\\\"contact_firstname\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":6,\\\"name\\\":\\\"contact_lastname\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":7,\\\"name\\\":\\\"contact_phone_number\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":8,\\\"name\\\":\\\"contact_title\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":9,\\\"name\\\":\\\"country\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":10,\\\"name\\\":\\\"customer_id\\\",\\\"required\\\":false,\\\"type\\\":\\\"long\\\"},{\\\"id\\\":11,\\\"name\\\":\\\"customer_name\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":12,\\\"name\\\":\\\"external_id\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":13,\\\"name\\\":\\\"external_id2\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":14,\\\"name\\\":\\\"is_service_org\\\",\\\"required\\\":false,\\\"type\\\":\\\"boolean\\\"},{\\\"id\\\":15,\\\"name\\\":\\\"is_system\\\",\\\"required\\\":false,\\\"type\\\":\\\"boolean\\\"},{\\\"id\\\":16,\\\"name\\\":\\\"maintenance_window_duration\\\",\\\"required\\\":false,\\\"type\\\":\\\"long\\\"},{\\\"id\\\":17,\\\"name\\\":\\\"maintenance_window_start\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":18,\\\"name\\\":\\\"parent_customer_name\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":19,\\\"name\\\":\\\"parent_id\\\",\\\"required\\\":false,\\\"type\\\":\\\"long\\\"},{\\\"id\\\":20,\\\"name\\\":\\\"phone\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":21,\\\"name\\\":\\\"postal_code\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":22,\\\"name\\\":\\\"programlevel_id\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":23,\\\"name\\\":\\\"registration_token\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":24,\\\"name\\\":\\\"registration_token_expiry_date\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":25,\\\"name\\\":\\\"state_prov\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":26,\\\"name\\\":\\\"street_1\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":27,\\\"name\\\":\\\"street_2\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"}]}\"\n",
      "    },\n",
      "    \"createdBy\" : \"parquet-mr version 1.13.1 (build db4183109d5b734ec5930d870cdae161e408ddba)\",\n",
      "    \"fileDecryptor\" : null,\n",
      "    \"encryptionType\" : \"UNENCRYPTED\"\n",
      "  },\n",
      "  \"blocks\" : [ {\n",
      "    \"columns\" : [ {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48235,\n",
      "        \"length\" : 27\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49202,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 4,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 1495,\n",
      "      \"totalUncompressedSize\" : 2055,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"dmluZWxhbmQ=\",\n",
      "          \"bytesUnsafe\" : \"dmluZWxhbmQ=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"QWpheA==\",\n",
      "          \"bytesUnsafe\" : \"QWpheA==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"QWpheA==\",\n",
      "        \"maxBytes\" : \"dmluZWxhbmQ=\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 49\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 880,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"city\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 4,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 1\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48262,\n",
      "        \"length\" : 31\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49214,\n",
      "        \"length\" : 11\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 1499,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 127,\n",
      "      \"totalUncompressedSize\" : 109,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"TWFya2V0aW5n\",\n",
      "          \"bytesUnsafe\" : \"TWFya2V0aW5n\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"Q2xpbmlj\",\n",
      "          \"bytesUnsafe\" : \"Q2xpbmlj\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"Q2xpbmlj\",\n",
      "        \"maxBytes\" : \"TWFya2V0aW5n\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 601\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 1564,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"contact_department\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 1499,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"contact_department\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 2\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48293,\n",
      "        \"length\" : 59\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49225,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 6495,\n",
      "      \"totalUncompressedSize\" : 14656,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"em9ocmFvbmVoZWFsdGhAZ21haWwuY29t\",\n",
      "          \"bytesUnsafe\" : \"em9ocmFvbmVoZWFsdGhAZ21haWwuY29t\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"MnBzb25laWxsQHJvZ2Vycy5jb20=\",\n",
      "          \"bytesUnsafe\" : \"MnBzb25laWxsQHJvZ2Vycy5jb20=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"MnBzb25laWxsQHJvZ2Vycy5jb20=\",\n",
      "        \"maxBytes\" : \"em9ocmFvbmVoZWFsdGhAZ21haWwuY29t\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 55\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 1626,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"contact_email\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 1626,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"contact_email\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 3\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48352,\n",
      "        \"length\" : 20\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49237,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 200,\n",
      "      \"totalUncompressedSize\" : 274,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"eCA1\",\n",
      "          \"bytesUnsafe\" : \"eCA1\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"MQ==\",\n",
      "          \"bytesUnsafe\" : \"MQ==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"MQ==\",\n",
      "        \"maxBytes\" : \"eCA1\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 579\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 8121,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"contact_ext\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 8121,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"contact_ext\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 4\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48372,\n",
      "        \"length\" : 24\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49249,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 8321,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 2980,\n",
      "      \"totalUncompressedSize\" : 5026,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"aW5mbw==\",\n",
      "          \"bytesUnsafe\" : \"aW5mbw==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"QWFyb24=\",\n",
      "          \"bytesUnsafe\" : \"QWFyb24=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"QWFyb24=\",\n",
      "        \"maxBytes\" : \"aW5mbw==\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 50\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 10543,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"contact_firstname\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 8321,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"contact_firstname\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 5\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48396,\n",
      "        \"length\" : 24\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49262,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 3161,\n",
      "      \"totalUncompressedSize\" : 5925,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"Wmh1\",\n",
      "          \"bytesUnsafe\" : \"Wmh1\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"QWJib3R0\",\n",
      "          \"bytesUnsafe\" : \"QWJib3R0\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"QWJib3R0\",\n",
      "        \"maxBytes\" : \"Wmh1\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 53\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 11301,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"contact_lastname\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 11301,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"contact_lastname\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 6\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdina-RECORD 0----------\n",
      " customer_id | 125 \n",
      " count       | 1   \n",
      "-RECORD 1----------\n",
      " customer_id | 158 \n",
      " count       | 1   \n",
      "-RECORD 2----------\n",
      " customer_id | 959 \n",
      " count       | 1   \n",
      "-RECORD 3----------\n",
      " customer_id | 964 \n",
      " count       | 1   \n",
      "-RECORD 4----------\n",
      " customer_id | 447 \n",
      " count       | 1   \n",
      "-RECORD 5----------\n",
      " customer_id | 298 \n",
      " count       | 1   \n",
      "-RECORD 6----------\n",
      " customer_id | 303 \n",
      " count       | 1   \n",
      "-RECORD 7----------\n",
      " customer_id | 389 \n",
      " count       | 1   \n",
      "-RECORD 8----------\n",
      " customer_id | 397 \n",
      " count       | 1   \n",
      "-RECORD 9----------\n",
      " customer_id | 410 \n",
      " count       | 1   \n",
      "only showing top 10 rows\n",
      "\n",
      "adsads\n",
      "l\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48420,\n",
      "        \"length\" : 39\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49275,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 3088,\n",
      "      \"totalUncompressedSize\" : 8114,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"OTE3Nzg5MjU2NA==\",\n",
      "          \"bytesUnsafe\" : \"OTE3Nzg5MjU2NA==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"KDIyNikgMzc4LTY1MTU=\",\n",
      "          \"bytesUnsafe\" : \"KDIyNikgMzc4LTY1MTU=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"KDIyNikgMzc4LTY1MTU=\",\n",
      "        \"maxBytes\" : \"OTE3Nzg5MjU2NA==\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 53\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 14462,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"contact_phone_number\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 14462,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"contact_phone_number\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 7\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48459,\n",
      "        \"length\" : 25\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49288,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 17550,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 1361,\n",
      "      \"totalUncompressedSize\" : 2119,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"V2lmZQ==\",\n",
      "          \"bytesUnsafe\" : \"V2lmZQ==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"QWRtaW4=\",\n",
      "          \"bytesUnsafe\" : \"QWRtaW4=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"QWRtaW4=\",\n",
      "        \"maxBytes\" : \"V2lmZQ==\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 148\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 18371,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"contact_title\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 17550,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"contact_title\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 8\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48484,\n",
      "        \"length\" : 19\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49301,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 18911,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 160,\n",
      "      \"totalUncompressedSize\" : 142,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"Q0E=\",\n",
      "          \"bytesUnsafe\" : \"Q0E=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"Q0E=\",\n",
      "          \"bytesUnsafe\" : \"Q0E=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"Q0E=\",\n",
      "        \"maxBytes\" : \"Q0E=\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 47\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 18945,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"country\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 18911,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"country\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 9\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48503,\n",
      "        \"length\" : 31\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49314,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 1021,\n",
      "      \"totalUncompressedSize\" : 4881,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : 976,\n",
      "        \"min\" : 100,\n",
      "        \"minBytes\" : \"ZAAAAAAAAAA=\",\n",
      "        \"maxBytes\" : \"0AMAAAAAAAA=\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 19071,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"INT64\",\n",
      "      \"path\" : [ \"customer_id\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 19071,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"customer_id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 10\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"INT64\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48534,\n",
      "        \"length\" : 51\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49327,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 6087,\n",
      "      \"totalUncompressedSize\" : 13701,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"WW91ciBUb3RhbCBIZWFsdGggQ2VudHJl\",\n",
      "          \"bytesUnsafe\" : \"WW91ciBUb3RhbCBIZWFsdGggQ2VudHJl\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"MS4gTWlncmF0aW9u\",\n",
      "          \"bytesUnsafe\" : \"MS4gTWlncmF0aW9u\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"MS4gTWlncmF0aW9u\",\n",
      "        \"maxBytes\" : \"WW91ciBUb3RhbCBIZWFsdGggQ2VudHJl\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 20092,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"customer_name\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 20092,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"customer_name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 11\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48585,\n",
      "        \"length\" : 39\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49340,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 5199,\n",
      "      \"totalUncompressedSize\" : 10420,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"eW9ya3JlZ2lvbmRlbnR1cmU=\",\n",
      "          \"bytesUnsafe\" : \"eW9ya3JlZ2lvbmRlbnR1cmU=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"MTEwMEZITg==\",\n",
      "          \"bytesUnsafe\" : \"MTEwMEZITg==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"MTEwMEZITg==\",\n",
      "        \"maxBytes\" : \"eW9ya3JlZ2lvbmRlbnR1cmU=\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 46\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 26179,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"external_id\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 26179,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"external_id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 12\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48624,\n",
      "        \"length\" : 16\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49353,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 40,\n",
      "      \"totalUncompressedSize\" : 31,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : null,\n",
      "        \"min\" : null,\n",
      "        \"minBytes\" : null,\n",
      "        \"maxBytes\" : null,\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 606\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 31378,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"external_id2\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 31378,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"external_id2\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 13\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48640,\n",
      "        \"length\" : 17\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49365,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 48,\n",
      "      \"totalUncompressedSize\" : 108,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : false,\n",
      "        \"min\" : false,\n",
      "        \"minBytes\" : \"AA==\",\n",
      "        \"maxBytes\" : \"AA==\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 31418,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BOOLEAN\",\n",
      "      \"path\" : [ \"is_service_org\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 31418,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"is_service_org\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 14\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BOOLEAN\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48657,\n",
      "        \"length\" : 17\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49377,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 48,\n",
      "      \"totalUncompressedSize\" : 108,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : false,\n",
      "        \"min\" : false,\n",
      "        \"minBytes\" : \"AA==\",\n",
      "        \"maxBytes\" : \"AA==\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 31466,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BOOLEAN\",\n",
      "      \"path\" : [ \"is_system\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 31466,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"is_system\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 15\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BOOLEAN\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48674,\n",
      "        \"length\" : 31\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49389,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 31514,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 78,\n",
      "      \"totalUncompressedSize\" : 60,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : 0,\n",
      "        \"min\" : 0,\n",
      "        \"minBytes\" : \"AAAAAAAAAAA=\",\n",
      "        \"maxBytes\" : \"AAAAAAAAAAA=\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 31549,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"INT64\",\n",
      "      \"path\" : [ \"maintenance_window_duration\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 31514,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"maintenance_window_duration\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 16\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"INT64\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48705,\n",
      "        \"length\" : 16\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49401,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 40,\n",
      "      \"totalUncompressedSize\" : 31,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : null,\n",
      "        \"min\" : null,\n",
      "        \"minBytes\" : null,\n",
      "        \"maxBytes\" : null,\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 606\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 31592,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"maintenance_window_start\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 31592,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"maintenance_window_start\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 17\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48721,\n",
      "        \"length\" : 57\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49413,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 31632,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 96,\n",
      "      \"totalUncompressedSize\" : 78,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"Qmx1ZUJpcmQgSVQgU29sdXRpb25z\",\n",
      "          \"bytesUnsafe\" : \"Qmx1ZUJpcmQgSVQgU29sdXRpb25z\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"Qmx1ZUJpcmQgSVQgU29sdXRpb25z\",\n",
      "          \"bytesUnsafe\" : \"Qmx1ZUJpcmQgSVQgU29sdXRpb25z\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"Qmx1ZUJpcmQgSVQgU29sdXRpb25z\",\n",
      "        \"maxBytes\" : \"Qmx1ZUJpcmQgSVQgU29sdXRpb25z\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 31685,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"parent_customer_name\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 31632,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"parent_customer_name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 18\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48778,\n",
      "        \"length\" : 31\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49425,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 31728,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 79,\n",
      "      \"totalUncompressedSize\" : 61,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : 50,\n",
      "        \"min\" : 50,\n",
      "        \"minBytes\" : \"MgAAAAAAAAA=\",\n",
      "        \"maxBytes\" : \"MgAAAAAAAAA=\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 31764,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"INT64\",\n",
      "      \"path\" : [ \"parent_id\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 31728,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"parent_id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 19\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"INT64\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48809,\n",
      "        \"length\" : 40\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49437,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 2841,\n",
      "      \"totalUncompressedSize\" : 7629,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"OTA1OTk3MjgwNQ==\",\n",
      "          \"bytesUnsafe\" : \"OTA1OTk3MjgwNQ==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"KDI4OSkgMjk2LTQ3MTI=\",\n",
      "          \"bytesUnsafe\" : \"KDI4OSkgMjk2LTQ3MTI=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"KDI4OSkgMjk2LTQ3MTI=\",\n",
      "        \"maxBytes\" : \"OTA1OTk3MjgwNQ==\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 73\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 31807,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"phone\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 31807,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"phone\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 20\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48849,\n",
      "        \"length\" : 29\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49450,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 34648,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 2823,\n",
      "      \"totalUncompressedSize\" : 5519,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"VjlQIDJIMQ==\",\n",
      "          \"bytesUnsafe\" : \"VjlQIDJIMQ==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"QjRWIDNOMg==\",\n",
      "          \"bytesUnsafe\" : \"QjRWIDNOMg==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"QjRWIDNOMg==\",\n",
      "        \"maxBytes\" : \"VjlQIDJIMQ==\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 54\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 36717,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"postal_code\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 34648,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"postal_code\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 21\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48878,\n",
      "        \"length\" : 16\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49463,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 40,\n",
      "      \"totalUncompressedSize\" : 31,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : null,\n",
      "        \"min\" : null,\n",
      "        \"minBytes\" : null,\n",
      "        \"maxBytes\" : null,\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 606\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 37471,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"programlevel_id\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 37471,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"programlevel_id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 22\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48894,\n",
      "        \"length\" : 88\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49475,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 2795,\n",
      "      \"totalUncompressedSize\" : 5357,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"ZmNhOGNkMzAtNWRiOC0xYzM3LWZlNGItNDM1ZDUwM2NiYzIy\",\n",
      "          \"bytesUnsafe\" : \"ZmNhOGNkMzAtNWRiOC0xYzM3LWZlNGItNDM1ZDUwM2NiYzIy\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"MDEyNWE5OWYtZTNmNS00YTlkLTFjMTktN2MzMjQ3NGFlODU4\",\n",
      "          \"bytesUnsafe\" : \"MDEyNWE5OWYtZTNmNS00YTlkLTFjMTktN2MzMjQ3NGFlODU4\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"MDEyNWE5OWYtZTNmNS00YTlkLTFjMTktN2MzMjQ3NGFlODU4\",\n",
      "        \"maxBytes\" : \"ZmNhOGNkMzAtNWRiOC0xYzM3LWZlNGItNDM1ZDUwM2NiYzIy\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 475\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 37511,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"registration_token\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 37511,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"registration_token\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 23\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 48982,\n",
      "        \"length\" : 74\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49488,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 40306,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 352,\n",
      "      \"totalUncompressedSize\" : 984,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"MjAyNC0wNy0yNCAyMzo1OTowMC4wMDAgLTA0MDA=\",\n",
      "          \"bytesUnsafe\" : \"MjAyNC0wNy0yNCAyMzo1OTowMC4wMDAgLTA0MDA=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"MjAyNC0wNi0yNSAyMzo1OTowMC4wMDAgLTA0MDA=\",\n",
      "          \"bytesUnsafe\" : \"MjAyNC0wNi0yNSAyMzo1OTowMC4wMDAgLTA0MDA=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"MjAyNC0wNi0yNSAyMzo1OTowMC4wMDAgLTA0MDA=\",\n",
      "        \"maxBytes\" : \"MjAyNC0wNy0yNCAyMzo1OTowMC4wMDAgLTA0MDA=\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 475\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 40445,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"registration_token_expiry_date\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 40306,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"registration_token_expiry_date\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 24\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 49056,\n",
      "        \"length\" : 20\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49501,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 40658,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 288,\n",
      "      \"totalUncompressedSize\" : 329,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"T04=\",\n",
      "          \"bytesUnsafe\" : \"T04=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"QUI=\",\n",
      "          \"bytesUnsafe\" : \"QUI=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"QUI=\",\n",
      "        \"maxBytes\" : \"T04=\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 66\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 40722,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"state_prov\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 40658,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"state_prov\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 25\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 49076,\n",
      "        \"length\" : 69\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49514,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 40946,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 5567,\n",
      "      \"totalUncompressedSize\" : 11585,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"U3Vubnlicm9vayBIZWFsdGggU2NpZW5jZXMgQ2VudHJl\",\n",
      "          \"bytesUnsafe\" : \"U3Vubnlicm9vayBIZWFsdGggU2NpZW5jZXMgQ2VudHJl\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"MSAtIDExMDAxIDIwdGggQXZlbnVl\",\n",
      "          \"bytesUnsafe\" : \"MSAtIDExMDAxIDIwdGggQXZlbnVl\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"MSAtIDExMDAxIDIwdGggQXZlbnVl\",\n",
      "        \"maxBytes\" : \"U3Vubnlicm9vayBIZWFsdGggU2NpZW5jZXMgQ2VudHJl\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 49\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 45754,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"street_1\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 40946,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"street_1\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 26\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 49145,\n",
      "        \"length\" : 57\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 49527,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 46513,\n",
      "      \"valueCount\" : 606,\n",
      "      \"totalSize\" : 1722,\n",
      "      \"totalUncompressedSize\" : 3789,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"V2hpdGJ5IE1lZGljYWwgQXJ0cyBDZW50cmUsIFN1aXRlIDIwMw==\",\n",
      "          \"bytesUnsafe\" : \"V2hpdGJ5IE1lZGljYWwgQXJ0cyBDZW50cmUsIFN1aXRlIDIwMw==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"IzIwMQ==\",\n",
      "          \"bytesUnsafe\" : \"IzIwMQ==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"IzIwMQ==\",\n",
      "        \"maxBytes\" : \"V2hpdGJ5IE1lZGljYWwgQXJ0cyBDZW50cmUsIFN1aXRlIDIwMw==\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 189\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 47678,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"street_2\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 46513,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"street_2\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 27\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    } ],\n",
      "    \"rowCount\" : 606,\n",
      "    \"totalByteSize\" : 103122,\n",
      "    \"path\" : null,\n",
      "    \"ordinal\" : 0,\n",
      "    \"rowIndexOffset\" : 0,\n",
      "    \"compressedSize\" : 48231,\n",
      "    \"startingPos\" : 4\n",
      "  } ]\n",
      "}\n",
      "18:14:32.634 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.aws.s3.S3InputStream -- Seek with new stream for s3a://lakehouse/ncentral/customers/data/00000-23-a07f6ea7-815e-4199-8ac8-f2c1fa439666-0-00001.parquet to offset 19071\n",
      "18:14:32.644 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] INFO org.apache.hadoop.io.compress.CodecPool -- Got brand-new decompressor [.zstd]\n",
      "18:14:32.645 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 4855 bytes\n",
      "18:14:32.645 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 4855 bytes\n",
      "18:14:32.646 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 4855 bytes and 606 records\n",
      "18:14:32.652 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:32.652 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 606 values of size 0 bits.\n",
      "18:14:32.652 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:32.652 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 7\n",
      "18:14:32.656 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.memory.TaskMemoryManager -- Task 63 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@1d5218a7\n",
      "18:14:32.665 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.memory.TaskMemoryManager -- Task 63 release 16.0 MiB from org.apache.spark.sql.catalyst.expressions.FixedLengthRowBasedKeyValueBatch@50b2aff8\n",
      "18:14:32.671 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter -- Writing shuffle index file for mapId 63 with length 50\n",
      "18:14:32.672 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver -- Shuffle index for mapId 63: [198,128,146,241,138,172,177,152,198,183,177,127,183,203,204,246,218,133,122,163,160,213,125,160,213,142,185,193,159,156,198,187,191,143,167,197,149,178,134,217,144,193,221,179,198,197,101,164,197,175]\n",
      "18:14:32.673 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] INFO org.apache.spark.executor.Executor -- Finished task 0.0 in stage 92.0 (TID 63). 5245 bytes result sent to driver\n",
      "18:14:32.673 [Executor task launch worker for task 0.0 in stage 92.0 (TID 63)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller -- stageTCMP: (92, 0) -> 0\n",
      "18:14:32.673 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl -- parentName: , name: TaskSet_92.0, runningTasks: 0\n",
      "18:14:32.673 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager -- No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "18:14:32.674 [task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager -- Finished task 0.0 in stage 92.0 (TID 63) in 217 ms on 10.0.0.146 (executor driver) (1/1)\n",
      "18:14:32.674 [task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl -- Removed TaskSet 92.0, whose tasks have all completed, from pool \n",
      "18:14:32.674 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- ShuffleMapTask finished on driver\n",
      "18:14:32.674 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- ShuffleMapStage 92 (showString at <unknown>:0) finished in 0.223 s\n",
      "18:14:32.675 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- looking for newly runnable stages\n",
      "18:14:32.675 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- running: Set()\n",
      "18:14:32.675 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- waiting: Set()\n",
      "18:14:32.675 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- failed: Set()\n",
      "18:14:32.675 [dag-scheduler-event-loop] DEBUG org.apache.spark.MapOutputTrackerMaster -- Increasing epoch to 30\n",
      "18:14:32.675 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- After removal of stage 92, remaining stages = 0\n",
      "18:14:32.677 [Thread-2] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage -- Physical stats available as Statistics(sizeInBytes=14.2 KiB, rowCount=606) for plan: HashAggregate(keys=[customer_id#2267L], functions=[count(1)], output=[toprettystring(customer_id)#2725, toprettystring(count)#2726])\n",
      "+- ShuffleQueryStage 0\n",
      "   +- Exchange hashpartitioning(customer_id#2267L, 50), ENSURE_REQUIREMENTS, [plan_id=1261]\n",
      "      +- *(1) HashAggregate(keys=[customer_id#2267L], functions=[partial_count(1)], output=[customer_id#2267L, count#2731L])\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- BatchScan rest_catalog.ncentral.customers[customer_id#2267L] rest_catalog.ncentral.customers (branch=null) [filters=, groupedBy=] RuntimeFilters: []\n",
      "\n",
      "18:14:32.681 [Thread-2] INFO org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil -- For shuffle(29), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "18:14:32.682 [Thread-2] INFO org.apache.spark.sql.execution.aggregate.HashAggregateExec -- spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "18:14:32.686 [Thread-2] DEBUG org.apache.spark.sql.execution.WholeStageCodegenExec -- \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 015 */\n",
      "/* 016 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 017 */     this.references = references;\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 021 */     partitionIndex = index;\n",
      "/* 022 */     this.inputs = inputs;\n",
      "/* 023 */\n",
      "/* 024 */     inputadapter_input_0 = inputs[0];\n",
      "/* 025 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 026 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 64);\n",
      "/* 027 */\n",
      "/* 028 */   }\n",
      "/* 029 */\n",
      "/* 030 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 031 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 032 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 033 */\n",
      "/* 034 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 035 */       long inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 036 */       -1L : (inputadapter_row_0.getLong(0));\n",
      "/* 037 */       long inputadapter_value_1 = inputadapter_row_0.getLong(1);\n",
      "/* 038 */\n",
      "/* 039 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1);\n",
      "/* 040 */       // shouldStop check is eliminated\n",
      "/* 041 */     }\n",
      "/* 042 */\n",
      "/* 043 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 044 */   }\n",
      "/* 045 */\n",
      "/* 046 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, long hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, long hashAgg_expr_1_0) throws java.io.IOException {\n",
      "/* 047 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 048 */\n",
      "/* 049 */     // generate grouping key\n",
      "/* 050 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 051 */\n",
      "/* 052 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 053 */\n",
      "/* 054 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 055 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 056 */     } else {\n",
      "/* 057 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 058 */     }\n",
      "/* 059 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 060 */     if (true) {\n",
      "/* 061 */       // try to get the buffer from hash map\n",
      "/* 062 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 063 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 064 */     }\n",
      "/* 065 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 066 */     // aggregation after processing all input rows.\n",
      "/* 067 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 068 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 069 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 070 */       } else {\n",
      "/* 071 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 072 */       }\n",
      "/* 073 */\n",
      "/* 074 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 075 */       // try to allocate buffer again.\n",
      "/* 076 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 077 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 078 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 079 */         // failed to allocate the first page\n",
      "/* 080 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 081 */       }\n",
      "/* 082 */     }\n",
      "/* 083 */\n",
      "/* 084 */     // common sub-expressions\n",
      "/* 085 */\n",
      "/* 086 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 087 */\n",
      "/* 088 */     long hashAgg_value_3 = hashAgg_unsafeRowAggBuffer_0.getLong(0);\n",
      "/* 089 */\n",
      "/* 090 */     long hashAgg_value_2 = -1L;\n",
      "/* 091 */\n",
      "/* 092 */     hashAgg_value_2 = hashAgg_value_3 + hashAgg_expr_1_0;\n",
      "/* 093 */\n",
      "/* 094 */     hashAgg_unsafeRowAggBuffer_0.setLong(0, hashAgg_value_2);\n",
      "/* 095 */\n",
      "/* 096 */   }\n",
      "/* 097 */\n",
      "/* 098 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 099 */   throws java.io.IOException {\n",
      "/* 100 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 101 */\n",
      "/* 102 */     boolean hashAgg_isNull_5 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 103 */     long hashAgg_value_5 = hashAgg_isNull_5 ?\n",
      "/* 104 */     -1L : (hashAgg_keyTerm_0.getLong(0));\n",
      "/* 105 */     long hashAgg_value_6 = hashAgg_bufferTerm_0.getLong(0);\n",
      "/* 106 */\n",
      "/* 107 */     UTF8String hashAgg_value_8;\n",
      "/* 108 */     if (hashAgg_isNull_5) {\n",
      "/* 109 */       hashAgg_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 110 */     } else {\n",
      "/* 111 */       hashAgg_value_8 = UTF8String.fromString(String.valueOf(hashAgg_value_5));\n",
      "/* 112 */     }\n",
      "/* 113 */     UTF8String hashAgg_value_10;\n",
      "/* 114 */     if (false) {\n",
      "/* 115 */       hashAgg_value_10 = UTF8String.fromString(\"NULL\");\n",
      "/* 116 */     } else {\n",
      "/* 117 */       hashAgg_value_10 = UTF8String.fromString(String.valueOf(hashAgg_value_6));\n",
      "/* 118 */     }\n",
      "/* 119 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 120 */\n",
      "/* 121 */     hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_8);\n",
      "/* 122 */\n",
      "/* 123 */     hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_10);\n",
      "/* 124 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 125 */\n",
      "/* 126 */   }\n",
      "/* 127 */\n",
      "/* 128 */   protected void processNext() throws java.io.IOException {\n",
      "/* 129 */     if (!hashAgg_initAgg_0) {\n",
      "/* 130 */       hashAgg_initAgg_0 = true;\n",
      "/* 131 */\n",
      "/* 132 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 133 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 134 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 135 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 136 */     }\n",
      "/* 137 */     // output the result\n",
      "/* 138 */\n",
      "/* 139 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 140 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 141 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 142 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 143 */       if (shouldStop()) return;\n",
      "/* 144 */     }\n",
      "/* 145 */     hashAgg_mapIter_0.close();\n",
      "/* 146 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 147 */       hashAgg_hashMap_0.free();\n",
      "/* 148 */     }\n",
      "/* 149 */   }\n",
      "/* 150 */\n",
      "/* 151 */ }\n",
      "\n",
      "18:14:32.712 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner -- Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "18:14:32.713 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner --  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "18:14:32.713 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner -- Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "18:14:32.714 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner --  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "18:14:32.714 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner -- Cleaning indylambda closure: $anonfun$runJob$5\n",
      "18:14:32.716 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner --  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "18:14:32.716 [Thread-2] INFO org.apache.spark.SparkContext -- Starting job: showString at <unknown>:0\n",
      "18:14:32.718 [Thread-2] DEBUG org.apache.spark.scheduler.DAGScheduler -- eagerlyComputePartitionsForRddAndAncestors for RDD 213 took 0.000045 seconds\n",
      "18:14:32.718 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- Merging stage rdd profiles: Set()\n",
      "18:14:32.718 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- Merging stage rdd profiles: Set()\n",
      "18:14:32.718 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Got job 64 (showString at <unknown>:0) with 1 output partitions\n",
      "18:14:32.718 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Final stage: ResultStage 94 (showString at <unknown>:0)\n",
      "18:14:32.718 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Parents of final stage: List(ShuffleMapStage 93)\n",
      "18:14:32.718 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Missing parents: List()\n",
      "18:14:32.722 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- submitStage(ResultStage 94 (name=showString at <unknown>:0;jobs=64))\n",
      "18:14:32.722 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- missing: List()\n",
      "18:14:32.722 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Submitting ResultStage 94 (MapPartitionsRDD[213] at showString at <unknown>:0), which has no missing parents\n",
      "18:14:32.722 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- submitMissingTasks(ResultStage 94)\n",
      "18:14:32.723 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_102 stored as values in memory (estimated size 38.8 KiB, free 434.1 MiB)\n",
      "18:14:32.723 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_102 locally took 0 ms\n",
      "18:14:32.724 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_102 without replication took 0 ms\n",
      "18:14:32.726 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_102_piece0 stored as bytes in memory (estimated size 17.7 KiB, free 434.1 MiB)\n",
      "18:14:32.726 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_102_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:32.726 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Added broadcast_102_piece0 in memory on 10.0.0.146:57286 (size: 17.7 KiB, free: 434.3 MiB)\n",
      "18:14:32.726 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_102_piece0\n",
      "18:14:32.726 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_102_piece0\n",
      "18:14:32.726 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_102_piece0 locally took 0 ms\n",
      "18:14:32.748 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_102_piece0 without replication took 21 ms\n",
      "18:14:32.748 [dag-scheduler-event-loop] INFO org.apache.spark.SparkContext -- Created broadcast 102 from broadcast at DAGScheduler.scala:1585\n",
      "18:14:32.748 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Submitting 1 missing tasks from ResultStage 94 (MapPartitionsRDD[213] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "18:14:32.748 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl -- Adding task set 94.0 with 1 tasks resource profile 0\n",
      "18:14:32.748 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager -- Epoch for TaskSet 94.0: 30\n",
      "18:14:32.748 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager -- Adding pending tasks took 0 ms\n",
      "18:14:32.748 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager -- Valid locality levels for TaskSet 94.0: NODE_LOCAL, ANY\n",
      "18:14:32.752 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl -- parentName: , name: TaskSet_94.0, runningTasks: 0\n",
      "18:14:32.752 [dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager -- Starting task 0.0 in stage 94.0 (TID 64) (10.0.0.146, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "18:14:32.753 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] INFO org.apache.spark.executor.Executor -- Running task 0.0 in stage 94.0 (TID 64)\n",
      "18:14:32.753 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller -- stageTCMP: (94, 0) -> 1\n",
      "18:14:32.753 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] DEBUG org.apache.spark.storage.BlockManager -- Getting local block broadcast_102\n",
      "18:14:32.753 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] DEBUG org.apache.spark.storage.BlockManager -- Level for block broadcast_102 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "18:14:32.759 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] DEBUG org.apache.spark.MapOutputTrackerMaster -- Fetching outputs for shuffle 29\n",
      "18:14:32.759 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] DEBUG org.apache.spark.MapOutputTrackerMaster -- Convert map statuses for shuffle 29, mappers 0-1, partitions 0-50\n",
      "18:14:32.760 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator -- maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647\n",
      "18:14:32.760 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator -- Getting 1 (9.0 KiB) non-empty blocks including 1 (9.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "18:14:32.768 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator -- Started 0 remote fetches in 8 ms\n",
      "18:14:32.768 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator -- Start fetching local blocks: (shuffle_29_63_0_50,0)\n",
      "18:14:32.768 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] DEBUG org.apache.spark.storage.BlockManager -- Getting local shuffle block shuffle_29_63_0_50\n",
      "18:14:32.769 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator -- Got local blocks in 9 ms\n",
      "18:14:32.769 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection -- code for 0:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     mutableStateArray_0[0].write(0, 0L);\n",
      "/* 033 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 034 */   }\n",
      "/* 035 */\n",
      "/* 036 */\n",
      "/* 037 */ }\n",
      "\n",
      "18:14:32.779 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection -- code for input[0, bigint, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     long value_0 = isNull_0 ?\n",
      "/* 033 */     -1L : (i.getLong(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "18:14:32.795 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] DEBUG org.apache.spark.memory.TaskMemoryManager -- Task 64 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@41f54707\n",
      "18:14:32.796 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection -- code for input[0, bigint, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     long value_0 = isNull_0 ?\n",
      "/* 033 */     -1L : (i.getLong(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "18:14:32.811 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] DEBUG org.apache.spark.memory.TaskMemoryManager -- Task 64 acquired 16.0 MiB for org.apache.spark.unsafe.map.BytesToBytesMap@41f54707\n",
      "18:14:32.812 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] DEBUG org.apache.spark.memory.TaskMemoryManager -- Task 64 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@41f54707\n",
      "18:14:32.812 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] DEBUG org.apache.spark.memory.TaskMemoryManager -- Task 64 release 16.0 MiB from org.apache.spark.unsafe.map.BytesToBytesMap@41f54707\n",
      "18:14:32.813 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] INFO org.apache.spark.executor.Executor -- Finished task 0.0 in stage 94.0 (TID 64). 7851 bytes result sent to driver\n",
      "18:14:32.813 [Executor task launch worker for task 0.0 in stage 94.0 (TID 64)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller -- stageTCMP: (94, 0) -> 0\n",
      "18:14:32.813 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl -- parentName: , name: TaskSet_94.0, runningTasks: 0\n",
      "18:14:32.822 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager -- No tasks for locality level NODE_LOCAL, so moving to locality level ANY\n",
      "18:14:32.822 [task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager -- Finished task 0.0 in stage 94.0 (TID 64) in 70 ms on 10.0.0.146 (executor driver) (1/1)\n",
      "18:14:32.822 [task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl -- Removed TaskSet 94.0, whose tasks have all completed, from pool \n",
      "18:14:32.822 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- ResultStage 94 (showString at <unknown>:0) finished in 0.099 s\n",
      "18:14:32.822 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- After removal of stage 94, remaining stages = 1\n",
      "18:14:32.824 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- After removal of stage 93, remaining stages = 0\n",
      "18:14:32.824 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Job 64 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "18:14:32.825 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl -- Killing all running tasks in stage 94: Stage finished\n",
      "18:14:32.825 [Thread-2] INFO org.apache.spark.scheduler.DAGScheduler -- Job 64 finished: showString at <unknown>:0, took 0.106885 s\n",
      "18:14:32.825 [Thread-2] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec -- Final plan:\n",
      "CollectLimit 11\n",
      "+- *(2) HashAggregate(keys=[customer_id#2267L], functions=[count(1)], output=[toprettystring(customer_id)#2725, toprettystring(count)#2726])\n",
      "   +- AQEShuffleRead coalesced\n",
      "      +- ShuffleQueryStage 0\n",
      "         +- Exchange hashpartitioning(customer_id#2267L, 50), ENSURE_REQUIREMENTS, [plan_id=1261]\n",
      "            +- *(1) HashAggregate(keys=[customer_id#2267L], functions=[partial_count(1)], output=[customer_id#2267L, count#2731L])\n",
      "               +- *(1) ColumnarToRow\n",
      "                  +- BatchScan rest_catalog.ncentral.customers[customer_id#2267L] rest_catalog.ncentral.customers (branch=null) [filters=, groupedBy=] RuntimeFilters: []\n",
      "\n",
      "18:14:32.831 [Thread-2] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection -- code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, StructField(toprettystring(customer_id),StringType,false), StructField(toprettystring(count),StringType,false)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[2];\n",
      "/* 024 */\n",
      "/* 025 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 026 */     boolean isNull_1 = true;\n",
      "/* 027 */     java.lang.String value_1 = null;\n",
      "/* 028 */     isNull_1 = false;\n",
      "/* 029 */     if (!isNull_1) {\n",
      "/* 030 */\n",
      "/* 031 */       Object funcResult_0 = null;\n",
      "/* 032 */       funcResult_0 = value_2.toString();\n",
      "/* 033 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 034 */\n",
      "/* 035 */     }\n",
      "/* 036 */     if (isNull_1) {\n",
      "/* 037 */       values_0[0] = null;\n",
      "/* 038 */     } else {\n",
      "/* 039 */       values_0[0] = value_1;\n",
      "/* 040 */     }\n",
      "/* 041 */\n",
      "/* 042 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 043 */     boolean isNull_3 = true;\n",
      "/* 044 */     java.lang.String value_3 = null;\n",
      "/* 045 */     isNull_3 = false;\n",
      "/* 046 */     if (!isNull_3) {\n",
      "/* 047 */\n",
      "/* 048 */       Object funcResult_1 = null;\n",
      "/* 049 */       funcResult_1 = value_4.toString();\n",
      "/* 050 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 051 */\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_3) {\n",
      "/* 054 */       values_0[1] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[1] = value_3;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 060 */     if (false) {\n",
      "/* 061 */       mutableRow.setNullAt(0);\n",
      "/* 062 */     } else {\n",
      "/* 063 */\n",
      "/* 064 */       mutableRow.update(0, value_0);\n",
      "/* 065 */     }\n",
      "/* 066 */\n",
      "/* 067 */     return mutableRow;\n",
      "/* 068 */   }\n",
      "/* 069 */\n",
      "/* 070 */\n",
      "/* 071 */ }\n",
      "\n",
      "18:14:35.961 [executor-heartbeater] DEBUG org.apache.spark.executor.ExecutorMetricsPoller -- removing (92, 0) from stageTCMP\n",
      "18:14:35.961 [executor-heartbeater] DEBUG org.apache.spark.executor.ExecutorMetricsPoller -- removing (94, 0) from stageTCMP\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Suppress all logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# If you want to be specific, you can configure the py4j logger as well\n",
    "logging.getLogger('py4j').setLevel(logging.ERROR)\n",
    "grouped = joined.groupBy(f\"{namespace}.customers.customer_id\").count()\n",
    "grouped.show(10, vertical=True)\n",
    "print(\"adsads\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd76b104-6cd1-4fcd-a687-9100715ef368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:14:57.074 [Thread-2] DEBUG org.apache.spark.sql.execution.SparkSqlParser -- Parsing command: SELECT * FROM ncentral.devices\n",
      "18:14:57.097 [Thread-2] DEBUG org.apache.iceberg.CachingCatalog -- Evicted ncentral.devices from the table cache (EXPIRED)\n",
      "18:14:57.100 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000050 preparing request execution\n",
      "18:14:57.100 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000050 target auth state: UNCHALLENGED\n",
      "18:14:57.100 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000050 proxy auth state: UNCHALLENGED\n",
      "18:14:57.100 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-0000000050 acquiring connection with route {}->http://10.0.0.146:8181\n",
      "18:14:57.100 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000050 acquiring endpoint (3 MINUTES)\n",
      "18:14:57.100 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000050 endpoint lease request (3 MINUTES) [route: {}->http://10.0.0.146:8181][total available: 1; route allocated: 1 of 100; total allocated: 1 of 100]\n",
      "18:14:57.101 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000050 endpoint leased [route: {}->http://10.0.0.146:8181][total available: 0; route allocated: 1 of 100; total allocated: 1 of 100]\n",
      "18:14:57.102 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"[read] I/O error: Read timed out\"\n",
      "18:14:57.102 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000050 acquired ep-0000000050\n",
      "18:14:57.102 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000050 acquired endpoint ep-0000000050\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000050 executing GET /v1/namespaces/ncentral/tables/devices?snapshots=all HTTP/1.1\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- ex-0000000050 Cookie spec selected: strict\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000050 start execution ex-0000000050\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000050 executing exchange ex-0000000050 over http-outgoing-19\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> GET /v1/namespaces/ncentral/tables/devices?snapshots=all HTTP/1.1\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Accept: application/json\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Content-Type: application/json\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Accept-Encoding: gzip, x-gzip, deflate\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> X-Client-Git-Commit-Short: cbb8530\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> X-Client-Version: Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Host: 10.0.0.146:8181\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Connection: keep-alive\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> User-Agent: Apache-HttpClient/5.3.1 (Java/21.0.3)\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"GET /v1/namespaces/ncentral/tables/devices?snapshots=all HTTP/1.1[\\r][\\n]\"\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Accept: application/json[\\r][\\n]\"\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Content-Type: application/json[\\r][\\n]\"\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Accept-Encoding: gzip, x-gzip, deflate[\\r][\\n]\"\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"X-Client-Git-Commit-Short: cbb8530[\\r][\\n]\"\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"X-Client-Version: Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)[\\r][\\n]\"\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Host: 10.0.0.146:8181[\\r][\\n]\"\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Connection: keep-alive[\\r][\\n]\"\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"User-Agent: Apache-HttpClient/5.3.1 (Java/21.0.3)[\\r][\\n]\"\n",
      "18:14:57.103 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"[\\r][\\n]\"\n",
      "18:14:57.362 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"HTTP/1.1 200 OK[\\r][\\n]\"\n",
      "18:14:57.362 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Date: Wed, 26 Jun 2024 22:14:57 GMT[\\r][\\n]\"\n",
      "18:14:57.362 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Content-Type: application/json[\\r][\\n]\"\n",
      "18:14:57.362 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Vary: Accept-Encoding, User-Agent[\\r][\\n]\"\n",
      "18:14:57.362 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Content-Encoding: gzip[\\r][\\n]\"\n",
      "18:14:57.362 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Content-Length: 962[\\r][\\n]\"\n",
      "18:14:57.362 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Server: Jetty(9.4.51.v20230217)[\\r][\\n]\"\n",
      "18:14:57.362 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"[\\r][\\n]\"\n",
      "18:14:57.362 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"[0x1f][0xffffff8b][0x8][0x0][0x0][0x0][0x0][0x0][0x0][0x0][0xffffff9d]V[0xffffffdd]r[0xfffffff3]([0xc]}[0x17]_[0xffffff87][0xffffffc4][0xffffffc6][0xffffffff]y[0xffffff8e][0xffffffbd][0xffffffda][0xffffff9d][0xffffff8e][0xffffff87][0xffffff80][0xffffff9c][0xffffffb0][0xffffffc5][0xffffffe0][0x5][0xffffffdc]N[0xffffffbf]N[0xffffffde]}[0xffffff85][0xffffff9d][0xffffffd8]I[0xffffffda][0xffffffed][0x17]o/[0xffffffda]ZB[0x7]q8H[0xfffffffa][0xffffff8c]:[0xfffffff0]L0[0xffffffcf][0xffffff88]2[0xffffff9c]yit[0xffffffb4][0xffffff8f]\\[0xffffffca][0xfffffff6][0xffffffbb][0xffffff9d]b[0xffffffaf]p2[0xffffff83][0xffffff83][0xffffff9d][0xffffffe6][0xffffffa0][0xffffffbd]ej'[0xffffffe0]Mrp[0xffffffbb]k[0xffffffd0].[0xe]?[0xffffff84][0xffffffd2][0xffffffb6][0xffffffe5][0xffffffac][0xffffffac]H[0xffffff96][0x2]'[0x19][0x7]J[0xe][0xffffff94][0xffffffb5]$[0xffffffa5]yBy\"x[0xffffffce][0xffffffab][0xffffffed]5f[0xfffffffb][0xffffffb7][0xffffffc3]M6[0xfffffff3][0xffffffc6][0xffffffd1][0xfffffffe]3j[0xffffff8d][0xffffffed][0xffffff98]'o`[0xffffffdd][0xffffff98][0x1][0xffffffdd]D[0xffffff9e][0x1d][0x14][0xffffff90]a[0xffffff90][0x2][0x13][0xffffffa2]yKsHcr[0x10][0xffffff90][0xffffff90][0xffffffac][0x14])[0xfffffffe][0xffffffc7][0xffffff81][0xffffffe4],)[0xffffffca][0xffffff8c][0x1e]J.*[0xffffff84]\\q[0xffffff84][0xffffffb0][0xffffff9a]9O[0x1c][0xfffffffc]3[0x0]:[0xffffff89][0x1e][0xffffffba][0x3][0xffffffd8]h[0xffffff9f]\\[0x1c]C[0xffffff8f][0xffffffc9][0xffffff81] [0xffffff9d]C[[0xffffff99][0xffffffd4]Y[0xffffff92][0xffffff96]q\\f[0xffffffc5][0xffffffc5][0xffffffcf][0xffffff8d][0x1a]:MBz4[0xffffffdb]D|[0xffffffb0][0x16]w [0xffffff8e][0xffffff9f][0xffffffa0]c[0xffffffa3]9[0xffffffde]D[0xffffffd3][0x17]\"[0xfffffffc][0xfffffff5][0x19][0xfffffff9][0xffffff8f][0x1e]Bb[0xffffffde][0xe][0xffffffdc]GW[0xffffffdf]ue+A[0xffffff89]ia[0xffffffb0]`[0x16][0xffffff9a]ua=;\"lsef[0x13]YLXZ[0xffffffc0]5-S[0xe]67[0xffffffb8]R[0x1f][0xffffffa3][0xfffffff3]f[0x2][0xffffffa0]3[0x0][0x1f][0xffffff9c]7[0x1d][0xffffffd8][0x6][0xffffffcd][0xffffffff][0x1d][0xffffffae][0xffffffcc]Mp[0xfffffffa]5x[0xfffffffc]~z[0xfffffff7]l[0x6][0x10][0xffffffe0][0xffffffb8][0xffffff95][0xffffffbd]_[0xffffff95]|~[0x13][0x1e]n[0xffffffab][0xffffffe1]H[0xffffffb9]{>[0xffffffbe][0xfffffff8]6[0xffffffbe]Q[0xffffffec][0x0][0xffffffea]y[0xffffff94][0xfffffff2][0x11][0xffffffe5]y[0x2][0xffffffab]%T:n[0xfffffff0][0xfffffff6]@[0xffffffac][0xffffffa4][0xffffffb0][0xffffff9e]![0xffffffa4][0xfffffff6]`[[0xffffff86][0x9][0xffffffac]VA[0x12]/([0xffffffae][0xffffffe9][0xffffffad]9[0xfffffffc][0xffffff94][0xffffffc1][0xffffffc1][0x18][0x5]L/[0xffffffd1][0xffffff8b][\\n]\"\n",
      "18:14:57.362 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"[0xffffff83][0xffffffe4][0x1b]e[0xffffff8e]G<[0xffffff87][0xffffffd4][\\r][0xffffffbe]([0xffffffbb]\"[0xffffff8b]E[0xffffff8c][\\n]\"\n",
      "18:14:57.363 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"y[0xffffffd4][0xe][0xffffff9a][0xffffffce][0xffffff88][0x15]\\$[0xffffff8b] [0x3][0xffffffcd]+[0xffffff99]L[0x16]5j[0xfffffff0][0xffffffef][0xffffffc6][0xffffffbe][0xfffffffe][0xf]\"[0x17]I[0x1a][0xfffffff7][0xffffffb3][0x10][0x1e]#[0x17]1Z[0xffffffe8][0xffffff8c]G1[0x1a][0xffffffac]DF5[0xffffff83][0xffffff95]+`[0x16]5:[0xffffff89] +)X[0x4][0xffffffe9][0xffffffcc][0xffffffda][0xffffffd8][0xfffffffa]&v[0xffffffb0]([0xffffffc3]U[0xffffff89][0xffffffd3]E[0xffffff82][0xffffffce]cEm[0xffffffc2][0xffffffef]a[0xffffffc5]c[0xffffffa6][0xffffffc9][\\r][0xffffff80]Tj[0xffffff91][0xffffffe1][\\n]\"\n",
      "18:14:57.363 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \")[0xffffffd3]E[0xffffff82]n[0xffffffe8]{c[0xffffffb1][0xffffffb4]7fM[0x16][0xffffffe9][0xffffffb7][0x0]kk[\\n]\"\n",
      "18:14:57.363 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"][0xffffffb4][0xfffffff8][0x1c][0xffffff8b]/[0xffffffe7][0xffffff97][\\r][0xffffffd6][0xffffff9f][0xffffff96][\\r][\\n]\"\n",
      "18:14:57.363 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"[0xffffff9b]K[0xffffff8f][0xffffffed][0xfffffff5][0xffffffd2]0zf[0xffffffbd][0xc]eu[0xffffffb4]N[0xffffff9d][0xffffffe3][0xffffffd6]?7[0xffffff94][0x11]alYK[0xffffffc8]X`[0xffffffea][0xfffffffa][0x6][0x19][0xf]D[0xffffff8c][0x15]`[0xffffffe7][0xffffffd6]5[&[0xffffffec];[0xffffffe7]=8[0xffffffd6][0xffffff95][0x1e][0x10][0x1a][0xffffffbb]*[0xfffffff6]r[0xfffffff3][0xffffffae]C[0x1f][0xffffff8d]:[0xffffffd9]a[0xffffffed];[0xffffffa9][0xf]<%[0xffffffb7]06S[0xffffffe6]C3[0xffffff8f]iF[0xffffffe2][0xffffff82][0xffffffd0][0xffffffe2][0xffffff8f]$[0xffffffdb][0xffffffe7][0xfffffff9]>[0xffffff8e][0xffffffb7]4[0xffffffae][0xffffffeb]*N+[0xfffffffa]'[0xffffffae]~[0xffffffb7][0xffffffa8][0xfffffff0]-f[0xffffff8b][0xffffffad][0xffffffd9]o[0xffffffb9][0xffffffe9]z[0xb].[0xffffffbc]Wl[0xffffffba][0x2]8B[0xfffffffc]r^ [0xffffffa7]K[0xffffffd7][0xffffffd5][0xffffffac]w'[0xffffffe3][0xffffffc7][0xfffffffc][0xfffffff2][0xffffffb4]L[0xffffffb1]O[0x17]yYeY[0xffffff95]d[0xffffffe1][0xffffffed]Zh[0xffffffc7][0xffffffe4]:[0xffffff86][0xffffffb2][0xffffffc1][0xffffffbf][0xffffffbf][\\r][0xffffffb8][0xffffffea][0xffffffc8]2[0xffffffcd]O[0xffffffd1][0x19][0xfffffff7][0xffffffba][0xffffff86]\\[0xffffffa8][0xfffffffe]fn[0xfffffff8]=[0xffffffa8][0xffffffec][0x0][0xffffffe5][0xffffffdf][0xfffffff5][0xffffffdf]M[0x15]n[0xffffffe8]:f?F[\\n]\"\n",
      "18:14:57.363 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"[0xffffff91][0xffffffce][0xffffffeb] [0xffffffc3][0xfffffffa][0x1e]t[0xffffffa8]7[0xe][0x9]y[0xffffffdd][0xffffffe2][0xffffffe7]v[0x1c][0xffffff89][0xffffffc2][0xffffffac][0xffffffa3][0xffffffc8][0x5][0xffffffa3][0xffffffa8][0xffffffaa][0xffffff82][0xffffff86][0x9][0xffffff88][0x9][0xffffff81]4[0xffffff8f][0xfffffff3]\\+U[0xffffffb8][0xffffff90]([0xffffff99][0xffffffcd][0x16]8[0xffffffde]b[0xffffffb0][0xffffffe5]u:[[0xffffffc7]u[0xffffffc4][0xffffffc9]_[0xffffffe1][0xffffffbc][0xffffffb4][0xffffffce][0xffffffe2]<\\[0xffffffd8][0xffffff89]i|d7[0xffffff92][0xffffffe1]f[0xffffffd0][0xfffffffe][0x2][0xffffffe7][0xffffff8d][0xffffffc7][0xffffffbd][0x1f][0xffffffe1]&[0xffffffeb][0xffffffb7]p[0xffffff93][0xffffffeb]KZ[0x17]3([0xfffffff0]0;[0xffffffe2][0xffffffd9][0xffffffd1][0x1b]7[0xffffffed]=[0xffffffad][0xffffffb8]w\"[0xffffffff]LI[0xffffffff]q[0xffffffe7][0xffffffc4]k[0xffffffea][0xffffff98][0xffffff96]-[0xffffffb2]L[0xffffff94]t~[0xffffffd5],[0x1b][0xffffffae][0xffffff8f]|[0xffffffbd]6[0xffffff92][0xffffff90][0xffffff84][0xffffffe6]1[0xffffffd4]EB[0x0]OJ[0xffffffb2]X[0x1c]H[0xffffff9d][0xb] [0xffffff90][0xffffffa6]\"n+V[0xffffffa4]U[0xffffffbb]eo[0xffffffd6]<[0xffffff8c]u[0xffffffe1]q[0xffffff84]r[0xffffff87][0xffffff99][0xffffffc8][0xfffffff1][0xffffff89][0xffffffbe][0xffffffdc][0xffffffbd][0xffffffdb][0x7][0xffffffcf],[0x1f],p[0xffffffd3][0xffffffc4][0xfffffff8][0xffffffa3]\\~[0x16][[0xffffffd8][0xfffffffa]f[0xffffffb4]?[0xffffff8e]o[0x15][0xffffffef][0xffffffd5][0xffffffe8]V[0xffffffe2][0xffffffc7][0xffffffe7][0xfffffff9][0xfffffffc]/[0x2][0xffffff93][0xffffff8b][0xffffffac][0xfffffffb][0xb][0x0][0x0]\"\n",
      "18:14:57.363 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << HTTP/1.1 200 OK\n",
      "18:14:57.363 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Date: Wed, 26 Jun 2024 22:14:57 GMT\n",
      "18:14:57.363 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Content-Type: application/json\n",
      "18:14:57.363 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Vary: Accept-Encoding, User-Agent\n",
      "18:14:57.363 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Content-Encoding: gzip\n",
      "18:14:57.363 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Content-Length: 962\n",
      "18:14:57.363 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Server: Jetty(9.4.51.v20230217)\n",
      "18:14:57.363 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000050 connection can be kept alive for 3 MINUTES\n",
      "18:14:57.364 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000050 releasing valid endpoint\n",
      "18:14:57.364 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000050 releasing endpoint\n",
      "18:14:57.364 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000050 connection http-outgoing-19 can be kept alive for 3 MINUTES\n",
      "18:14:57.364 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000050 connection released [route: {}->http://10.0.0.146:8181][total available: 1; route allocated: 1 of 100; total allocated: 1 of 100]\n",
      "18:14:57.390 [Thread-2] INFO org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown -- \n",
      "Output: agent_version#2735, customer_id#2736L, customer_name#2737, description#2738, device_class#2739, device_class_label#2740, device_id#2741L, discovered_name#2742, interface_version#2743, is_probe#2744, last_logged_in_user#2745, license_mode#2746, long_name#2747, network_version#2748, os_id#2749, remote_control_uri#2750, site_name#2751, so_name#2752, source_uri#2753, state_status#2754, still_logged_in#2755, supported_os#2756, supported_os_label#2757, uri#2758\n",
      "         \n",
      "18:14:57.395 [Thread-2] INFO org.apache.iceberg.SnapshotScan -- Scanning table rest_catalog.ncentral.devices snapshot 537374665784481415 created at 2024-06-26T14:55:00.746+00:00 with filter true\n",
      "18:14:57.433 [Thread-2] DEBUG org.apache.iceberg.aws.s3.S3InputStream -- Seek with new stream for s3a://lakehouse/ncentral/devices/metadata/snap-537374665784481415-1-1250e961-e593-40db-95de-e33d0f8a638f.avro to offset 0\n",
      "18:14:57.444 [Thread-2] INFO org.apache.iceberg.BaseDistributedDataScan -- Planning file tasks locally for table rest_catalog.ncentral.devices\n",
      "18:14:57.445 [Thread-2] DEBUG org.apache.iceberg.aws.s3.S3InputStream -- Seek with new stream for s3a://lakehouse/ncentral/devices/metadata/1250e961-e593-40db-95de-e33d0f8a638f-m0.avro to offset 0\n",
      "18:14:57.453 [Thread-2] INFO org.apache.iceberg.metrics.LoggingMetricsReporter -- Received metrics report: ScanReport{tableName=rest_catalog.ncentral.devices, snapshotId=537374665784481415, filter=true, schemaId=0, projectedFieldIds=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], projectedFieldNames=[agent_version, customer_id, customer_name, description, device_class, device_class_label, device_id, discovered_name, interface_version, is_probe, last_logged_in_user, license_mode, long_name, network_version, os_id, remote_control_uri, site_name, so_name, source_uri, state_status, still_logged_in, supported_os, supported_os_label, uri], scanMetrics=ScanMetricsResult{totalPlanningDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.056339958S, count=1}, resultDataFiles=CounterResult{unit=COUNT, value=1}, resultDeleteFiles=CounterResult{unit=COUNT, value=0}, totalDataManifests=CounterResult{unit=COUNT, value=1}, totalDeleteManifests=CounterResult{unit=COUNT, value=0}, scannedDataManifests=CounterResult{unit=COUNT, value=1}, skippedDataManifests=CounterResult{unit=COUNT, value=0}, totalFileSizeInBytes=CounterResult{unit=BYTES, value=29405}, totalDeleteFileSizeInBytes=CounterResult{unit=BYTES, value=0}, skippedDataFiles=CounterResult{unit=COUNT, value=0}, skippedDeleteFiles=CounterResult{unit=COUNT, value=0}, scannedDeleteManifests=CounterResult{unit=COUNT, value=0}, skippedDeleteManifests=CounterResult{unit=COUNT, value=0}, indexedDeleteFiles=CounterResult{unit=COUNT, value=0}, equalityDeleteFiles=CounterResult{unit=COUNT, value=0}, positionalDeleteFiles=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.1, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82), app-id=local-1719428368459, engine-name=spark}}\n",
      "18:14:57.453 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000051 preparing request execution\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000051 target auth state: UNCHALLENGED\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.ProtocolExec -- ex-0000000051 proxy auth state: UNCHALLENGED\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.ConnectExec -- ex-0000000051 acquiring connection with route {}->http://10.0.0.146:8181\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000051 acquiring endpoint (3 MINUTES)\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000051 endpoint lease request (3 MINUTES) [route: {}->http://10.0.0.146:8181][total available: 1; route allocated: 1 of 100; total allocated: 1 of 100]\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000051 endpoint leased [route: {}->http://10.0.0.146:8181][total available: 0; route allocated: 1 of 100; total allocated: 1 of 100]\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ex-0000000051 acquired ep-0000000051\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ex-0000000051 acquired endpoint ep-0000000051\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000051 executing POST /v1/namespaces/ncentral/tables/devices/metrics HTTP/1.1\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.protocol.RequestAddCookies -- ex-0000000051 Cookie spec selected: strict\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000051 start execution ex-0000000051\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000051 executing exchange ex-0000000051 over http-outgoing-19\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> POST /v1/namespaces/ncentral/tables/devices/metrics HTTP/1.1\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Accept: application/json\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Content-Type: application/json\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Accept-Encoding: gzip, x-gzip, deflate\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> X-Client-Git-Commit-Short: cbb8530\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> X-Client-Version: Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Content-Length: 1676\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Host: 10.0.0.146:8181\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> Connection: keep-alive\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 >> User-Agent: Apache-HttpClient/5.3.1 (Java/21.0.3)\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"POST /v1/namespaces/ncentral/tables/devices/metrics HTTP/1.1[\\r][\\n]\"\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Accept: application/json[\\r][\\n]\"\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Content-Type: application/json[\\r][\\n]\"\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Accept-Encoding: gzip, x-gzip, deflate[\\r][\\n]\"\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"X-Client-Git-Commit-Short: cbb8530[\\r][\\n]\"\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"X-Client-Version: Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)[\\r][\\n]\"\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Content-Length: 1676[\\r][\\n]\"\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Host: 10.0.0.146:8181[\\r][\\n]\"\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"Connection: keep-alive[\\r][\\n]\"\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"User-Agent: Apache-HttpClient/5.3.1 (Java/21.0.3)[\\r][\\n]\"\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"[\\r][\\n]\"\n",
      "18:14:57.454 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 >> \"{\"report-type\":\"scan-report\",\"table-name\":\"rest_catalog.ncentral.devices\",\"snapshot-id\":537374665784481415,\"filter\":true,\"schema-id\":0,\"projected-field-ids\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24],\"projected-field-names\":[\"agent_version\",\"customer_id\",\"customer_name\",\"description\",\"device_class\",\"device_class_label\",\"device_id\",\"discovered_name\",\"interface_version\",\"is_probe\",\"last_logged_in_user\",\"license_mode\",\"long_name\",\"network_version\",\"os_id\",\"remote_control_uri\",\"site_name\",\"so_name\",\"source_uri\",\"state_status\",\"still_logged_in\",\"supported_os\",\"supported_os_label\",\"uri\"],\"metrics\":{\"total-planning-duration\":{\"count\":1,\"time-unit\":\"nanoseconds\",\"total-duration\":56339958},\"result-data-files\":{\"unit\":\"count\",\"value\":1},\"result-delete-files\":{\"unit\":\"count\",\"value\":0},\"total-data-manifests\":{\"unit\":\"count\",\"value\":1},\"total-delete-manifests\":{\"unit\":\"count\",\"value\":0},\"scanned-data-manifests\":{\"unit\":\"count\",\"value\":1},\"skipped-data-manifests\":{\"unit\":\"count\",\"value\":0},\"total-file-size-in-bytes\":{\"unit\":\"bytes\",\"value\":29405},\"total-delete-file-size-in-bytes\":{\"unit\":\"bytes\",\"value\":0},\"skipped-data-files\":{\"unit\":\"count\",\"value\":0},\"skipped-delete-files\":{\"unit\":\"count\",\"value\":0},\"scanned-delete-manifests\":{\"unit\":\"count\",\"value\":0},\"skipped-delete-manifests\":{\"unit\":\"count\",\"value\":0},\"indexed-delete-files\":{\"unit\":\"count\",\"value\":0},\"equality-delete-files\":{\"unit\":\"count\",\"value\":0},\"positional-delete-files\":{\"unit\":\"count\",\"value\":0}},\"metadata\":{\"engine-version\":\"3.5.1\",\"iceberg-version\":\"Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)\",\"app-id\":\"local-1719428368459\",\"engine-name\":\"spark\"}}\"\n",
      "18:14:57.457 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"HTTP/1.1 200 OK[\\r][\\n]\"\n",
      "18:14:57.458 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Date: Wed, 26 Jun 2024 22:14:57 GMT[\\r][\\n]\"\n",
      "18:14:57.458 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Content-Type: application/json[\\r][\\n]\"\n",
      "18:14:57.458 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Content-Length: 0[\\r][\\n]\"\n",
      "18:14:57.458 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"Server: Jetty(9.4.51.v20230217)[\\r][\\n]\"\n",
      "18:14:57.458 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.wire -- http-outgoing-19 << \"[\\r][\\n]\"\n",
      "18:14:57.458 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << HTTP/1.1 200 OK\n",
      "18:14:57.458 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Date: Wed, 26 Jun 2024 22:14:57 GMT\n",
      "18:14:57.458 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Content-Type: application/json\n",
      "18:14:57.458 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Content-Length: 0\n",
      "18:14:57.458 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.headers -- http-outgoing-19 << Server: Jetty(9.4.51.v20230217)\n",
      "18:14:57.458 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.MainClientExec -- ex-0000000051 connection can be kept alive for 3 MINUTES\n",
      "18:14:57.458 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.classic.InternalHttpClient -- ep-0000000051 releasing valid endpoint\n",
      "18:14:57.458 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000051 releasing endpoint\n",
      "18:14:57.458 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000051 connection http-outgoing-19 can be kept alive for 3 MINUTES\n",
      "18:14:57.458 [Thread-2] DEBUG org.apache.iceberg.shaded.org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager -- ep-0000000051 connection released [route: {}->http://10.0.0.146:8181][total available: 1; route allocated: 1 of 100; total allocated: 1 of 100]\n",
      "18:14:57.458 [Thread-2] DEBUG org.apache.iceberg.spark.source.SparkPartitioningAwareScan -- Planned 1 task group(s) without data grouping for table rest_catalog.ncentral.devices\n",
      "18:14:57.458 [Thread-2] INFO org.apache.iceberg.spark.source.SparkPartitioningAwareScan -- Reporting UnknownPartitioning with 1 partition(s) for table rest_catalog.ncentral.devices\n",
      "18:14:57.461 [Thread-2] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_103 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)\n",
      "18:14:57.461 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_103 locally took 0 ms\n",
      "18:14:57.461 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_103 without replication took 0 ms\n",
      "18:14:57.465 [Thread-2] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_103_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.0 MiB)\n",
      "18:14:57.465 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_103_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:57.465 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Added broadcast_103_piece0 in memory on 10.0.0.146:57286 (size: 3.8 KiB, free: 434.3 MiB)\n",
      "18:14:57.465 [Thread-2] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_103_piece0\n",
      "18:14:57.465 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_103_piece0\n",
      "18:14:57.465 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_103_piece0 locally took 0 ms\n",
      "18:14:57.465 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_103_piece0 without replication took 0 ms\n",
      "18:14:57.465 [Thread-2] INFO org.apache.spark.SparkContext -- Created broadcast 103 from broadcast at SparkBatch.java:79\n",
      "18:14:57.466 [Thread-2] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_104 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)\n",
      "18:14:57.466 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_104 locally took 0 ms\n",
      "18:14:57.466 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_104 without replication took 0 ms\n",
      "18:14:57.468 [Thread-2] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_104_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.0 MiB)\n",
      "18:14:57.468 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_104_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:57.468 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Added broadcast_104_piece0 in memory on 10.0.0.146:57286 (size: 3.8 KiB, free: 434.3 MiB)\n",
      "18:14:57.468 [Thread-2] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_104_piece0\n",
      "18:14:57.468 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_104_piece0\n",
      "18:14:57.468 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_104_piece0 locally took 0 ms\n",
      "18:14:57.468 [Thread-2] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_104_piece0 without replication took 0 ms\n",
      "18:14:57.468 [Thread-2] INFO org.apache.spark.SparkContext -- Created broadcast 104 from broadcast at SparkBatch.java:79\n",
      "18:14:57.478 [Thread-2] DEBUG org.apache.spark.sql.execution.WholeStageCodegenExec -- \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private int columnartorow_batchIdx_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] columnartorow_mutableStateArray_3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 011 */   private org.apache.spark.sql.vectorized.ColumnarBatch[] columnartorow_mutableStateArray_1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];\n",
      "/* 012 */   private scala.collection.Iterator[] columnartorow_mutableStateArray_0 = new scala.collection.Iterator[1];\n",
      "/* 013 */   private org.apache.spark.sql.vectorized.ColumnVector[] columnartorow_mutableStateArray_2 = new org.apache.spark.sql.vectorized.ColumnVector[24];\n",
      "/* 014 */\n",
      "/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 016 */     this.references = references;\n",
      "/* 017 */   }\n",
      "/* 018 */\n",
      "/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 020 */     partitionIndex = index;\n",
      "/* 021 */     this.inputs = inputs;\n",
      "/* 022 */     columnartorow_mutableStateArray_0[0] = inputs[0];\n",
      "/* 023 */\n",
      "/* 024 */     columnartorow_mutableStateArray_3[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(24, 640);\n",
      "/* 025 */     columnartorow_mutableStateArray_3[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(24, 768);\n",
      "/* 026 */\n",
      "/* 027 */   }\n",
      "/* 028 */\n",
      "/* 029 */   private void columnartorow_nextBatch_0() throws java.io.IOException {\n",
      "/* 030 */     if (columnartorow_mutableStateArray_0[0].hasNext()) {\n",
      "/* 031 */       columnartorow_mutableStateArray_1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)columnartorow_mutableStateArray_0[0].next();\n",
      "/* 032 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numInputBatches */).add(1);\n",
      "/* 033 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(columnartorow_mutableStateArray_1[0].numRows());\n",
      "/* 034 */       columnartorow_batchIdx_0 = 0;\n",
      "/* 035 */       columnartorow_mutableStateArray_2[0] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(0);\n",
      "/* 036 */       columnartorow_mutableStateArray_2[1] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(1);\n",
      "/* 037 */       columnartorow_mutableStateArray_2[2] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(2);\n",
      "/* 038 */       columnartorow_mutableStateArray_2[3] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(3);\n",
      "/* 039 */       columnartorow_mutableStateArray_2[4] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(4);\n",
      "/* 040 */       columnartorow_mutableStateArray_2[5] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(5);\n",
      "/* 041 */       columnartorow_mutableStateArray_2[6] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(6);\n",
      "/* 042 */       columnartorow_mutableStateArray_2[7] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(7);\n",
      "/* 043 */       columnartorow_mutableStateArray_2[8] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(8);\n",
      "/* 044 */       columnartorow_mutableStateArray_2[9] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(9);\n",
      "/* 045 */       columnartorow_mutableStateArray_2[10] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(10);\n",
      "/* 046 */       columnartorow_mutableStateArray_2[11] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(11);\n",
      "/* 047 */       columnartorow_mutableStateArray_2[12] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(12);\n",
      "/* 048 */       columnartorow_mutableStateArray_2[13] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(13);\n",
      "/* 049 */       columnartorow_mutableStateArray_2[14] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(14);\n",
      "/* 050 */       columnartorow_mutableStateArray_2[15] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(15);\n",
      "/* 051 */       columnartorow_mutableStateArray_2[16] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(16);\n",
      "/* 052 */       columnartorow_mutableStateArray_2[17] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(17);\n",
      "/* 053 */       columnartorow_mutableStateArray_2[18] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(18);\n",
      "/* 054 */       columnartorow_mutableStateArray_2[19] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(19);\n",
      "/* 055 */       columnartorow_mutableStateArray_2[20] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(20);\n",
      "/* 056 */       columnartorow_mutableStateArray_2[21] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(21);\n",
      "/* 057 */       columnartorow_mutableStateArray_2[22] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(22);\n",
      "/* 058 */       columnartorow_mutableStateArray_2[23] = (org.apache.spark.sql.vectorized.ColumnVector) columnartorow_mutableStateArray_1[0].column(23);\n",
      "/* 059 */\n",
      "/* 060 */     }\n",
      "/* 061 */   }\n",
      "/* 062 */\n",
      "/* 063 */   protected void processNext() throws java.io.IOException {\n",
      "/* 064 */     if (columnartorow_mutableStateArray_1[0] == null) {\n",
      "/* 065 */       columnartorow_nextBatch_0();\n",
      "/* 066 */     }\n",
      "/* 067 */     while ( columnartorow_mutableStateArray_1[0] != null) {\n",
      "/* 068 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();\n",
      "/* 069 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;\n",
      "/* 070 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {\n",
      "/* 071 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;\n",
      "/* 072 */         // common sub-expressions\n",
      "/* 073 */\n",
      "/* 074 */         boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 075 */         UTF8String columnartorow_value_0 = columnartorow_isNull_0 ? null : (columnartorow_mutableStateArray_2[0].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 076 */         UTF8String project_value_0;\n",
      "/* 077 */         if (columnartorow_isNull_0) {\n",
      "/* 078 */           project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 079 */         } else {\n",
      "/* 080 */           project_value_0 = columnartorow_value_0;\n",
      "/* 081 */         }\n",
      "/* 082 */         boolean columnartorow_isNull_1 = columnartorow_mutableStateArray_2[1].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 083 */         long columnartorow_value_1 = columnartorow_isNull_1 ? -1L : (columnartorow_mutableStateArray_2[1].getLong(columnartorow_rowIdx_0));\n",
      "/* 084 */         UTF8String project_value_2;\n",
      "/* 085 */         if (columnartorow_isNull_1) {\n",
      "/* 086 */           project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 087 */         } else {\n",
      "/* 088 */           project_value_2 = UTF8String.fromString(String.valueOf(columnartorow_value_1));\n",
      "/* 089 */         }\n",
      "/* 090 */         boolean columnartorow_isNull_2 = columnartorow_mutableStateArray_2[2].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 091 */         UTF8String columnartorow_value_2 = columnartorow_isNull_2 ? null : (columnartorow_mutableStateArray_2[2].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 092 */         UTF8String project_value_4;\n",
      "/* 093 */         if (columnartorow_isNull_2) {\n",
      "/* 094 */           project_value_4 = UTF8String.fromString(\"NULL\");\n",
      "/* 095 */         } else {\n",
      "/* 096 */           project_value_4 = columnartorow_value_2;\n",
      "/* 097 */         }\n",
      "/* 098 */         boolean columnartorow_isNull_3 = columnartorow_mutableStateArray_2[3].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 099 */         UTF8String columnartorow_value_3 = columnartorow_isNull_3 ? null : (columnartorow_mutableStateArray_2[3].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 100 */         UTF8String project_value_6;\n",
      "/* 101 */         if (columnartorow_isNull_3) {\n",
      "/* 102 */           project_value_6 = UTF8String.fromString(\"NULL\");\n",
      "/* 103 */         } else {\n",
      "/* 104 */           project_value_6 = columnartorow_value_3;\n",
      "/* 105 */         }\n",
      "/* 106 */         boolean columnartorow_isNull_4 = columnartorow_mutableStateArray_2[4].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 107 */         UTF8String columnartorow_value_4 = columnartorow_isNull_4 ? null : (columnartorow_mutableStateArray_2[4].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 108 */         UTF8String project_value_8;\n",
      "/* 109 */         if (columnartorow_isNull_4) {\n",
      "/* 110 */           project_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 111 */         } else {\n",
      "/* 112 */           project_value_8 = columnartorow_value_4;\n",
      "/* 113 */         }\n",
      "/* 114 */         boolean columnartorow_isNull_5 = columnartorow_mutableStateArray_2[5].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 115 */         UTF8String columnartorow_value_5 = columnartorow_isNull_5 ? null : (columnartorow_mutableStateArray_2[5].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 116 */         UTF8String project_value_10;\n",
      "/* 117 */         if (columnartorow_isNull_5) {\n",
      "/* 118 */           project_value_10 = UTF8String.fromString(\"NULL\");\n",
      "/* 119 */         } else {\n",
      "/* 120 */           project_value_10 = columnartorow_value_5;\n",
      "/* 121 */         }\n",
      "/* 122 */         boolean columnartorow_isNull_6 = columnartorow_mutableStateArray_2[6].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 123 */         long columnartorow_value_6 = columnartorow_isNull_6 ? -1L : (columnartorow_mutableStateArray_2[6].getLong(columnartorow_rowIdx_0));\n",
      "/* 124 */         UTF8String project_value_12;\n",
      "/* 125 */         if (columnartorow_isNull_6) {\n",
      "/* 126 */           project_value_12 = UTF8String.fromString(\"NULL\");\n",
      "/* 127 */         } else {\n",
      "/* 128 */           project_value_12 = UTF8String.fromString(String.valueOf(columnartorow_value_6));\n",
      "/* 129 */         }\n",
      "/* 130 */         boolean columnartorow_isNull_7 = columnartorow_mutableStateArray_2[7].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 131 */         UTF8String columnartorow_value_7 = columnartorow_isNull_7 ? null : (columnartorow_mutableStateArray_2[7].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 132 */         UTF8String project_value_14;\n",
      "/* 133 */         if (columnartorow_isNull_7) {\n",
      "/* 134 */           project_value_14 = UTF8String.fromString(\"NULL\");\n",
      "/* 135 */         } else {\n",
      "/* 136 */           project_value_14 = columnartorow_value_7;\n",
      "/* 137 */         }\n",
      "/* 138 */         boolean columnartorow_isNull_8 = columnartorow_mutableStateArray_2[8].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 139 */         UTF8String columnartorow_value_8 = columnartorow_isNull_8 ? null : (columnartorow_mutableStateArray_2[8].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 140 */         UTF8String project_value_16;\n",
      "/* 141 */         if (columnartorow_isNull_8) {\n",
      "/* 142 */           project_value_16 = UTF8String.fromString(\"NULL\");\n",
      "/* 143 */         } else {\n",
      "/* 144 */           project_value_16 = columnartorow_value_8;\n",
      "/* 145 */         }\n",
      "/* 146 */         boolean columnartorow_isNull_9 = columnartorow_mutableStateArray_2[9].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 147 */         boolean columnartorow_value_9 = columnartorow_isNull_9 ? false : (columnartorow_mutableStateArray_2[9].getBoolean(columnartorow_rowIdx_0));\n",
      "/* 148 */         UTF8String project_value_18;\n",
      "/* 149 */         if (columnartorow_isNull_9) {\n",
      "/* 150 */           project_value_18 = UTF8String.fromString(\"NULL\");\n",
      "/* 151 */         } else {\n",
      "/* 152 */           project_value_18 = UTF8String.fromString(String.valueOf(columnartorow_value_9));\n",
      "/* 153 */         }\n",
      "/* 154 */         boolean columnartorow_isNull_10 = columnartorow_mutableStateArray_2[10].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 155 */         UTF8String columnartorow_value_10 = columnartorow_isNull_10 ? null : (columnartorow_mutableStateArray_2[10].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 156 */         UTF8String project_value_20;\n",
      "/* 157 */         if (columnartorow_isNull_10) {\n",
      "/* 158 */           project_value_20 = UTF8String.fromString(\"NULL\");\n",
      "/* 159 */         } else {\n",
      "/* 160 */           project_value_20 = columnartorow_value_10;\n",
      "/* 161 */         }\n",
      "/* 162 */         boolean columnartorow_isNull_11 = columnartorow_mutableStateArray_2[11].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 163 */         UTF8String columnartorow_value_11 = columnartorow_isNull_11 ? null : (columnartorow_mutableStateArray_2[11].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 164 */         UTF8String project_value_22;\n",
      "/* 165 */         if (columnartorow_isNull_11) {\n",
      "/* 166 */           project_value_22 = UTF8String.fromString(\"NULL\");\n",
      "/* 167 */         } else {\n",
      "/* 168 */           project_value_22 = columnartorow_value_11;\n",
      "/* 169 */         }\n",
      "/* 170 */         boolean columnartorow_isNull_12 = columnartorow_mutableStateArray_2[12].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 171 */         UTF8String columnartorow_value_12 = columnartorow_isNull_12 ? null : (columnartorow_mutableStateArray_2[12].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 172 */         UTF8String project_value_24;\n",
      "/* 173 */         if (columnartorow_isNull_12) {\n",
      "/* 174 */           project_value_24 = UTF8String.fromString(\"NULL\");\n",
      "/* 175 */         } else {\n",
      "/* 176 */           project_value_24 = columnartorow_value_12;\n",
      "/* 177 */         }\n",
      "/* 178 */         boolean columnartorow_isNull_13 = columnartorow_mutableStateArray_2[13].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 179 */         UTF8String columnartorow_value_13 = columnartorow_isNull_13 ? null : (columnartorow_mutableStateArray_2[13].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 180 */         UTF8String project_value_26;\n",
      "/* 181 */         if (columnartorow_isNull_13) {\n",
      "/* 182 */           project_value_26 = UTF8String.fromString(\"NULL\");\n",
      "/* 183 */         } else {\n",
      "/* 184 */           project_value_26 = columnartorow_value_13;\n",
      "/* 185 */         }\n",
      "/* 186 */         boolean columnartorow_isNull_14 = columnartorow_mutableStateArray_2[14].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 187 */         UTF8String columnartorow_value_14 = columnartorow_isNull_14 ? null : (columnartorow_mutableStateArray_2[14].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 188 */         UTF8String project_value_28;\n",
      "/* 189 */         if (columnartorow_isNull_14) {\n",
      "/* 190 */           project_value_28 = UTF8String.fromString(\"NULL\");\n",
      "/* 191 */         } else {\n",
      "/* 192 */           project_value_28 = columnartorow_value_14;\n",
      "/* 193 */         }\n",
      "/* 194 */         boolean columnartorow_isNull_15 = columnartorow_mutableStateArray_2[15].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 195 */         UTF8String columnartorow_value_15 = columnartorow_isNull_15 ? null : (columnartorow_mutableStateArray_2[15].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 196 */         UTF8String project_value_30;\n",
      "/* 197 */         if (columnartorow_isNull_15) {\n",
      "/* 198 */           project_value_30 = UTF8String.fromString(\"NULL\");\n",
      "/* 199 */         } else {\n",
      "/* 200 */           project_value_30 = columnartorow_value_15;\n",
      "/* 201 */         }\n",
      "/* 202 */         boolean columnartorow_isNull_16 = columnartorow_mutableStateArray_2[16].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 203 */         UTF8String columnartorow_value_16 = columnartorow_isNull_16 ? null : (columnartorow_mutableStateArray_2[16].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 204 */         UTF8String project_value_32;\n",
      "/* 205 */         if (columnartorow_isNull_16) {\n",
      "/* 206 */           project_value_32 = UTF8String.fromString(\"NULL\");\n",
      "/* 207 */         } else {\n",
      "/* 208 */           project_value_32 = columnartorow_value_16;\n",
      "/* 209 */         }\n",
      "/* 210 */         boolean columnartorow_isNull_17 = columnartorow_mutableStateArray_2[17].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 211 */         UTF8String columnartorow_value_17 = columnartorow_isNull_17 ? null : (columnartorow_mutableStateArray_2[17].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 212 */         UTF8String project_value_34;\n",
      "/* 213 */         if (columnartorow_isNull_17) {\n",
      "/* 214 */           project_value_34 = UTF8String.fromString(\"NULL\");\n",
      "/* 215 */         } else {\n",
      "/* 216 */           project_value_34 = columnartorow_value_17;\n",
      "/* 217 */         }\n",
      "/* 218 */         boolean columnartorow_isNull_18 = columnartorow_mutableStateArray_2[18].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 219 */         UTF8String columnartorow_value_18 = columnartorow_isNull_18 ? null : (columnartorow_mutableStateArray_2[18].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 220 */         UTF8String project_value_36;\n",
      "/* 221 */         if (columnartorow_isNull_18) {\n",
      "/* 222 */           project_value_36 = UTF8String.fromString(\"NULL\");\n",
      "/* 223 */         } else {\n",
      "/* 224 */           project_value_36 = columnartorow_value_18;\n",
      "/* 225 */         }\n",
      "/* 226 */         boolean columnartorow_isNull_19 = columnartorow_mutableStateArray_2[19].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 227 */         UTF8String columnartorow_value_19 = columnartorow_isNull_19 ? null : (columnartorow_mutableStateArray_2[19].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 228 */         UTF8String project_value_38;\n",
      "/* 229 */         if (columnartorow_isNull_19) {\n",
      "/* 230 */           project_value_38 = UTF8String.fromString(\"NULL\");\n",
      "/* 231 */         } else {\n",
      "/* 232 */           project_value_38 = columnartorow_value_19;\n",
      "/* 233 */         }\n",
      "/* 234 */         boolean columnartorow_isNull_20 = columnartorow_mutableStateArray_2[20].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 235 */         boolean columnartorow_value_20 = columnartorow_isNull_20 ? false : (columnartorow_mutableStateArray_2[20].getBoolean(columnartorow_rowIdx_0));\n",
      "/* 236 */         UTF8String project_value_40;\n",
      "/* 237 */         if (columnartorow_isNull_20) {\n",
      "/* 238 */           project_value_40 = UTF8String.fromString(\"NULL\");\n",
      "/* 239 */         } else {\n",
      "/* 240 */           project_value_40 = UTF8String.fromString(String.valueOf(columnartorow_value_20));\n",
      "/* 241 */         }\n",
      "/* 242 */         boolean columnartorow_isNull_21 = columnartorow_mutableStateArray_2[21].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 243 */         UTF8String columnartorow_value_21 = columnartorow_isNull_21 ? null : (columnartorow_mutableStateArray_2[21].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 244 */         UTF8String project_value_42;\n",
      "/* 245 */         if (columnartorow_isNull_21) {\n",
      "/* 246 */           project_value_42 = UTF8String.fromString(\"NULL\");\n",
      "/* 247 */         } else {\n",
      "/* 248 */           project_value_42 = columnartorow_value_21;\n",
      "/* 249 */         }\n",
      "/* 250 */         boolean columnartorow_isNull_22 = columnartorow_mutableStateArray_2[22].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 251 */         UTF8String columnartorow_value_22 = columnartorow_isNull_22 ? null : (columnartorow_mutableStateArray_2[22].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 252 */         UTF8String project_value_44;\n",
      "/* 253 */         if (columnartorow_isNull_22) {\n",
      "/* 254 */           project_value_44 = UTF8String.fromString(\"NULL\");\n",
      "/* 255 */         } else {\n",
      "/* 256 */           project_value_44 = columnartorow_value_22;\n",
      "/* 257 */         }\n",
      "/* 258 */         boolean columnartorow_isNull_23 = columnartorow_mutableStateArray_2[23].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 259 */         UTF8String columnartorow_value_23 = columnartorow_isNull_23 ? null : (columnartorow_mutableStateArray_2[23].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 260 */         UTF8String project_value_46;\n",
      "/* 261 */         if (columnartorow_isNull_23) {\n",
      "/* 262 */           project_value_46 = UTF8String.fromString(\"NULL\");\n",
      "/* 263 */         } else {\n",
      "/* 264 */           project_value_46 = columnartorow_value_23;\n",
      "/* 265 */         }\n",
      "/* 266 */         columnartorow_mutableStateArray_3[1].reset();\n",
      "/* 267 */\n",
      "/* 268 */         columnartorow_mutableStateArray_3[1].write(0, project_value_0);\n",
      "/* 269 */\n",
      "/* 270 */         columnartorow_mutableStateArray_3[1].write(1, project_value_2);\n",
      "/* 271 */\n",
      "/* 272 */         columnartorow_mutableStateArray_3[1].write(2, project_value_4);\n",
      "/* 273 */\n",
      "/* 274 */         columnartorow_mutableStateArray_3[1].write(3, project_value_6);\n",
      "/* 275 */\n",
      "/* 276 */         columnartorow_mutableStateArray_3[1].write(4, project_value_8);\n",
      "/* 277 */\n",
      "/* 278 */         columnartorow_mutableStateArray_3[1].write(5, project_value_10);\n",
      "/* 279 */\n",
      "/* 280 */         columnartorow_mutableStateArray_3[1].write(6, project_value_12);\n",
      "/* 281 */\n",
      "/* 282 */         columnartorow_mutableStateArray_3[1].write(7, project_value_14);\n",
      "/* 283 */\n",
      "/* 284 */         columnartorow_mutableStateArray_3[1].write(8, project_value_16);\n",
      "/* 285 */\n",
      "/* 286 */         columnartorow_mutableStateArray_3[1].write(9, project_value_18);\n",
      "/* 287 */\n",
      "/* 288 */         columnartorow_mutableStateArray_3[1].write(10, project_value_20);\n",
      "/* 289 */\n",
      "/* 290 */         columnartorow_mutableStateArray_3[1].write(11, project_value_22);\n",
      "/* 291 */\n",
      "/* 292 */         columnartorow_mutableStateArray_3[1].write(12, project_value_24);\n",
      "/* 293 */\n",
      "/* 294 */         columnartorow_mutableStateArray_3[1].write(13, project_value_26);\n",
      "/* 295 */\n",
      "/* 296 */         columnartorow_mutableStateArray_3[1].write(14, project_value_28);\n",
      "/* 297 */\n",
      "/* 298 */         columnartorow_mutableStateArray_3[1].write(15, project_value_30);\n",
      "/* 299 */\n",
      "/* 300 */         columnartorow_mutableStateArray_3[1].write(16, project_value_32);\n",
      "/* 301 */\n",
      "/* 302 */         columnartorow_mutableStateArray_3[1].write(17, project_value_34);\n",
      "/* 303 */\n",
      "/* 304 */         columnartorow_mutableStateArray_3[1].write(18, project_value_36);\n",
      "/* 305 */\n",
      "/* 306 */         columnartorow_mutableStateArray_3[1].write(19, project_value_38);\n",
      "/* 307 */\n",
      "/* 308 */         columnartorow_mutableStateArray_3[1].write(20, project_value_40);\n",
      "/* 309 */\n",
      "/* 310 */         columnartorow_mutableStateArray_3[1].write(21, project_value_42);\n",
      "/* 311 */\n",
      "/* 312 */         columnartorow_mutableStateArray_3[1].write(22, project_value_44);\n",
      "/* 313 */\n",
      "/* 314 */         columnartorow_mutableStateArray_3[1].write(23, project_value_46);\n",
      "/* 315 */         append((columnartorow_mutableStateArray_3[1].getRow()));\n",
      "/* 316 */         if (shouldStop()) { columnartorow_batchIdx_0 = columnartorow_rowIdx_0 + 1; return; }\n",
      "/* 317 */       }\n",
      "/* 318 */       columnartorow_batchIdx_0 = columnartorow_numRows_0;\n",
      "/* 319 */       columnartorow_mutableStateArray_1[0] = null;\n",
      "/* 320 */       columnartorow_nextBatch_0();\n",
      "/* 321 */     }\n",
      "/* 322 */   }\n",
      "/* 323 */\n",
      "/* 324 */ }\n",
      "\n",
      "18:14:57.481 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner -- Cleaning indylambda closure: $anonfun$doExecuteColumnar$1\n",
      "18:14:57.487 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner --  +++ indylambda closure ($anonfun$doExecuteColumnar$1) is now cleaned +++\n",
      "18:14:57.489 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner -- Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "18:14:57.490 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner --  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "18:14:57.490 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner -- Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "18:14:57.492 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner --  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "18:14:57.492 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner -- Cleaning indylambda closure: $anonfun$runJob$5\n",
      "18:14:57.494 [Thread-2] DEBUG org.apache.spark.util.ClosureCleaner --  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "18:14:57.494 [Thread-2] INFO org.apache.spark.SparkContext -- Starting job: showString at <unknown>:0\n",
      "18:14:57.494 [Thread-2] DEBUG org.apache.spark.scheduler.DAGScheduler -- eagerlyComputePartitionsForRddAndAncestors for RDD 217 took 0.000069 seconds\n",
      "18:14:57.495 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- Merging stage rdd profiles: Set()\n",
      "18:14:57.495 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Got job 65 (showString at <unknown>:0) with 1 output partitions\n",
      "18:14:57.495 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Final stage: ResultStage 95 (showString at <unknown>:0)\n",
      "18:14:57.495 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Parents of final stage: List()\n",
      "18:14:57.495 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Missing parents: List()\n",
      "18:14:57.495 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- submitStage(ResultStage 95 (name=showString at <unknown>:0;jobs=65))\n",
      "18:14:57.495 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- missing: List()\n",
      "18:14:57.495 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Submitting ResultStage 95 (MapPartitionsRDD[217] at showString at <unknown>:0), which has no missing parents\n",
      "18:14:57.495 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- submitMissingTasks(ResultStage 95)\n",
      "18:14:57.496 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_105 stored as values in memory (estimated size 27.6 KiB, free 434.0 MiB)\n",
      "18:14:57.496 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_105 locally took 0 ms\n",
      "18:14:57.496 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_105 without replication took 0 ms\n",
      "18:14:57.497 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore -- Block broadcast_105_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.0 MiB)\n",
      "18:14:57.497 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_105_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:57.497 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Added broadcast_105_piece0 in memory on 10.0.0.146:57286 (size: 7.4 KiB, free: 434.3 MiB)\n",
      "18:14:57.497 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_105_piece0\n",
      "18:14:57.497 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_105_piece0\n",
      "18:14:57.497 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Put block broadcast_105_piece0 locally took 0 ms\n",
      "18:14:57.497 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager -- Putting block broadcast_105_piece0 without replication took 0 ms\n",
      "18:14:57.497 [dag-scheduler-event-loop] INFO org.apache.spark.SparkContext -- Created broadcast 105 from broadcast at DAGScheduler.scala:1585\n",
      "18:14:57.497 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Submitting 1 missing tasks from ResultStage 95 (MapPartitionsRDD[217] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "18:14:57.497 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl -- Adding task set 95.0 with 1 tasks resource profile 0\n",
      "18:14:57.498 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager -- Epoch for TaskSet 95.0: 30\n",
      "18:14:57.498 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager -- Adding pending tasks took 0 ms\n",
      "18:14:57.498 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager -- Valid locality levels for TaskSet 95.0: NO_PREF, ANY\n",
      "18:14:57.498 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl -- parentName: , name: TaskSet_95.0, runningTasks: 0\n",
      "18:14:57.498 [dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager -- Starting task 0.0 in stage 95.0 (TID 65) (10.0.0.146, executor driver, partition 0, PROCESS_LOCAL, 14634 bytes) \n",
      "18:14:57.499 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] INFO org.apache.spark.executor.Executor -- Running task 0.0 in stage 95.0 (TID 65)\n",
      "18:14:57.500 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller -- stageTCMP: (95, 0) -> 1\n",
      "18:14:57.500 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.spark.storage.BlockManager -- Getting local block broadcast_105\n",
      "18:14:57.500 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.spark.storage.BlockManager -- Level for block broadcast_105 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "18:14:57.502 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.spark.storage.BlockManager -- Getting local block broadcast_104\n",
      "18:14:57.502 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.spark.storage.BlockManager -- Level for block broadcast_104 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "18:14:57.502 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.spark.source.BatchDataReader -- Reading 1 file split(s) for table rest_catalog.ncentral.devices\n",
      "18:14:57.502 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.spark.source.BatchDataReader -- Opening data file s3a://lakehouse/ncentral/devices/data/00000-39-52c56d33-aefc-46d2-8a76-946277cc9425-0-00001.parquet\n",
      "18:14:57.502 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader -- File length 29405\n",
      "18:14:57.502 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader -- reading footer index at 29397\n",
      "18:14:57.502 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.aws.s3.S3InputStream -- Seek with new stream for s3a://lakehouse/ncentral/devices/data/00000-39-52c56d33-aefc-46d2-8a76-946277cc9425-0-00001.parquet to offset 29397\n",
      "18:14:57.509 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader -- read footer length: 4899, footer index: 24498\n",
      "18:14:57.509 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.aws.s3.S3InputStream -- Seek with new stream for s3a://lakehouse/ncentral/devices/data/00000-39-52c56d33-aefc-46d2-8a76-946277cc9425-0-00001.parquet to offset 24498\n",
      "18:14:57.515 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileReader -- Finished to read all footer bytes.\n",
      "18:14:57.515 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.format.converter.ParquetMetadataConverter -- FileMetaData(version:1, schema:[SchemaElement(name:table, num_children:24), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:agent_version, converted_type:UTF8, field_id:1, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:INT64, repetition_type:OPTIONAL, name:customer_id, field_id:2), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:customer_name, converted_type:UTF8, field_id:3, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:description, converted_type:UTF8, field_id:4, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:device_class, converted_type:UTF8, field_id:5, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:device_class_label, converted_type:UTF8, field_id:6, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:INT64, repetition_type:OPTIONAL, name:device_id, field_id:7), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:discovered_name, converted_type:UTF8, field_id:8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:interface_version, converted_type:UTF8, field_id:9, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BOOLEAN, repetition_type:OPTIONAL, name:is_probe, field_id:10), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:last_logged_in_user, converted_type:UTF8, field_id:11, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:license_mode, converted_type:UTF8, field_id:12, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:long_name, converted_type:UTF8, field_id:13, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:network_version, converted_type:UTF8, field_id:14, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:os_id, converted_type:UTF8, field_id:15, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:remote_control_uri, converted_type:UTF8, field_id:16, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:site_name, converted_type:UTF8, field_id:17, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:so_name, converted_type:UTF8, field_id:18, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:source_uri, converted_type:UTF8, field_id:19, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:state_status, converted_type:UTF8, field_id:20, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BOOLEAN, repetition_type:OPTIONAL, name:still_logged_in, field_id:21), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:supported_os, converted_type:UTF8, field_id:22, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:supported_os_label, converted_type:UTF8, field_id:23, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:uri, converted_type:UTF8, field_id:24, logicalType:<LogicalType STRING:StringType()>)], num_rows:593, row_groups:[RowGroup(columns:[ColumnChunk(file_offset:95, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[agent_version], codec:ZSTD, num_values:593, total_uncompressed_size:288, total_compressed_size:280, data_page_offset:95, dictionary_page_offset:4, statistics:Statistics(null_count:163, max_value:32 30 32 34 2E 32 2E 30 2E 32 30, min_value:31 2E 31 30 2E 31 2E 35 36 30), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:24198, offset_index_length:12, column_index_offset:23345, column_index_length:37), ColumnChunk(file_offset:424, meta_data:ColumnMetaData(type:INT64, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[customer_id], codec:ZSTD, num_values:593, total_uncompressed_size:356, total_compressed_size:260, data_page_offset:424, dictionary_page_offset:284, statistics:Statistics(max:CC 03 00 00 00 00 00 00, min:7C 00 00 00 00 00 00 00, null_count:0, max_value:CC 03 00 00 00 00 00 00, min_value:7C 00 00 00 00 00 00 00), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:24210, offset_index_length:12, column_index_offset:23382, column_index_length:31), ColumnChunk(file_offset:936, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[customer_name], codec:ZSTD, num_values:593, total_uncompressed_size:787, total_compressed_size:512, data_page_offset:936, dictionary_page_offset:544, statistics:Statistics(null_count:0, max_value:42 65 61 63 68 73 69 64 65 20 48 65 61 6C 74 68, min_value:31 2E 20 4D 69 67 72 61 74 69 6F 6E), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:24222, offset_index_length:12, column_index_offset:23413, column_index_length:43), ColumnChunk(file_offset:1056, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[description], codec:ZSTD, num_values:593, total_uncompressed_size:36712, total_compressed_size:3509, data_page_offset:1056, statistics:Statistics(null_count:7, max_value:57 69 6E 64 6F 77 73 20 50 72 6F 62 65, min_value:4E 65 74 77 6F 72 6B 20 64 65 76 69 63 65 20 64 69 73 63 6F 76 65 72 65 64 20 75 73 69 6E 67 20 41 73 73 65 74 20 44 69 73 63 6F 76 65 72 79 20 2D 20 31 30 30 31 30 35 31 32 39 32), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:24234, offset_index_length:12, column_index_offset:23456, column_index_length:88), ColumnChunk(file_offset:4739, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[device_class], codec:ZSTD, num_values:593, total_uncompressed_size:527, total_compressed_size:403, data_page_offset:4739, dictionary_page_offset:4565, statistics:Statistics(null_count:0, max_value:57 6F 72 6B 73 74 61 74 69 6F 6E 73 20 2D 20 57 69 6E 64 6F 77 73, min_value:4C 61 70 74 6F 70 20 2D 20 57 69 6E 64 6F 77 73), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:24246, offset_index_length:12, column_index_offset:23544, column_index_length:53), ColumnChunk(file_offset:5142, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[device_class_label], codec:ZSTD, num_values:593, total_uncompressed_size:527, total_compressed_size:403, data_page_offset:5142, dictionary_page_offset:4968, statistics:Statistics(null_count:0, max_value:57 6F 72 6B 73 74 61 74 69 6F 6E 73 20 2D 20 57 69 6E 64 6F 77 73, min_value:4C 61 70 74 6F 70 20 2D 20 57 69 6E 64 6F 77 73), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:24258, offset_index_length:12, column_index_offset:23597, column_index_length:53), ColumnChunk(file_offset:5371, meta_data:ColumnMetaData(type:INT64, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[device_id], codec:ZSTD, num_values:593, total_uncompressed_size:4777, total_compressed_size:2536, data_page_offset:5371, statistics:Statistics(max:02 0E D0 7F 00 00 00 00, min:5E BF 73 00 00 00 00 00, null_count:0, max_value:02 0E D0 7F 00 00 00 00, min_value:5E BF 73 00 00 00 00 00), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:24270, offset_index_length:12, column_index_offset:23650, column_index_length:31), ColumnChunk(file_offset:7907, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[discovered_name], codec:ZSTD, num_values:593, total_uncompressed_size:9955, total_compressed_size:3127, data_page_offset:7907, statistics:Statistics(null_count:19, max_value:73 31, min_value:31 30 2E 31 30 2E 31 30 2E 37 30), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:24282, offset_index_length:12, column_index_offset:23681, column_index_length:28), ColumnChunk(file_offset:11034, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[interface_version], codec:ZSTD, num_values:593, total_uncompressed_size:31, total_compressed_size:40, data_page_offset:11034, statistics:Statistics(null_count:593), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:24294, offset_index_length:12, column_index_offset:23709, column_index_length:16), ColumnChunk(file_offset:11074, meta_data:ColumnMetaData(type:BOOLEAN, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[is_probe], codec:ZSTD, num_values:593, total_uncompressed_size:108, total_compressed_size:96, data_page_offset:11074, statistics:Statistics(max:01, min:00, null_count:0, max_value:01, min_value:00), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:24306, offset_index_length:13, column_index_offset:23725, column_index_length:17), ColumnChunk(file_offset:13956, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[last_logged_in_user], codec:ZSTD, num_values:593, total_uncompressed_size:7627, total_compressed_size:3430, data_page_offset:13956, dictionary_page_offset:11170, statistics:Statistics(null_count:0, max_value:73 75 72 67 65 72 79, min_value:2D), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:24319, offset_index_length:13, column_index_offset:23742, column_index_length:23), ColumnChunk(file_offset:14657, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[license_mode], codec:ZSTD, num_values:593, total_uncompressed_size:159, total_compressed_size:173, data_page_offset:14657, dictionary_page_offset:14600, statistics:Statistics(null_count:0, max_value:50 72 6F 66 65 73 73 69 6F 6E 61 6C, min_value:45 73 73 65 6E 74 69 61 6C), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:24332, offset_index_length:13, column_index_offset:23765, column_index_length:36), ColumnChunk(file_offset:14773, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[long_name], codec:ZSTD, num_values:593, total_uncompressed_size:10325, total_compressed_size:3254, data_page_offset:14773, statistics:Statistics(null_count:0, max_value:73 67 33 30 30 2D 32 38 70 2D 30 33, min_value:31 30 2E 31 30 2E 31 30 2E 37 30), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:24345, offset_index_length:13, column_index_offset:23801, column_index_length:38), ColumnChunk(file_offset:18027, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[network_version], codec:ZSTD, num_values:593, total_uncompressed_size:31, total_compressed_size:40, data_page_offset:18027, statistics:Statistics(null_count:593), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:24358, offset_index_length:12, column_index_offset:23839, column_index_length:16), ColumnChunk(file_offset:18134, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[os_id], codec:ZSTD, num_values:593, total_uncompressed_size:200, total_compressed_size:205, data_page_offset:18134, dictionary_page_offset:18067, statistics:Statistics(null_count:0, max_value:77 69 6E 6E 74, min_value:4E 4F 41 47 45 4E 54), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:24370, offset_index_length:13, column_index_offset:23855, column_index_length:27), ColumnChunk(file_offset:18272, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[remote_control_uri], codec:ZSTD, num_values:593, total_uncompressed_size:31, total_compressed_size:40, data_page_offset:18272, statistics:Statistics(null_count:593), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:24383, offset_index_length:12, column_index_offset:23882, column_index_length:16), ColumnChunk(file_offset:18441, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[site_name], codec:ZSTD, num_values:593, total_uncompressed_size:243, total_compressed_size:246, data_page_offset:18441, dictionary_page_offset:18312, statistics:Statistics(null_count:449, max_value:53 75 6E 6E 79 62 72 6F 6F 6B 20 53 69 74 65, min_value:34 30 55 6E 69 76 65 72 73 69 74 79), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:24395, offset_index_length:13, column_index_offset:23898, column_index_length:43), ColumnChunk(file_offset:18611, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[so_name], codec:ZSTD, num_values:593, total_uncompressed_size:78, total_compressed_size:96, data_page_offset:18611, dictionary_page_offset:18558, statistics:Statistics(max:42 6C 75 65 42 69 72 64 20 49 54 20 53 6F 6C 75 74 69 6F 6E 73, min:42 6C 75 65 42 69 72 64 20 49 54 20 53 6F 6C 75 74 69 6F 6E 73, null_count:0, max_value:42 6C 75 65 42 69 72 64 20 49 54 20 53 6F 6C 75 74 69 6F 6E 73, min_value:42 6C 75 65 42 69 72 64 20 49 54 20 53 6F 6C 75 74 69 6F 6E 73), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:24408, offset_index_length:12, column_index_offset:23941, column_index_length:57), ColumnChunk(file_offset:18654, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[source_uri], codec:ZSTD, num_values:593, total_uncompressed_size:879, total_compressed_size:390, data_page_offset:18654, statistics:Statistics(null_count:547, max_value:31 39 32 2E 31 36 38 2E 38 30 2E 32, min_value:31 30 2E 30 2E 30 2E 32 33 30), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:24420, offset_index_length:13, column_index_offset:23998, column_index_length:38), ColumnChunk(file_offset:19141, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[state_status], codec:ZSTD, num_values:593, total_uncompressed_size:358, total_compressed_size:340, data_page_offset:19141, dictionary_page_offset:19044, statistics:Statistics(null_count:2, max_value:57 61 72 6E 69 6E 67, min_value:44 69 73 63 6F 6E 6E 65 63 74 65 64), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:24433, offset_index_length:13, column_index_offset:24036, column_index_length:34), ColumnChunk(file_offset:19384, meta_data:ColumnMetaData(type:BOOLEAN, encodings:[RLE, BIT_PACKED, PLAIN], path_in_schema:[still_logged_in], codec:ZSTD, num_values:593, total_uncompressed_size:172, total_compressed_size:181, data_page_offset:19384, statistics:Statistics(max:01, min:00, null_count:172, max_value:01, min_value:00), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:24446, offset_index_length:13, column_index_offset:24070, column_index_length:18), ColumnChunk(file_offset:19804, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[supported_os], codec:ZSTD, num_values:593, total_uncompressed_size:1030, total_compressed_size:608, data_page_offset:19804, dictionary_page_offset:19565, statistics:Statistics(null_count:19, max_value:6D 61 63 4F 53 20 76 65 72 73 69 6F 6E 20 31 34, min_value:4C 69 6E 75 78), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:24459, offset_index_length:13, column_index_offset:24088, column_index_length:36), ColumnChunk(file_offset:20406, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[supported_os_label], codec:ZSTD, num_values:593, total_uncompressed_size:991, total_compressed_size:602, data_page_offset:20406, dictionary_page_offset:20173, statistics:Statistics(null_count:19, max_value:6D 61 63 4F 53 20 31 34, min_value:4C 69 6E 75 78), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:24472, offset_index_length:13, column_index_offset:24124, column_index_length:28), ColumnChunk(file_offset:22625, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN_DICTIONARY, RLE, BIT_PACKED], path_in_schema:[uri], codec:ZSTD, num_values:593, total_uncompressed_size:10450, total_compressed_size:2570, data_page_offset:22625, dictionary_page_offset:20775, statistics:Statistics(null_count:0, max_value:67 75 65 6C 70 68 6C 61 62 2D 70 63 2E 61 66 2E 6C 6F 63 61 6C, min_value:31 30 2E 30 2E 30 2E 31 31 31), encoding_stats:[PageEncodingStats(page_type:DICTIONARY_PAGE, encoding:PLAIN_DICTIONARY, count:1), PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN_DICTIONARY, count:1)]), offset_index_offset:24485, offset_index_length:13, column_index_offset:24152, column_index_length:46)], total_byte_size:86642, num_rows:593, file_offset:4, total_compressed_size:23341, ordinal:0)], key_value_metadata:[KeyValue(key:iceberg.schema, value:{\"type\":\"struct\",\"schema-id\":0,\"fields\":[{\"id\":1,\"name\":\"agent_version\",\"required\":false,\"type\":\"string\"},{\"id\":2,\"name\":\"customer_id\",\"required\":false,\"type\":\"long\"},{\"id\":3,\"name\":\"customer_name\",\"required\":false,\"type\":\"string\"},{\"id\":4,\"name\":\"description\",\"required\":false,\"type\":\"string\"},{\"id\":5,\"name\":\"device_class\",\"required\":false,\"type\":\"string\"},{\"id\":6,\"name\":\"device_class_label\",\"required\":false,\"type\":\"string\"},{\"id\":7,\"name\":\"device_id\",\"required\":false,\"type\":\"long\"},{\"id\":8,\"name\":\"discovered_name\",\"required\":false,\"type\":\"string\"},{\"id\":9,\"name\":\"interface_version\",\"required\":false,\"type\":\"string\"},{\"id\":10,\"name\":\"is_probe\",\"required\":false,\"type\":\"boolean\"},{\"id\":11,\"name\":\"last_logged_in_user\",\"required\":false,\"type\":\"string\"},{\"id\":12,\"name\":\"license_mode\",\"required\":false,\"type\":\"string\"},{\"id\":13,\"name\":\"long_name\",\"required\":false,\"type\":\"string\"},{\"id\":14,\"name\":\"network_version\",\"required\":false,\"type\":\"string\"},{\"id\":15,\"name\":\"os_id\",\"required\":false,\"type\":\"string\"},{\"id\":16,\"name\":\"remote_control_uri\",\"required\":false,\"type\":\"string\"},{\"id\":17,\"name\":\"site_name\",\"required\":false,\"type\":\"string\"},{\"id\":18,\"name\":\"so_name\",\"required\":false,\"type\":\"string\"},{\"id\":19,\"name\":\"source_uri\",\"required\":false,\"type\":\"string\"},{\"id\":20,\"name\":\"state_status\",\"required\":false,\"type\":\"string\"},{\"id\":21,\"name\":\"still_logged_in\",\"required\":false,\"type\":\"boolean\"},{\"id\":22,\"name\":\"supported_os\",\"required\":false,\"type\":\"string\"},{\"id\":23,\"name\":\"supported_os_label\",\"required\":false,\"type\":\"string\"},{\"id\":24,\"name\":\"uri\",\"required\":false,\"type\":\"string\"}]})], created_by:parquet-mr version 1.13.1 (build db4183109d5b734ec5930d870cdae161e408ddba), column_orders:[<ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>])\n",
      "18:14:57.524 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.format.converter.ParquetMetadataConverter -- {\n",
      "  \"fileMetaData\" : {\n",
      "    \"schema\" : {\n",
      "      \"name\" : \"table\",\n",
      "      \"repetition\" : \"REPEATED\",\n",
      "      \"logicalTypeAnnotation\" : null,\n",
      "      \"id\" : null,\n",
      "      \"fields\" : [ {\n",
      "        \"name\" : \"agent_version\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 1\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"customer_id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 2\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"INT64\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }, {\n",
      "        \"name\" : \"customer_name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 3\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"description\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 4\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"device_class\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 5\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"device_class_label\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 6\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"device_id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 7\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"INT64\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }, {\n",
      "        \"name\" : \"discovered_name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 8\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"interface_version\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 9\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"is_probe\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 10\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BOOLEAN\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }, {\n",
      "        \"name\" : \"last_logged_in_user\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 11\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"license_mode\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 12\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"long_name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 13\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"network_version\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 14\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"os_id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 15\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"remote_control_uri\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 16\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"site_name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 17\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"so_name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 18\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"source_uri\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 19\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"state_status\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 20\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"still_logged_in\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 21\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BOOLEAN\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }, {\n",
      "        \"name\" : \"supported_os\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 22\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"supported_os_label\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 23\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"uri\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 24\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      } ],\n",
      "      \"paths\" : [ [ \"agent_version\" ], [ \"customer_id\" ], [ \"customer_name\" ], [ \"description\" ], [ \"device_class\" ], [ \"device_class_label\" ], [ \"device_id\" ], [ \"discovered_name\" ], [ \"interface_version\" ], [ \"is_probe\" ], [ \"last_logged_in_user\" ], [ \"license_mode\" ], [ \"long_name\" ], [ \"network_version\" ], [ \"os_id\" ], [ \"remote_control_uri\" ], [ \"site_name\" ], [ \"so_name\" ], [ \"source_uri\" ], [ \"state_status\" ], [ \"still_logged_in\" ], [ \"supported_os\" ], [ \"supported_os_label\" ], [ \"uri\" ] ],\n",
      "      \"columns\" : [ {\n",
      "        \"path\" : [ \"agent_version\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"agent_version\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 1\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"customer_id\" ],\n",
      "        \"type\" : \"INT64\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"customer_id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : null,\n",
      "          \"id\" : {\n",
      "            \"id\" : 2\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"INT64\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : null\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"customer_name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"customer_name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 3\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"description\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"description\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 4\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"device_class\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"device_class\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 5\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"device_class_label\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"device_class_label\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 6\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"device_id\" ],\n",
      "        \"type\" : \"INT64\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"device_id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : null,\n",
      "          \"id\" : {\n",
      "            \"id\" : 7\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"INT64\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : null\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"discovered_name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"discovered_name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 8\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"interface_version\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"interface_version\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 9\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"is_probe\" ],\n",
      "        \"type\" : \"BOOLEAN\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"is_probe\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : null,\n",
      "          \"id\" : {\n",
      "            \"id\" : 10\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BOOLEAN\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : null\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"last_logged_in_user\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"last_logged_in_user\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 11\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"license_mode\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"license_mode\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 12\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"long_name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"long_name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 13\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"network_version\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"network_version\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 14\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"os_id\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"os_id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 15\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"remote_control_uri\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"remote_control_uri\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 16\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"site_name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"site_name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 17\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"so_name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"so_name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 18\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"source_uri\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"source_uri\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 19\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"state_status\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"state_status\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 20\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"still_logged_in\" ],\n",
      "        \"type\" : \"BOOLEAN\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"still_logged_in\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : null,\n",
      "          \"id\" : {\n",
      "            \"id\" : 21\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BOOLEAN\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : null\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"supported_os\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"supported_os\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 22\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"supported_os_label\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"supported_os_label\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 23\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      }, {\n",
      "        \"path\" : [ \"uri\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"uri\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : {\n",
      "            \"id\" : 24\n",
      "          },\n",
      "          \"primitive\" : true,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"typeLength\" : 0,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        },\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1\n",
      "      } ],\n",
      "      \"primitive\" : false,\n",
      "      \"fieldCount\" : 24,\n",
      "      \"originalType\" : null\n",
      "    },\n",
      "    \"keyValueMetaData\" : {\n",
      "      \"iceberg.schema\" : \"{\\\"type\\\":\\\"struct\\\",\\\"schema-id\\\":0,\\\"fields\\\":[{\\\"id\\\":1,\\\"name\\\":\\\"agent_version\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":2,\\\"name\\\":\\\"customer_id\\\",\\\"required\\\":false,\\\"type\\\":\\\"long\\\"},{\\\"id\\\":3,\\\"name\\\":\\\"customer_name\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":4,\\\"name\\\":\\\"description\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":5,\\\"name\\\":\\\"device_class\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":6,\\\"name\\\":\\\"device_class_label\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":7,\\\"name\\\":\\\"device_id\\\",\\\"required\\\":false,\\\"type\\\":\\\"long\\\"},{\\\"id\\\":8,\\\"name\\\":\\\"discovered_name\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":9,\\\"name\\\":\\\"interface_version\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":10,\\\"name\\\":\\\"is_probe\\\",\\\"required\\\":false,\\\"type\\\":\\\"boolean\\\"},{\\\"id\\\":11,\\\"name\\\":\\\"last_logged_in_user\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":12,\\\"name\\\":\\\"license_mode\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":13,\\\"name\\\":\\\"long_name\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":14,\\\"name\\\":\\\"network_version\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":15,\\\"name\\\":\\\"os_id\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":16,\\\"name\\\":\\\"remote_control_uri\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":17,\\\"name\\\":\\\"site_name\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":18,\\\"name\\\":\\\"so_name\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":19,\\\"name\\\":\\\"source_uri\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":20,\\\"name\\\":\\\"state_status\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":21,\\\"name\\\":\\\"still_logged_in\\\",\\\"required\\\":false,\\\"type\\\":\\\"boolean\\\"},{\\\"id\\\":22,\\\"name\\\":\\\"supported_os\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":23,\\\"name\\\":\\\"supported_os_label\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"},{\\\"id\\\":24,\\\"name\\\":\\\"uri\\\",\\\"required\\\":false,\\\"type\\\":\\\"string\\\"}]}\"\n",
      "    },\n",
      "    \"createdBy\" : \"parquet-mr version 1.13.1 (build db4183109d5b734ec5930d870cdae161e408ddba)\",\n",
      "    \"fileDecryptor\" : null,\n",
      "    \"encryptionType\" : \"UNENCRYPTED\"\n",
      "  },\n",
      "  \"blocks\" : [ {\n",
      "    \"columns\" : [ {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23345,\n",
      "        \"length\" : 37\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24198,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 4,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 280,\n",
      "      \"totalUncompressedSize\" : 288,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"MjAyNC4yLjAuMjA=\",\n",
      "          \"bytesUnsafe\" : \"MjAyNC4yLjAuMjA=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"MS4xMC4xLjU2MA==\",\n",
      "          \"bytesUnsafe\" : \"MS4xMC4xLjU2MA==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"MS4xMC4xLjU2MA==\",\n",
      "        \"maxBytes\" : \"MjAyNC4yLjAuMjA=\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 163\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 95,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"agent_version\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 4,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"agent_version\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 1\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23382,\n",
      "        \"length\" : 31\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24210,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 284,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 260,\n",
      "      \"totalUncompressedSize\" : 356,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : 972,\n",
      "        \"min\" : 124,\n",
      "        \"minBytes\" : \"fAAAAAAAAAA=\",\n",
      "        \"maxBytes\" : \"zAMAAAAAAAA=\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 424,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"INT64\",\n",
      "      \"path\" : [ \"customer_id\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 284,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"customer_id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 2\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"INT64\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23413,\n",
      "        \"length\" : 43\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24222,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 544,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 512,\n",
      "      \"totalUncompressedSize\" : 787,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"QmVhY2hzaWRlIEhlYWx0aA==\",\n",
      "          \"bytesUnsafe\" : \"QmVhY2hzaWRlIEhlYWx0aA==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"MS4gTWlncmF0aW9u\",\n",
      "          \"bytesUnsafe\" : \"MS4gTWlncmF0aW9u\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"MS4gTWlncmF0aW9u\",\n",
      "        \"maxBytes\" : \"QmVhY2hzaWRlIEhlYWx0aA==\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 936,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"customer_name\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 544,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"customer_name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 3\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23456,\n",
      "        \"length\" : 88\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24234,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 3509,\n",
      "      \"totalUncompressedSize\" : 36712,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"V2luZG93cyBQcm9iZQ==\",\n",
      "          \"bytesUnsafe\" : \"V2luZG93cyBQcm9iZQ==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"TmV0d29yayBkZXZpY2UgZGlzY292ZXJlZCB1c2luZyBBc3NldCBEaXNjb3ZlcnkgLSAxMDAxMDUxMjky\",\n",
      "          \"bytesUnsafe\" : \"TmV0d29yayBkZXZpY2UgZGlzY292ZXJlZCB1c2luZyBBc3NldCBEaXNjb3ZlcnkgLSAxMDAxMDUxMjky\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"TmV0d29yayBkZXZpY2UgZGlzY292ZXJlZCB1c2luZyBBc3NldCBEaXNjb3ZlcnkgLSAxMDAxMDUxMjky\",\n",
      "        \"maxBytes\" : \"V2luZG93cyBQcm9iZQ==\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 7\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 1056,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"description\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 1056,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"description\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 4\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23544,\n",
      "        \"length\" : 53\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24246,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 4565,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 403,\n",
      "      \"totalUncompressedSize\" : 527,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"V29ya3N0YXRpb25zIC0gV2luZG93cw==\",\n",
      "          \"bytesUnsafe\" : \"V29ya3N0YXRpb25zIC0gV2luZG93cw==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"TGFwdG9wIC0gV2luZG93cw==\",\n",
      "          \"bytesUnsafe\" : \"TGFwdG9wIC0gV2luZG93cw==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"TGFwdG9wIC0gV2luZG93cw==\",\n",
      "        \"maxBytes\" : \"V29ya3N0YXRpb25zIC0gV2luZG93cw==\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 4739,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"device_class\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 4565,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"device_class\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 5\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23597,\n",
      "        \"length\" : 53\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24258,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 4968,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 403,\n",
      "      \"totalUncompressedSize\" : 527,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"V29ya3N0YXRpb25zIC0gV2luZG93cw==\",\n",
      "          \"bytesUnsafe\" : \"V29ya3N0YXRpb25zIC0gV2luZG93cw==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"TGFwdG9wIC0gV2luZG93cw==\",\n",
      "          \"bytesUnsafe\" : \"TGFwdG9wIC0gV2luZG93cw==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"TGFwdG9wIC0gV2luZG93cw==\",\n",
      "        \"maxBytes\" : \"V29ya3N0YXRpb25zIC0gV2luZG93cw==\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 5142,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"device_class_label\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 4968,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"device_class_label\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 6\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23650,\n",
      "        \"length\" : 31\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24270,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 2536,\n",
      "      \"totalUncompressedSize\" : 4777,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : 2144341506,\n",
      "        \"min\" : 7585630,\n",
      "        \"minBytes\" : \"Xr9zAAAAAAA=\",\n",
      "        \"maxBytes\" : \"Ag7QfwAAAAA=\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 5371,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"INT64\",\n",
      "      \"path\" : [ \"device_id\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 5371,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"device_id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 7\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"INT64\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23681,\n",
      "        \"length\" : 28\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24282,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 3127,\n",
      "      \"totalUncompressedSize\" : 9955,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"czE=\",\n",
      "          \"bytesUnsafe\" : \"czE=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"MTAuMTAuMTAuNzA=\",\n",
      "          \"bytesUnsafe\" : \"MTAuMTAuMTAuNzA=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"MTAuMTAuMTAuNzA=\",\n",
      "        \"maxBytes\" : \"czE=\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 19\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 7907,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"discovered_name\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 7907,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"discovered_name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 8\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23709,\n",
      "        \"length\" : 16\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24294,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 40,\n",
      "      \"totalUncompressedSize\" : 31,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : null,\n",
      "        \"min\" : null,\n",
      "        \"minBytes\" : null,\n",
      "        \"maxBytes\" : null,\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 593\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 11034,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"interface_version\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 11034,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"interface_version\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 9\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23725,\n",
      "        \"length\" : 17\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24306,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 96,\n",
      "      \"totalUncompressedSize\" : 108,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : true,\n",
      "        \"min\" : false,\n",
      "        \"minBytes\" : \"AA==\",\n",
      "        \"maxBytes\" : \"AQ==\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 11074,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BOOLEAN\",\n",
      "      \"path\" : [ \"is_probe\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 11074,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"is_probe\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 10\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BOOLEAN\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23742,\n",
      "        \"length\" : 23\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24319,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 11170,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 3430,\n",
      "      \"totalUncompressedSize\" : 7627,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"c3VyZ2VyeQ==\",\n",
      "          \"bytesUnsafe\" : \"c3VyZ2VyeQ==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"LQ==\",\n",
      "          \"bytesUnsafe\" : \"LQ==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"LQ==\",\n",
      "        \"maxBytes\" : \"c3VyZ2VyeQ==\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 13956,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"last_logged_in_user\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 11170,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"last_logged_in_user\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 11\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23765,\n",
      "        \"length\" : 36\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24332,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 14600,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 173,\n",
      "      \"totalUncompressedSize\" : 159,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"UHJvZmVzc2lvbmFs\",\n",
      "          \"bytesUnsafe\" : \"UHJvZmVzc2lvbmFs\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"RXNzZW50aWFs\",\n",
      "          \"bytesUnsafe\" : \"RXNzZW50aWFs\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"RXNzZW50aWFs\",\n",
      "        \"maxBytes\" : \"UHJvZmVzc2lvbmFs\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 14657,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"license_mode\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 14600,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"license_mode\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 12\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23801,\n",
      "        \"length\" : 38\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24345,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 3254,\n",
      "      \"totalUncompressedSize\" : 10325,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"c2czMDAtMjhwLTAz\",\n",
      "          \"bytesUnsafe\" : \"c2czMDAtMjhwLTAz\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"MTAuMTAuMTAuNzA=\",\n",
      "          \"bytesUnsafe\" : \"MTAuMTAuMTAuNzA=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"MTAuMTAuMTAuNzA=\",\n",
      "        \"maxBytes\" : \"c2czMDAtMjhwLTAz\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 14773,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"long_name\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 14773,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"long_name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 13\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23839,\n",
      "        \"length\" : 16\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24358,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 40,\n",
      "      \"totalUncompressedSize\" : 31,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : null,\n",
      "        \"min\" : null,\n",
      "        \"minBytes\" : null,\n",
      "        \"maxBytes\" : null,\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 593\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 18027,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"network_version\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 18027,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"network_version\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 14\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23855,\n",
      "        \"length\" : 27\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24370,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 18067,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 205,\n",
      "      \"totalUncompressedSize\" : 200,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"d2lubnQ=\",\n",
      "          \"bytesUnsafe\" : \"d2lubnQ=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"Tk9BR0VOVA==\",\n",
      "          \"bytesUnsafe\" : \"Tk9BR0VOVA==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"Tk9BR0VOVA==\",\n",
      "        \"maxBytes\" : \"d2lubnQ=\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 18134,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"os_id\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 18067,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"os_id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 15\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23882,\n",
      "        \"length\" : 16\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24383,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 40,\n",
      "      \"totalUncompressedSize\" : 31,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : null,\n",
      "        \"min\" : null,\n",
      "        \"minBytes\" : null,\n",
      "        \"maxBytes\" : null,\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 593\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 18272,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"remote_control_uri\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 18272,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"remote_control_uri\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 16\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23898,\n",
      "        \"length\" : 43\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24395,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 18312,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 246,\n",
      "      \"totalUncompressedSize\" : 243,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"U3Vubnlicm9vayBTaXRl\",\n",
      "          \"bytesUnsafe\" : \"U3Vubnlicm9vayBTaXRl\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"NDBVbml2ZXJzaXR5\",\n",
      "          \"bytesUnsafe\" : \"NDBVbml2ZXJzaXR5\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"NDBVbml2ZXJzaXR5\",\n",
      "        \"maxBytes\" : \"U3Vubnlicm9vayBTaXRl\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 449\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 18441,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"site_name\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 18312,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"site_name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 17\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23941,\n",
      "        \"length\" : 57\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24408,\n",
      "        \"length\" : 12\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 18558,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 96,\n",
      "      \"totalUncompressedSize\" : 78,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"Qmx1ZUJpcmQgSVQgU29sdXRpb25z\",\n",
      "          \"bytesUnsafe\" : \"Qmx1ZUJpcmQgSVQgU29sdXRpb25z\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"Qmx1ZUJpcmQgSVQgU29sdXRpb25z\",\n",
      "          \"bytesUnsafe\" : \"Qmx1ZUJpcmQgSVQgU29sdXRpb25z\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"Qmx1ZUJpcmQgSVQgU29sdXRpb25z\",\n",
      "        \"maxBytes\" : \"Qmx1ZUJpcmQgSVQgU29sdXRpb25z\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 18611,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"so_name\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 18558,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"so_name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 18\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 23998,\n",
      "        \"length\" : 38\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24420,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"diction+-------------+-----------+-------------+--------------------+--------------------+--------------------+----------+---------------+-----------------+--------+--------------------+------------+---------------+---------------+-----+------------------+---------+--------------------+-------------+------------+---------------+--------------------+--------------------+---------------+\n",
      "|agent_version|customer_id|customer_name|         description|        device_class|  device_class_label| device_id|discovered_name|interface_version|is_probe| last_logged_in_user|license_mode|      long_name|network_version|os_id|remote_control_uri|site_name|             so_name|   source_uri|state_status|still_logged_in|        supported_os|  supported_os_label|            uri|\n",
      "+-------------+-----------+-------------+--------------------+--------------------+--------------------+----------+---------------+-----------------+--------+--------------------+------------+---------------+---------------+-----+------------------+---------+--------------------+-------------+------------+---------------+--------------------+--------------------+---------------+\n",
      "|  2024.2.0.20|        149| 1. Migration|Network device di...|    Laptop - Windows|    Laptop - Windows|2003079455|         Admin2|             NULL|   false|       ADMIN2\\admin2|Professional|         Admin2|           NULL|winnt|              NULL|     NULL|BlueBird IT Solut...|         NULL|      Failed|           true|Microsoft Windows...|Microsoft Windows...| 192.168.50.220|\n",
      "|  2023.9.1.30|        149| 1. Migration|Network device di...|Workstations - Wi...|Workstations - Wi...|1778560646|   BB22218-R-HE|             NULL|   false| BB22218-R-HE\\Clinic|Professional|   BB22218-R-HE|           NULL|winnt|              NULL|     NULL|BlueBird IT Solut...|         NULL|Disconnected|          false|Microsoft Windows...|Microsoft Windows...|  192.168.1.163|\n",
      "|  2023.9.1.30|        149| 1. Migration|Network device di...|    Laptop - Windows|    Laptop - Windows|2134896242|  BB22724-E-SAH|             NULL|   false| BB22724-E-SAH\\Admin|Professional|  BB22724-E-SAH|           NULL|winnt|              NULL|     NULL|BlueBird IT Solut...|192.168.68.91|Disconnected|          false|Microsoft Windows...|Microsoft Windows...|  192.168.68.91|\n",
      "|  2023.9.1.30|        149| 1. Migration|Network device di...|Workstations - Wi...|Workstations - Wi...|2017740413|  BB23643-R-LNG|             NULL|   false|                   -|Professional|  BB23643-R-LNG|           NULL|winnt|              NULL|     NULL|BlueBird IT Solut...|         NULL|      Normal|           NULL|Microsoft Windows...|Microsoft Windows...|    10.10.50.50|\n",
      "|  2023.9.1.30|        149| 1. Migration|Network device di...|Workstations - Wi...|Workstations - Wi...| 314205022|DESKTOP-BPP6GTB|             NULL|   false|DESKTOP-BPP6GTB\\T...|Professional|DESKTOP-BPP6GTB|           NULL|winnt|              NULL|     NULL|BlueBird IT Solut...|         NULL|Disconnected|          false|Microsoft Windows...|Microsoft Windows...|192.168.197.134|\n",
      "+-------------+-----------+-------------+--------------------+--------------------+--------------------+----------+---------------+-----------------+--------+--------------------+------------+---------------+---------------+-----+------------------+---------+--------------------+-------------+------------+---------------+--------------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "aryPageOffset\" : 0,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 390,\n",
      "      \"totalUncompressedSize\" : 879,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"MTkyLjE2OC44MC4y\",\n",
      "          \"bytesUnsafe\" : \"MTkyLjE2OC44MC4y\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"MTAuMC4wLjIzMA==\",\n",
      "          \"bytesUnsafe\" : \"MTAuMC4wLjIzMA==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"MTAuMC4wLjIzMA==\",\n",
      "        \"maxBytes\" : \"MTkyLjE2OC44MC4y\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 547\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 18654,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"source_uri\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 18654,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"source_uri\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 19\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 24036,\n",
      "        \"length\" : 34\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24433,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 19044,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 340,\n",
      "      \"totalUncompressedSize\" : 358,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"V2FybmluZw==\",\n",
      "          \"bytesUnsafe\" : \"V2FybmluZw==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"RGlzY29ubmVjdGVk\",\n",
      "          \"bytesUnsafe\" : \"RGlzY29ubmVjdGVk\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"RGlzY29ubmVjdGVk\",\n",
      "        \"maxBytes\" : \"V2FybmluZw==\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 2\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 19141,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"state_status\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 19044,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"state_status\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 20\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 24070,\n",
      "        \"length\" : 18\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24446,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 181,\n",
      "      \"totalUncompressedSize\" : 172,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : true,\n",
      "        \"min\" : false,\n",
      "        \"minBytes\" : \"AA==\",\n",
      "        \"maxBytes\" : \"AQ==\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 172\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 19384,\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"type\" : \"BOOLEAN\",\n",
      "      \"path\" : [ \"still_logged_in\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 19384,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"still_logged_in\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : null,\n",
      "        \"id\" : {\n",
      "          \"id\" : 21\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BOOLEAN\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : null\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 24088,\n",
      "        \"length\" : 36\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24459,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 19565,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 608,\n",
      "      \"totalUncompressedSize\" : 1030,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"bWFjT1MgdmVyc2lvbiAxNA==\",\n",
      "          \"bytesUnsafe\" : \"bWFjT1MgdmVyc2lvbiAxNA==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"TGludXg=\",\n",
      "          \"bytesUnsafe\" : \"TGludXg=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"TGludXg=\",\n",
      "        \"maxBytes\" : \"bWFjT1MgdmVyc2lvbiAxNA==\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 19\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 19804,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"supported_os\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 19565,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"supported_os\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 22\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 24124,\n",
      "        \"length\" : 28\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24472,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 20173,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 602,\n",
      "      \"totalUncompressedSize\" : 991,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"bWFjT1MgMTQ=\",\n",
      "          \"bytesUnsafe\" : \"bWFjT1MgMTQ=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"TGludXg=\",\n",
      "          \"bytesUnsafe\" : \"TGludXg=\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"TGludXg=\",\n",
      "        \"maxBytes\" : \"bWFjT1MgMTQ=\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 19\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 20406,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"supported_os_label\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 20173,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"supported_os_label\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 23\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ \"PLAIN_DICTIONARY\" ],\n",
      "        \"dataEncodings\" : [ \"PLAIN_DICTIONARY\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 24152,\n",
      "        \"length\" : 46\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 24485,\n",
      "        \"length\" : 13\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 20775,\n",
      "      \"valueCount\" : 593,\n",
      "      \"totalSize\" : 2570,\n",
      "      \"totalUncompressedSize\" : 10450,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytes\" : \"Z3VlbHBobGFiLXBjLmFmLmxvY2Fs\",\n",
      "          \"bytesUnsafe\" : \"Z3VlbHBobGFiLXBjLmFmLmxvY2Fs\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytes\" : \"MTAuMC4wLjExMQ==\",\n",
      "          \"bytesUnsafe\" : \"MTAuMC4wLjExMQ==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"MTAuMC4wLjExMQ==\",\n",
      "        \"maxBytes\" : \"Z3VlbHBobGFiLXBjLmFmLmxvY2Fs\",\n",
      "        \"empty\" : false,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"numNulls\" : 0\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 22625,\n",
      "      \"encodings\" : [ \"BIT_PACKED\", \"RLE\", \"PLAIN_DICTIONARY\" ],\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"uri\" ],\n",
      "      \"encrypted\" : false,\n",
      "      \"codec\" : \"ZSTD\",\n",
      "      \"startingPos\" : 20775,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"uri\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : {\n",
      "          \"id\" : 24\n",
      "        },\n",
      "        \"primitive\" : true,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }\n",
      "    } ],\n",
      "    \"rowCount\" : 593,\n",
      "    \"totalByteSize\" : 86642,\n",
      "    \"path\" : null,\n",
      "    \"ordinal\" : 0,\n",
      "    \"rowIndexOffset\" : 0,\n",
      "    \"compressedSize\" : 23341,\n",
      "    \"startingPos\" : 4\n",
      "  } ]\n",
      "}\n",
      "18:14:57.653 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.aws.s3.S3InputStream -- Seek with new stream for s3a://lakehouse/ncentral/devices/data/00000-39-52c56d33-aefc-46d2-8a76-946277cc9425-0-00001.parquet to offset 4\n",
      "18:14:57.663 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] INFO org.apache.hadoop.io.compress.CodecPool -- Got brand-new decompressor [.zstd]\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 73 bytes\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 73 bytes\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 168 bytes\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 168 bytes\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 168 bytes and 593 records\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 84\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 224 bytes\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 224 bytes\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.plain.PlainValuesReader -- init from page at offset 0 for length 224\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 85 bytes\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 85 bytes\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 85 bytes and 593 records\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 7\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 655 bytes\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 655 bytes\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 85 bytes\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 85 bytes\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 85 bytes and 593 records\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 7\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 36685 bytes\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 36685 bytes\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 36685 bytes and 593 records\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 31\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 225 bytes\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 225 bytes\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 255 bytes\n",
      "18:14:57.664 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 255 bytes\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 255 bytes and 593 records\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 7\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 225 bytes\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 225 bytes\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 255 bytes\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 255 bytes\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 255 bytes and 593 records\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 7\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 4751 bytes\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 4751 bytes\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 4751 bytes and 593 records\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 7\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 9928 bytes\n",
      "18:14:57.665 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 9928 bytes\n",
      "18:14:57.666 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 9928 bytes and 593 records\n",
      "18:14:57.666 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.666 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.666 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.666 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 59\n",
      "18:14:57.667 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 7 bytes\n",
      "18:14:57.667 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 7 bytes\n",
      "18:14:57.667 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 7 bytes and 593 records\n",
      "18:14:57.667 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.667 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.667 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.668 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 7\n",
      "18:14:57.668 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 82 bytes\n",
      "18:14:57.668 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 82 bytes\n",
      "18:14:57.668 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 82 bytes and 593 records\n",
      "18:14:57.668 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.668 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.668 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.668 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 7\n",
      "18:14:57.668 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 6923 bytes\n",
      "18:14:57.668 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 6923 bytes\n",
      "18:14:57.669 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 656 bytes\n",
      "18:14:57.669 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 656 bytes\n",
      "18:14:57.669 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 656 bytes and 593 records\n",
      "18:14:57.669 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.669 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.669 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.669 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 7\n",
      "18:14:57.670 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 29 bytes\n",
      "18:14:57.670 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 29 bytes\n",
      "18:14:57.670 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 85 bytes\n",
      "18:14:57.670 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 85 bytes\n",
      "18:14:57.671 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 85 bytes and 593 records\n",
      "18:14:57.671 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.671 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.671 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.671 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 7\n",
      "18:14:57.671 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 10298 bytes\n",
      "18:14:57.671 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 10298 bytes\n",
      "18:14:57.672 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 10298 bytes and 593 records\n",
      "18:14:57.672 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.672 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.672 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.672 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 7\n",
      "18:14:57.672 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 7 bytes\n",
      "18:14:57.672 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 7 bytes\n",
      "18:14:57.673 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 7 bytes and 593 records\n",
      "18:14:57.673 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.673 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.673 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.673 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 7\n",
      "18:14:57.673 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 39 bytes\n",
      "18:14:57.673 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 39 bytes\n",
      "18:14:57.674 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 116 bytes\n",
      "18:14:57.674 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 116 bytes\n",
      "18:14:57.674 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 116 bytes and 593 records\n",
      "18:14:57.674 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.674 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.674 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.674 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 7\n",
      "18:14:57.674 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 7 bytes\n",
      "18:14:57.675 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 7 bytes\n",
      "18:14:57.675 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 7 bytes and 593 records\n",
      "18:14:57.675 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.675 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.675 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.675 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 7\n",
      "18:14:57.676 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 115 bytes\n",
      "18:14:57.676 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 115 bytes\n",
      "18:14:57.676 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 82 bytes\n",
      "18:14:57.676 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 82 bytes\n",
      "18:14:57.676 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 82 bytes and 593 records\n",
      "18:14:57.676 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.677 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.677 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.677 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 32\n",
      "18:14:57.677 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 25 bytes\n",
      "18:14:57.677 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 25 bytes\n",
      "18:14:57.679 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 10 bytes\n",
      "18:14:57.679 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 10 bytes\n",
      "18:14:57.679 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 10 bytes and 593 records\n",
      "18:14:57.679 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.679 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.679 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.680 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 7\n",
      "18:14:57.680 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 853 bytes\n",
      "18:14:57.680 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 853 bytes\n",
      "18:14:57.680 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 853 bytes and 593 records\n",
      "18:14:57.680 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.682 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.682 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.682 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 85\n",
      "18:14:57.682 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 73 bytes\n",
      "18:14:57.682 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 73 bytes\n",
      "18:14:57.683 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 238 bytes\n",
      "18:14:57.684 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 238 bytes\n",
      "18:14:57.684 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 238 bytes and 593 records\n",
      "18:14:57.684 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.684 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.684 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.687 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 17\n",
      "18:14:57.687 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 146 bytes\n",
      "18:14:57.687 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 146 bytes\n",
      "18:14:57.687 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 146 bytes and 593 records\n",
      "18:14:57.687 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.687 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.691 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.691 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 93\n",
      "18:14:57.691 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 620 bytes\n",
      "18:14:57.691 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 620 bytes\n",
      "18:14:57.691 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 364 bytes\n",
      "18:14:57.691 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 364 bytes\n",
      "18:14:57.693 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 364 bytes and 593 records\n",
      "18:14:57.693 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.693 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.693 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.693 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 59\n",
      "18:14:57.695 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 580 bytes\n",
      "18:14:57.695 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 580 bytes\n",
      "18:14:57.695 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 364 bytes\n",
      "18:14:57.695 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 364 bytes\n",
      "18:14:57.695 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 364 bytes and 593 records\n",
      "18:14:57.695 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.697 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.697 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.697 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 59\n",
      "18:14:57.697 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 9716 bytes\n",
      "18:14:57.697 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 9716 bytes\n",
      "18:14:57.699 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput$StreamBytesInput -- read all 685 bytes\n",
      "18:14:57.699 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.bytes.BytesInput -- BytesInput from array of 685 bytes\n",
      "18:14:57.699 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- page size 685 bytes and 593 records\n",
      "18:14:57.699 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading repetition levels at 0\n",
      "18:14:57.699 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.shaded.org.apache.parquet.column.values.bitpacking.ByteBitPackingValuesReader -- reading 0 bytes for 593 values of size 0 bits.\n",
      "18:14:57.699 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading definition levels at 0\n",
      "18:14:57.701 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.iceberg.parquet.BasePageIterator -- reading data at 7\n",
      "18:14:57.711 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] INFO org.apache.spark.executor.Executor -- Finished task 0.0 in stage 95.0 (TID 65). 5814 bytes result sent to driver\n",
      "18:14:57.711 [Executor task launch worker for task 0.0 in stage 95.0 (TID 65)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller -- stageTCMP: (95, 0) -> 0\n",
      "18:14:57.711 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl -- parentName: , name: TaskSet_95.0, runningTasks: 0\n",
      "18:14:57.712 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager -- No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "18:14:57.712 [task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager -- Finished task 0.0 in stage 95.0 (TID 65) in 214 ms on 10.0.0.146 (executor driver) (1/1)\n",
      "18:14:57.713 [task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl -- Removed TaskSet 95.0, whose tasks have all completed, from pool \n",
      "18:14:57.713 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- ResultStage 95 (showString at <unknown>:0) finished in 0.218 s\n",
      "18:14:57.713 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler -- After removal of stage 95, remaining stages = 0\n",
      "18:14:57.713 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler -- Job 65 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "18:14:57.714 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl -- Killing all running tasks in stage 95: Stage finished\n",
      "18:14:57.714 [Thread-2] INFO org.apache.spark.scheduler.DAGScheduler -- Job 65 finished: showString at <unknown>:0, took 0.219650 s\n",
      "18:14:57.733 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5460)\n",
      "18:14:57.733 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5460\n",
      "18:14:57.733 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5460\n",
      "18:14:57.733 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5606)\n",
      "18:14:57.733 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5606\n",
      "18:14:57.733 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5606\n",
      "18:14:57.733 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5718)\n",
      "18:14:57.733 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5718\n",
      "18:14:57.733 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5718\n",
      "18:14:57.733 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5935)\n",
      "18:14:57.734 [Thread-2] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection -- code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, input[3, string, false].toString, input[4, string, false].toString, input[5, string, false].toString, input[6, string, false].toString, input[7, string, false].toString, input[8, string, false].toString, input[9, string, false].toString, input[10, string, false].toString, input[11, string, false].toString, input[12, string, false].toString, input[13, string, false].toString, input[14, string, false].toString, input[15, string, false].toString, input[16, string, false].toString, input[17, string, false].toString, input[18, string, false].toString, input[19, string, false].toString, input[20, string, false].toString, input[21, string, false].toString, input[22, string, false].toString, input[23, string, false].toString, ... 24 more fields):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[24];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     createExternalRow_0_2(i, values_0);\n",
      "/* 027 */     createExternalRow_0_3(i, values_0);\n",
      "/* 028 */     createExternalRow_0_4(i, values_0);\n",
      "/* 029 */     createExternalRow_0_5(i, values_0);\n",
      "/* 030 */     createExternalRow_0_6(i, values_0);\n",
      "/* 031 */     createExternalRow_0_7(i, values_0);\n",
      "/* 032 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 033 */     if (false) {\n",
      "/* 034 */       mutableRow.setNullAt(0);\n",
      "/* 035 */     } else {\n",
      "/* 036 */\n",
      "/* 037 */       mutableRow.update(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     return mutableRow;\n",
      "/* 041 */   }\n",
      "/* 042 */\n",
      "/* 043 */\n",
      "/* 044 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {\n",
      "/* 045 */\n",
      "/* 046 */     UTF8String value_14 = i.getUTF8String(6);\n",
      "/* 047 */     boolean isNull_13 = true;\n",
      "/* 048 */     java.lang.String value_13 = null;\n",
      "/* 049 */     isNull_13 = false;\n",
      "/* 050 */     if (!isNull_13) {\n",
      "/* 051 */\n",
      "/* 052 */       Object funcResult_6 = null;\n",
      "/* 053 */       funcResult_6 = value_14.toString();\n",
      "/* 054 */       value_13 = (java.lang.String) funcResult_6;\n",
      "/* 055 */\n",
      "/* 056 */     }\n",
      "/* 057 */     if (isNull_13) {\n",
      "/* 058 */       values_0[6] = null;\n",
      "/* 059 */     } else {\n",
      "/* 060 */       values_0[6] = value_13;\n",
      "/* 061 */     }\n",
      "/* 062 */\n",
      "/* 063 */     UTF8String value_16 = i.getUTF8String(7);\n",
      "/* 064 */     boolean isNull_15 = true;\n",
      "/* 065 */     java.lang.String value_15 = null;\n",
      "/* 066 */     isNull_15 = false;\n",
      "/* 067 */     if (!isNull_15) {\n",
      "/* 068 */\n",
      "/* 069 */       Object funcResult_7 = null;\n",
      "/* 070 */       funcResult_7 = value_16.toString();\n",
      "/* 071 */       value_15 = (java.lang.String) funcResult_7;\n",
      "/* 072 */\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_15) {\n",
      "/* 075 */       values_0[7] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[7] = value_15;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     UTF8String value_18 = i.getUTF8String(8);\n",
      "/* 081 */     boolean isNull_17 = true;\n",
      "/* 082 */     java.lang.String value_17 = null;\n",
      "/* 083 */     isNull_17 = false;\n",
      "/* 084 */     if (!isNull_17) {\n",
      "/* 085 */\n",
      "/* 086 */       Object funcResult_8 = null;\n",
      "/* 087 */       funcResult_8 = value_18.toString();\n",
      "/* 088 */       value_17 = (java.lang.String) funcResult_8;\n",
      "/* 089 */\n",
      "/* 090 */     }\n",
      "/* 091 */     if (isNull_17) {\n",
      "/* 092 */       values_0[8] = null;\n",
      "/* 093 */     } else {\n",
      "/* 094 */       values_0[8] = value_17;\n",
      "/* 095 */     }\n",
      "/* 096 */\n",
      "/* 097 */   }\n",
      "/* 098 */\n",
      "/* 099 */\n",
      "/* 100 */   private void createExternalRow_0_5(InternalRow i, Object[] values_0) {\n",
      "/* 101 */\n",
      "/* 102 */     UTF8String value_32 = i.getUTF8String(15);\n",
      "/* 103 */     boolean isNull_31 = true;\n",
      "/* 104 */     java.lang.String value_31 = null;\n",
      "/* 105 */     isNull_31 = false;\n",
      "/* 106 */     if (!isNull_31) {\n",
      "/* 107 */\n",
      "/* 108 */       Object funcResult_15 = null;\n",
      "/* 109 */       funcResult_15 = value_32.toString();\n",
      "/* 110 */       value_31 = (java.lang.String) funcResult_15;\n",
      "/* 111 */\n",
      "/* 112 */     }\n",
      "/* 113 */     if (isNull_31) {\n",
      "/* 114 */       values_0[15] = null;\n",
      "/* 115 */     } else {\n",
      "/* 116 */       values_0[15] = value_31;\n",
      "/* 117 */     }\n",
      "/* 118 */\n",
      "/* 119 */     UTF8String value_34 = i.getUTF8String(16);\n",
      "/* 120 */     boolean isNull_33 = true;\n",
      "/* 121 */     java.lang.String value_33 = null;\n",
      "/* 122 */     isNull_33 = false;\n",
      "/* 123 */     if (!isNull_33) {\n",
      "/* 124 */\n",
      "/* 125 */       Object funcResult_16 = null;\n",
      "/* 126 */       funcResult_16 = value_34.toString();\n",
      "/* 127 */       value_33 = (java.lang.String) funcResult_16;\n",
      "/* 128 */\n",
      "/* 129 */     }\n",
      "/* 130 */     if (isNull_33) {\n",
      "/* 131 */       values_0[16] = null;\n",
      "/* 132 */     } else {\n",
      "/* 133 */       values_0[16] = value_33;\n",
      "/* 134 */     }\n",
      "/* 135 */\n",
      "/* 136 */     UTF8String value_36 = i.getUTF8String(17);\n",
      "/* 137 */     boolean isNull_35 = true;\n",
      "/* 138 */     java.lang.String value_35 = null;\n",
      "/* 139 */     isNull_35 = false;\n",
      "/* 140 */     if (!isNull_35) {\n",
      "/* 141 */\n",
      "/* 142 */       Object funcResult_17 = null;\n",
      "/* 143 */       funcResult_17 = value_36.toString();\n",
      "/* 144 */       value_35 = (java.lang.String) funcResult_17;\n",
      "/* 145 */\n",
      "/* 146 */     }\n",
      "/* 147 */     if (isNull_35) {\n",
      "/* 148 */       values_0[17] = null;\n",
      "/* 149 */     } else {\n",
      "/* 150 */       values_0[17] = value_35;\n",
      "/* 151 */     }\n",
      "/* 152 */\n",
      "/* 153 */   }\n",
      "/* 154 */\n",
      "/* 155 */\n",
      "/* 156 */   private void createExternalRow_0_4(InternalRow i, Object[] values_0) {\n",
      "/* 157 */\n",
      "/* 158 */     UTF8String value_26 = i.getUTF8String(12);\n",
      "/* 159 */     boolean isNull_25 = true;\n",
      "/* 160 */     java.lang.String value_25 = null;\n",
      "/* 161 */     isNull_25 = false;\n",
      "/* 162 */     if (!isNull_25) {\n",
      "/* 163 */\n",
      "/* 164 */       Object funcResult_12 = null;\n",
      "/* 165 */       funcResult_12 = value_26.toString();\n",
      "/* 166 */       value_25 = (java.lang.String) funcResult_12;\n",
      "/* 167 */\n",
      "/* 168 */     }\n",
      "/* 169 */     if (isNull_25) {\n",
      "/* 170 */       values_0[12] = null;\n",
      "/* 171 */     } else {\n",
      "/* 172 */       values_0[12] = value_25;\n",
      "/* 173 */     }\n",
      "/* 174 */\n",
      "/* 175 */     UTF8String value_28 = i.getUTF8String(13);\n",
      "/* 176 */     boolean isNull_27 = true;\n",
      "/* 177 */     java.lang.String value_27 = null;\n",
      "/* 178 */     isNull_27 = false;\n",
      "/* 179 */     if (!isNull_27) {\n",
      "/* 180 */\n",
      "/* 181 */       Object funcResult_13 = null;\n",
      "/* 182 */       funcResult_13 = value_28.toString();\n",
      "/* 183 */       value_27 = (java.lang.String) funcResult_13;\n",
      "/* 184 */\n",
      "/* 185 */     }\n",
      "/* 186 */     if (isNull_27) {\n",
      "/* 187 */       values_0[13] = null;\n",
      "/* 188 */     } else {\n",
      "/* 189 */       values_0[13] = value_27;\n",
      "/* 190 */     }\n",
      "/* 191 */\n",
      "/* 192 */     UTF8String value_30 = i.getUTF8String(14);\n",
      "/* 193 */     boolean isNull_29 = true;\n",
      "/* 194 */     java.lang.String value_29 = null;\n",
      "/* 195 */     isNull_29 = false;\n",
      "/* 196 */     if (!isNull_29) {\n",
      "/* 197 */\n",
      "/* 198 */       Object funcResult_14 = null;\n",
      "/* 199 */       funcResult_14 = value_30.toString();\n",
      "/* 200 */       value_29 = (java.lang.String) funcResult_14;\n",
      "/* 201 */\n",
      "/* 202 */     }\n",
      "/* 203 */     if (isNull_29) {\n",
      "/* 204 */       values_0[14] = null;\n",
      "/* 205 */     } else {\n",
      "/* 206 */       values_0[14] = value_29;\n",
      "/* 207 */     }\n",
      "/* 208 */\n",
      "/* 209 */   }\n",
      "/* 210 */\n",
      "/* 211 */\n",
      "/* 212 */   private void createExternalRow_0_7(InternalRow i, Object[] values_0) {\n",
      "/* 213 */\n",
      "/* 214 */     UTF8String value_44 = i.getUTF8String(21);\n",
      "/* 215 */     boolean isNull_43 = true;\n",
      "/* 216 */     java.lang.String value_43 = null;\n",
      "/* 217 */     isNull_43 = false;\n",
      "/* 218 */     if (!isNull_43) {\n",
      "/* 219 */\n",
      "/* 220 */       Object funcResult_21 = null;\n",
      "/* 221 */       funcResult_21 = value_44.toString();\n",
      "/* 222 */       value_43 = (java.lang.String) funcResult_21;\n",
      "/* 223 */\n",
      "/* 224 */     }\n",
      "/* 225 */     if (isNull_43) {\n",
      "/* 226 */       values_0[21] = null;\n",
      "/* 227 */     } else {\n",
      "/* 228 */       values_0[21] = value_43;\n",
      "/* 229 */     }\n",
      "/* 230 */\n",
      "/* 231 */     UTF8String value_46 = i.getUTF8String(22);\n",
      "/* 232 */     boolean isNull_45 = true;\n",
      "/* 233 */     java.lang.String value_45 = null;\n",
      "/* 234 */     isNull_45 = false;\n",
      "/* 235 */     if (!isNull_45) {\n",
      "/* 236 */\n",
      "/* 237 */       Object funcResult_22 = null;\n",
      "/* 238 */       funcResult_22 = value_46.toString();\n",
      "/* 239 */       value_45 = (java.lang.String) funcResult_22;\n",
      "/* 240 */\n",
      "/* 241 */     }\n",
      "/* 242 */     if (isNull_45) {\n",
      "/* 243 */       values_0[22] = null;\n",
      "/* 244 */     } else {\n",
      "/* 245 */       values_0[22] = value_45;\n",
      "/* 246 */     }\n",
      "/* 247 */\n",
      "/* 248 */     UTF8String value_48 = i.getUTF8String(23);\n",
      "/* 249 */     boolean isNull_47 = true;\n",
      "/* 250 */     java.lang.String value_47 = null;\n",
      "/* 251 */     isNull_47 = false;\n",
      "/* 252 */     if (!isNull_47) {\n",
      "/* 253 */\n",
      "/* 254 */       Object funcResult_23 = null;\n",
      "/* 255 */       funcResult_23 = value_48.toString();\n",
      "/* 256 */       value_47 = (java.lang.String) funcResult_23;\n",
      "/* 257 */\n",
      "/* 258 */     }\n",
      "/* 259 */     if (isNull_47) {\n",
      "/* 260 */       values_0[23] = null;\n",
      "/* 261 */     } else {\n",
      "/* 262 */       values_0[23] = value_47;\n",
      "/* 263 */     }\n",
      "/* 264 */\n",
      "/* 265 */   }\n",
      "/* 266 */\n",
      "/* 267 */\n",
      "/* 268 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 269 */\n",
      "/* 270 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 271 */     boolean isNull_7 = true;\n",
      "/* 272 */     java.lang.String value_7 = null;\n",
      "/* 273 */     isNull_7 = false;\n",
      "/* 274 */     if (!isNull_7) {\n",
      "/* 275 */\n",
      "/* 276 */       Object funcResult_3 = null;\n",
      "/* 277 */       funcResult_3 = value_8.toString();\n",
      "/* 278 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 279 */\n",
      "/* 280 */     }\n",
      "/* 281 */     if (isNull_7) {\n",
      "/* 282 */       values_0[3] = null;\n",
      "/* 283 */     } else {\n",
      "/* 284 */       values_0[3] = value_7;\n",
      "/* 285 */     }\n",
      "/* 286 */\n",
      "/* 287 */     UTF8String value_10 = i.getUTF8String(4);\n",
      "/* 288 */     boolean isNull_9 = true;\n",
      "/* 289 */     java.lang.String value_9 = null;\n",
      "/* 290 */     isNull_9 = false;\n",
      "/* 291 */     if (!isNull_9) {\n",
      "/* 292 */\n",
      "/* 293 */       Object funcResult_4 = null;\n",
      "/* 294 */       funcResult_4 = value_10.toString();\n",
      "/* 295 */       value_9 = (java.lang.String) funcResult_4;\n",
      "/* 296 */\n",
      "/* 297 */     }\n",
      "/* 298 */     if (isNull_9) {\n",
      "/* 299 */       values_0[4] = null;\n",
      "/* 300 */     } else {\n",
      "/* 301 */       values_0[4] = value_9;\n",
      "/* 302 */     }\n",
      "/* 303 */\n",
      "/* 304 */     UTF8String value_12 = i.getUTF8String(5);\n",
      "/* 305 */     boolean isNull_11 = true;\n",
      "/* 306 */     java.lang.String value_11 = null;\n",
      "/* 307 */     isNull_11 = false;\n",
      "/* 308 */     if (!isNull_11) {\n",
      "/* 309 */\n",
      "/* 310 */       Object funcResult_5 = null;\n",
      "/* 311 */       funcResult_5 = value_12.toString();\n",
      "/* 312 */       value_11 = (java.lang.String) funcResult_5;\n",
      "/* 313 */\n",
      "/* 314 */     }\n",
      "/* 315 */     if (isNull_11) {\n",
      "/* 316 */       values_0[5] = null;\n",
      "/* 317 */     } else {\n",
      "/* 318 */       values_0[5] = value_11;\n",
      "/* 319 */     }\n",
      "/* 320 */\n",
      "/* 321 */   }\n",
      "/* 322 */\n",
      "/* 323 */\n",
      "/* 324 */   private void createExternalRow_0_3(InternalRow i, Object[] values_0) {\n",
      "/* 325 */\n",
      "/* 326 */     UTF8String value_20 = i.getUTF8String(9);\n",
      "/* 327 */     boolean isNull_19 = true;\n",
      "/* 328 */     java.lang.String value_19 = null;\n",
      "/* 329 */     isNull_19 = false;\n",
      "/* 330 */     if (!isNull_19) {\n",
      "/* 331 */\n",
      "/* 332 */       Object funcResult_9 = null;\n",
      "/* 333 */       funcResult_9 = value_20.toString();\n",
      "/* 334 */       value_19 = (java.lang.String) funcResult_9;\n",
      "/* 335 */\n",
      "/* 336 */     }\n",
      "/* 337 */     if (isNull_19) {\n",
      "/* 338 */       values_0[9] = null;\n",
      "/* 339 */     } else {\n",
      "/* 340 */       values_0[9] = value_19;\n",
      "/* 341 */     }\n",
      "/* 342 */\n",
      "/* 343 */     UTF8String value_22 = i.getUTF8String(10);\n",
      "/* 344 */     boolean isNull_21 = true;\n",
      "/* 345 */     java.lang.String value_21 = null;\n",
      "/* 346 */     isNull_21 = false;\n",
      "/* 347 */     if (!isNull_21) {\n",
      "/* 348 */\n",
      "/* 349 */       Object funcResult_10 = null;\n",
      "/* 350 */       funcResult_10 = value_22.toString();\n",
      "/* 351 */       value_21 = (java.lang.String) funcResult_10;\n",
      "/* 352 */\n",
      "/* 353 */     }\n",
      "/* 354 */     if (isNull_21) {\n",
      "/* 355 */       values_0[10] = null;\n",
      "/* 356 */     } else {\n",
      "/* 357 */       values_0[10] = value_21;\n",
      "/* 358 */     }\n",
      "/* 359 */\n",
      "/* 360 */     UTF8String value_24 = i.getUTF8String(11);\n",
      "/* 361 */     boolean isNull_23 = true;\n",
      "/* 362 */     java.lang.String value_23 = null;\n",
      "/* 363 */     isNull_23 = false;\n",
      "/* 364 */     if (!isNull_23) {\n",
      "/* 365 */\n",
      "/* 366 */       Object funcResult_11 = null;\n",
      "/* 367 */       funcResult_11 = value_24.toString();\n",
      "/* 368 */       value_23 = (java.lang.String) funcResult_11;\n",
      "/* 369 */\n",
      "/* 370 */     }\n",
      "/* 371 */     if (isNull_23) {\n",
      "/* 372 */       values_0[11] = null;\n",
      "/* 373 */     } else {\n",
      "/* 374 */       values_0[11] = value_23;\n",
      "/* 375 */     }\n",
      "/* 376 */\n",
      "/* 377 */   }\n",
      "/* 378 */\n",
      "/* 379 */\n",
      "/* 380 */   private void createExternalRow_0_6(InternalRow i, Object[] values_0) {\n",
      "/* 381 */\n",
      "/* 382 */     UTF8String value_38 = i.getUTF8String(18);\n",
      "/* 383 */     boolean isNull_37 = true;\n",
      "/* 384 */     java.lang.String value_37 = null;\n",
      "/* 385 */     isNull_37 = false;\n",
      "/* 386 */     if (!isNull_37) {\n",
      "/* 387 */\n",
      "/* 388 */       Object funcResult_18 = null;\n",
      "/* 389 */       funcResult_18 = value_38.toString();\n",
      "/* 390 */       value_37 = (java.lang.String) funcResult_18;\n",
      "/* 391 */\n",
      "/* 392 */     }\n",
      "/* 393 */     if (isNull_37) {\n",
      "/* 394 */       values_0[18] = null;\n",
      "/* 395 */     } else {\n",
      "/* 396 */       values_0[18] = value_37;\n",
      "/* 397 */     }\n",
      "/* 398 */\n",
      "/* 399 */     UTF8String value_40 = i.getUTF8String(19);\n",
      "/* 400 */     boolean isNull_39 = true;\n",
      "/* 401 */     java.lang.String value_39 = null;\n",
      "/* 402 */     isNull_39 = false;\n",
      "/* 403 */     if (!isNull_39) {\n",
      "/* 404 */\n",
      "/* 405 */       Object funcResult_19 = null;\n",
      "/* 406 */       funcResult_19 = value_40.toString();\n",
      "/* 407 */       value_39 = (java.lang.String) funcResult_19;\n",
      "/* 408 */\n",
      "/* 409 */     }\n",
      "/* 410 */     if (isNull_39) {\n",
      "/* 411 */       values_0[19] = null;\n",
      "/* 412 */     } else {\n",
      "/* 413 */       values_0[19] = value_39;\n",
      "/* 414 */     }\n",
      "/* 415 */\n",
      "/* 416 */     UTF8String value_42 = i.getUTF8String(20);\n",
      "/* 417 */     boolean isNull_41 = true;\n",
      "/* 418 */     java.lang.String value_41 = null;\n",
      "/* 419 */     isNull_41 = false;\n",
      "/* 420 */     if (!isNull_41) {\n",
      "/* 421 */\n",
      "/* 422 */       Object funcResult_20 = null;\n",
      "/* 423 */       funcResult_20 = value_42.toString();\n",
      "/* 424 */       value_41 = (java.lang.String) funcResult_20;\n",
      "/* 425 */\n",
      "/* 426 */     }\n",
      "/* 427 */     if (isNull_41) {\n",
      "/* 428 */       values_0[20] = null;\n",
      "/* 429 */     } else {\n",
      "/* 430 */       values_0[20] = value_41;\n",
      "/* 431 */     }\n",
      "/* 432 */\n",
      "/* 433 */   }\n",
      "/* 434 */\n",
      "/* 435 */\n",
      "/* 436 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 437 */\n",
      "/* 438 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 439 */     boolean isNull_1 = true;\n",
      "/* 440 */     java.lang.String value_1 = null;\n",
      "/* 441 */     isNull_1 = false;\n",
      "/* 442 */     if (!isNull_1) {\n",
      "/* 443 */\n",
      "/* 444 */       Object funcResult_0 = null;\n",
      "/* 445 */       funcResult_0 = value_2.toString();\n",
      "/* 446 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 447 */\n",
      "/* 448 */     }\n",
      "/* 449 */     if (isNull_1) {\n",
      "/* 450 */       values_0[0] = null;\n",
      "/* 451 */     } else {\n",
      "/* 452 */       values_0[0] = value_1;\n",
      "/* 453 */     }\n",
      "/* 454 */\n",
      "/* 455 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 456 */     boolean isNull_3 = true;\n",
      "/* 457 */     java.lang.String value_3 = null;\n",
      "/* 458 */     isNull_3 = false;\n",
      "/* 459 */     if (!isNull_3) {\n",
      "/* 460 */\n",
      "/* 461 */       Object funcResult_1 = null;\n",
      "/* 462 */       funcResult_1 = value_4.toString();\n",
      "/* 463 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 464 */\n",
      "/* 465 */     }\n",
      "/* 466 */     if (isNull_3) {\n",
      "/* 467 */       values_0[1] = null;\n",
      "/* 468 */     } else {\n",
      "/* 469 */       values_0[1] = value_3;\n",
      "/* 470 */     }\n",
      "/* 471 */\n",
      "/* 472 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 473 */     boolean isNull_5 = true;\n",
      "/* 474 */     java.lang.String value_5 = null;\n",
      "/* 475 */     isNull_5 = false;\n",
      "/* 476 */     if (!isNull_5) {\n",
      "/* 477 */\n",
      "/* 478 */       Object funcResult_2 = null;\n",
      "/* 479 */       funcResult_2 = value_6.toString();\n",
      "/* 480 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 481 */\n",
      "/* 482 */     }\n",
      "/* 483 */     if (isNull_5) {\n",
      "/* 484 */       values_0[2] = null;\n",
      "/* 485 */     } else {\n",
      "/* 486 */       values_0[2] = value_5;\n",
      "/* 487 */     }\n",
      "/* 488 */\n",
      "/* 489 */   }\n",
      "/* 490 */\n",
      "/* 491 */ }\n",
      "\n",
      "18:14:57.737 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5935\n",
      "18:14:57.780 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5935\n",
      "18:14:57.780 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5517)\n",
      "18:14:57.780 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5517\n",
      "18:14:57.780 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5517\n",
      "18:14:57.780 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5714)\n",
      "18:14:57.780 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5714\n",
      "18:14:57.780 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5714\n",
      "18:14:57.786 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5785)\n",
      "18:14:57.786 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5785\n",
      "18:14:57.786 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5785\n",
      "18:14:57.786 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5656)\n",
      "18:14:57.786 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5656\n",
      "18:14:57.786 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5656\n",
      "18:14:57.786 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5677)\n",
      "18:14:57.786 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5677\n",
      "18:14:57.786 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5677\n",
      "18:14:57.786 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5297)\n",
      "18:14:57.788 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5297\n",
      "18:14:57.788 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5297\n",
      "18:14:57.788 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5577)\n",
      "18:14:57.788 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5577\n",
      "18:14:57.788 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5577\n",
      "18:14:57.788 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5475)\n",
      "18:14:57.788 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5475\n",
      "18:14:57.788 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5475\n",
      "18:14:57.788 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5455)\n",
      "18:14:57.790 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5455\n",
      "18:14:57.790 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5455\n",
      "18:14:57.790 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5567)\n",
      "18:14:57.790 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5567\n",
      "18:14:57.790 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5567\n",
      "18:14:57.790 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5423)\n",
      "18:14:57.790 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5423\n",
      "18:14:57.790 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5423\n",
      "18:14:57.790 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5383)\n",
      "18:14:57.792 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5383\n",
      "18:14:57.792 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5383\n",
      "18:14:57.792 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5743)\n",
      "18:14:57.792 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5743\n",
      "18:14:57.792 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5743\n",
      "18:14:57.792 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5777)\n",
      "18:14:57.792 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5777\n",
      "18:14:57.792 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5777\n",
      "18:14:57.792 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5825)\n",
      "18:14:57.792 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5825\n",
      "18:14:57.794 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5825\n",
      "18:14:57.794 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5386)\n",
      "18:14:57.794 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5386\n",
      "18:14:57.794 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5386\n",
      "18:14:57.794 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5858)\n",
      "18:14:57.794 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5858\n",
      "18:14:57.794 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5858\n",
      "18:14:57.794 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5859)\n",
      "18:14:57.794 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5859\n",
      "18:14:57.796 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5859\n",
      "18:14:57.796 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5335)\n",
      "18:14:57.796 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5335\n",
      "18:14:57.796 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5335\n",
      "18:14:57.796 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5379)\n",
      "18:14:57.796 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5379\n",
      "18:14:57.796 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5379\n",
      "18:14:57.796 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5435)\n",
      "18:14:57.796 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5435\n",
      "18:14:57.796 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5435\n",
      "18:14:57.799 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5384)\n",
      "18:14:57.799 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5384\n",
      "18:14:57.799 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5384\n",
      "18:14:57.799 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5807)\n",
      "18:14:57.799 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5807\n",
      "18:14:57.799 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5807\n",
      "18:14:57.799 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5287)\n",
      "18:14:57.799 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5287\n",
      "18:14:57.799 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5287\n",
      "18:14:57.801 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5536)\n",
      "18:14:57.801 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5536\n",
      "18:14:57.801 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5536\n",
      "18:14:57.801 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5696)\n",
      "18:14:57.801 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5696\n",
      "18:14:57.801 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5696\n",
      "18:14:57.801 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5325)\n",
      "18:14:57.801 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5325\n",
      "18:14:57.801 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5325\n",
      "18:14:57.802 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5530)\n",
      "18:14:57.802 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5530\n",
      "18:14:57.802 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5530\n",
      "18:14:57.802 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5824)\n",
      "18:14:57.802 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5824\n",
      "18:14:57.802 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5824\n",
      "18:14:57.802 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5632)\n",
      "18:14:57.802 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5632\n",
      "18:14:57.802 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5632\n",
      "18:14:57.802 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5559)\n",
      "18:14:57.804 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5559\n",
      "18:14:57.804 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5559\n",
      "18:14:57.804 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5823)\n",
      "18:14:57.804 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5823\n",
      "18:14:57.804 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5823\n",
      "18:14:57.804 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5844)\n",
      "18:14:57.804 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5844\n",
      "18:14:57.804 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5844\n",
      "18:14:57.804 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5782)\n",
      "18:14:57.806 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5782\n",
      "18:14:57.806 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5782\n",
      "18:14:57.806 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5519)\n",
      "18:14:57.806 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5519\n",
      "18:14:57.806 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5519\n",
      "18:14:57.806 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5765)\n",
      "18:14:57.806 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5765\n",
      "18:14:57.806 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5765\n",
      "18:14:57.806 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5603)\n",
      "18:14:57.810 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5603\n",
      "18:14:57.810 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5603\n",
      "18:14:57.810 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5909)\n",
      "18:14:57.810 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5909\n",
      "18:14:57.810 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5909\n",
      "18:14:57.810 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanBroadcast(105)\n",
      "18:14:57.810 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning broadcast 105\n",
      "18:14:57.810 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast -- Unpersisting TorrentBroadcast 105\n",
      "18:14:57.811 [block-manager-storage-async-thread-pool-360] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- removing broadcast 105\n",
      "18:14:57.811 [block-manager-storage-async-thread-pool-360] DEBUG org.apache.spark.storage.BlockManager -- Removing broadcast 105\n",
      "18:14:57.811 [block-manager-storage-async-thread-pool-360] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_105_piece0\n",
      "18:14:57.811 [block-manager-storage-async-thread-pool-360] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_105_piece0 of size 7611 dropped from memory (free 455042366)\n",
      "18:14:57.811 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_105_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:57.811 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Removed broadcast_105_piece0 on 10.0.0.146:57286 in memory (size: 7.4 KiB, free: 434.3 MiB)\n",
      "18:14:57.811 [block-manager-storage-async-thread-pool-360] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_105_piece0\n",
      "18:14:57.811 [block-manager-storage-async-thread-pool-360] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_105_piece0\n",
      "18:14:57.811 [block-manager-storage-async-thread-pool-360] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_105\n",
      "18:14:57.811 [block-manager-storage-async-thread-pool-360] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_105 of size 28224 dropped from memory (free 455070590)\n",
      "18:14:57.811 [block-manager-storage-async-thread-pool-362] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Done removing broadcast 105, response is 0\n",
      "18:14:57.811 [block-manager-storage-async-thread-pool-362] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Sent response: 0 to 10.0.0.146:57282\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned broadcast 105\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5324)\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5324\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5324\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5672)\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5672\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5672\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanShuffle(27)\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning shuffle 27\n",
      "18:14:57.812 [block-manager-storage-async-thread-pool-363] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- removing shuffle 27\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned shuffle 27\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5594)\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5594\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5594\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5597)\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5597\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5597\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5646)\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5646\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5646\n",
      "18:14:57.812 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5849)\n",
      "18:14:57.813 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5849\n",
      "18:14:57.814 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5849\n",
      "18:14:57.814 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5680)\n",
      "18:14:57.814 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5680\n",
      "18:14:57.814 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5680\n",
      "18:14:57.814 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5735)\n",
      "18:14:57.814 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5735\n",
      "18:14:57.814 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5735\n",
      "18:14:57.814 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5288)\n",
      "18:14:57.812 [block-manager-storage-async-thread-pool-365] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Done removing shuffle 27, response is true\n",
      "18:14:57.814 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5288\n",
      "18:14:57.814 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5288\n",
      "18:14:57.814 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5512)\n",
      "18:14:57.814 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5512\n",
      "18:14:57.814 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5512\n",
      "18:14:57.814 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5725)\n",
      "18:14:57.814 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5725\n",
      "18:14:57.814 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5725\n",
      "18:14:57.815 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5448)\n",
      "18:14:57.815 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5448\n",
      "18:14:57.815 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5448\n",
      "18:14:57.815 [block-manager-storage-async-thread-pool-365] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Sent response: true to 10.0.0.146:57282\n",
      "18:14:57.815 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5307)\n",
      "18:14:57.815 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5307\n",
      "18:14:57.815 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5307\n",
      "18:14:57.815 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5409)\n",
      "18:14:57.815 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5409\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5409\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5531)\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5531\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5531\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5499)\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5499\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5499\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5611)\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5611\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5611\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5852)\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5852\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5852\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5802)\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5802\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5802\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5563)\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5563\n",
      "18:14:57.816 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5563\n",
      "18:14:57.817 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5923)\n",
      "18:14:57.817 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5923\n",
      "18:14:57.817 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5923\n",
      "18:14:57.817 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5739)\n",
      "18:14:57.817 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5739\n",
      "18:14:57.817 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5739\n",
      "18:14:57.817 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5315)\n",
      "18:14:57.817 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5315\n",
      "18:14:57.817 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5315\n",
      "18:14:57.818 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5445)\n",
      "18:14:57.818 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5445\n",
      "18:14:57.818 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5445\n",
      "18:14:57.818 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5593)\n",
      "18:14:57.818 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5593\n",
      "18:14:57.818 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5593\n",
      "18:14:57.818 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5763)\n",
      "18:14:57.818 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5763\n",
      "18:14:57.818 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5763\n",
      "18:14:57.818 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5431)\n",
      "18:14:57.818 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5431\n",
      "18:14:57.818 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5431\n",
      "18:14:57.818 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanBroadcast(102)\n",
      "18:14:57.818 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning broadcast 102\n",
      "18:14:57.818 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast -- Unpersisting TorrentBroadcast 102\n",
      "18:14:57.819 [block-manager-storage-async-thread-pool-366] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- removing broadcast 102\n",
      "18:14:57.819 [block-manager-storage-async-thread-pool-366] DEBUG org.apache.spark.storage.BlockManager -- Removing broadcast 102\n",
      "18:14:57.819 [block-manager-storage-async-thread-pool-366] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_102_piece0\n",
      "18:14:57.820 [block-manager-storage-async-thread-pool-366] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_102_piece0 of size 18174 dropped from memory (free 455088764)\n",
      "18:14:57.820 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_102_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:57.820 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Removed broadcast_102_piece0 on 10.0.0.146:57286 in memory (size: 17.7 KiB, free: 434.3 MiB)\n",
      "18:14:57.820 [block-manager-storage-async-thread-pool-366] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_102_piece0\n",
      "18:14:57.820 [block-manager-storage-async-thread-pool-366] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_102_piece0\n",
      "18:14:57.820 [block-manager-storage-async-thread-pool-366] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_102\n",
      "18:14:57.820 [block-manager-storage-async-thread-pool-366] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_102 of size 39760 dropped from memory (free 455128524)\n",
      "18:14:57.824 [block-manager-storage-async-thread-pool-368] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Done removing broadcast 102, response is 0\n",
      "18:14:57.827 [block-manager-storage-async-thread-pool-368] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Sent response: 0 to 10.0.0.146:57282\n",
      "18:14:57.827 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned broadcast 102\n",
      "18:14:57.827 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5693)\n",
      "18:14:57.827 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5693\n",
      "18:14:57.827 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5693\n",
      "18:14:57.829 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5753)\n",
      "18:14:57.829 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5753\n",
      "18:14:57.829 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5753\n",
      "18:14:57.829 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5657)\n",
      "18:14:57.829 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5657\n",
      "18:14:57.829 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5657\n",
      "18:14:57.829 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5377)\n",
      "18:14:57.829 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5377\n",
      "18:14:57.829 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5377\n",
      "18:14:57.856 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5548)\n",
      "18:14:57.856 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5548\n",
      "18:14:57.856 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5548\n",
      "18:14:57.856 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5924)\n",
      "18:14:57.856 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5924\n",
      "18:14:57.856 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5924\n",
      "18:14:57.856 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5401)\n",
      "18:14:57.856 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5401\n",
      "18:14:57.856 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5401\n",
      "18:14:57.856 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5544)\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5544\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5544\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5660)\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5660\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5660\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5418)\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5418\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5418\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5751)\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5751\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5751\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5333)\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5333\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5333\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5835)\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5835\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5835\n",
      "18:14:57.859 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5623)\n",
      "18:14:57.860 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5623\n",
      "18:14:57.860 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5623\n",
      "18:14:57.860 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5492)\n",
      "18:14:57.860 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5492\n",
      "18:14:57.860 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5492\n",
      "18:14:57.860 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5374)\n",
      "18:14:57.860 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5374\n",
      "18:14:57.860 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5374\n",
      "18:14:57.860 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5380)\n",
      "18:14:57.860 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5380\n",
      "18:14:57.862 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5380\n",
      "18:14:57.862 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5642)\n",
      "18:14:57.862 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5642\n",
      "18:14:57.862 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5642\n",
      "18:14:57.862 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5716)\n",
      "18:14:57.862 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5716\n",
      "18:14:57.862 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5716\n",
      "18:14:57.862 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5291)\n",
      "18:14:57.862 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5291\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5291\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5792)\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5792\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5792\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5734)\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5734\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5734\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5300)\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5300\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5300\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5484)\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5484\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5484\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5851)\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5851\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5851\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5922)\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5922\n",
      "18:14:57.863 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5922\n",
      "18:14:57.864 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5805)\n",
      "18:14:57.864 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5805\n",
      "18:14:57.864 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5805\n",
      "18:14:57.864 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5466)\n",
      "18:14:57.864 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5466\n",
      "18:14:57.864 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5466\n",
      "18:14:57.864 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5701)\n",
      "18:14:57.864 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5701\n",
      "18:14:57.864 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5701\n",
      "18:14:57.865 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanShuffle(28)\n",
      "18:14:57.865 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning shuffle 28\n",
      "18:14:57.865 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned shuffle 28\n",
      "18:14:57.865 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5721)\n",
      "18:14:57.865 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5721\n",
      "18:14:57.865 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5721\n",
      "18:14:57.865 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5919)\n",
      "18:14:57.865 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5919\n",
      "18:14:57.865 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5919\n",
      "18:14:57.865 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanBroadcast(99)\n",
      "18:14:57.865 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning broadcast 99\n",
      "18:14:57.865 [block-manager-storage-async-thread-pool-369] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- removing shuffle 28\n",
      "18:14:57.865 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast -- Unpersisting TorrentBroadcast 99\n",
      "18:14:57.865 [block-manager-storage-async-thread-pool-370] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- removing broadcast 99\n",
      "18:14:57.865 [block-manager-storage-async-thread-pool-370] DEBUG org.apache.spark.storage.BlockManager -- Removing broadcast 99\n",
      "18:14:57.865 [block-manager-storage-async-thread-pool-370] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_99\n",
      "18:14:57.865 [block-manager-storage-async-thread-pool-370] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_99 of size 32792 dropped from memory (free 455161316)\n",
      "18:14:57.866 [block-manager-storage-async-thread-pool-372] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Done removing shuffle 28, response is true\n",
      "18:14:57.866 [block-manager-storage-async-thread-pool-370] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_99_piece0\n",
      "18:14:57.866 [block-manager-storage-async-thread-pool-370] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_99_piece0 of size 3933 dropped from memory (free 455165249)\n",
      "18:14:57.866 [block-manager-storage-async-thread-pool-372] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Sent response: true to 10.0.0.146:57282\n",
      "18:14:57.866 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_99_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:57.866 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Removed broadcast_99_piece0 on 10.0.0.146:57286 in memory (size: 3.8 KiB, free: 434.3 MiB)\n",
      "18:14:57.867 [block-manager-storage-async-thread-pool-370] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_99_piece0\n",
      "18:14:57.867 [block-manager-storage-async-thread-pool-370] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_99_piece0\n",
      "18:14:57.867 [block-manager-storage-async-thread-pool-374] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Done removing broadcast 99, response is 0\n",
      "18:14:57.867 [block-manager-storage-async-thread-pool-374] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Sent response: 0 to 10.0.0.146:57282\n",
      "18:14:57.867 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned broadcast 99\n",
      "18:14:57.867 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5465)\n",
      "18:14:57.867 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5465\n",
      "18:14:57.867 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5465\n",
      "18:14:57.867 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5707)\n",
      "18:14:57.867 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5707\n",
      "18:14:57.867 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5707\n",
      "18:14:57.867 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5583)\n",
      "18:14:57.867 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5583\n",
      "18:14:57.867 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5583\n",
      "18:14:57.867 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5595)\n",
      "18:14:57.867 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5595\n",
      "18:14:57.867 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5595\n",
      "18:14:57.868 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5469)\n",
      "18:14:57.868 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5469\n",
      "18:14:57.868 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5469\n",
      "18:14:57.868 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5727)\n",
      "18:14:57.868 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5727\n",
      "18:14:57.868 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5727\n",
      "18:14:57.868 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5648)\n",
      "18:14:57.868 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5648\n",
      "18:14:57.868 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5648\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5538)\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5538\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5538\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5438)\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5438\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5438\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5560)\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5560\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5560\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5662)\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5662\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5662\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5539)\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5539\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5539\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5814)\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5814\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5814\n",
      "18:14:57.879 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5313)\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5313\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5313\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5708)\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5708\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5708\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5456)\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5456\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5456\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5596)\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5596\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5596\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5591)\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5591\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5591\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5407)\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5407\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5407\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5845)\n",
      "18:14:57.880 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5845\n",
      "18:14:57.881 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5845\n",
      "18:14:57.881 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5532)\n",
      "18:14:57.881 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5532\n",
      "18:14:57.881 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5532\n",
      "18:14:57.881 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5842)\n",
      "18:14:57.881 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5842\n",
      "18:14:57.881 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5842\n",
      "18:14:57.881 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5710)\n",
      "18:14:57.881 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5710\n",
      "18:14:57.882 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5710\n",
      "18:14:57.882 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5930)\n",
      "18:14:57.882 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5930\n",
      "18:14:57.882 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5930\n",
      "18:14:57.882 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5795)\n",
      "18:14:57.882 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5795\n",
      "18:14:57.882 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5795\n",
      "18:14:57.882 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5317)\n",
      "18:14:57.882 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5317\n",
      "18:14:57.886 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5317\n",
      "18:14:57.886 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5712)\n",
      "18:14:57.886 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5712\n",
      "18:14:57.886 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5712\n",
      "18:14:57.886 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5327)\n",
      "18:14:57.886 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5327\n",
      "18:14:57.886 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5327\n",
      "18:14:57.886 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5476)\n",
      "18:14:57.886 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5476\n",
      "18:14:57.886 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5476\n",
      "18:14:57.888 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5391)\n",
      "18:14:57.888 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5391\n",
      "18:14:57.888 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5391\n",
      "18:14:57.888 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5537)\n",
      "18:14:57.888 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5537\n",
      "18:14:57.888 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5537\n",
      "18:14:57.888 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5556)\n",
      "18:14:57.888 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5556\n",
      "18:14:57.888 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5556\n",
      "18:14:57.891 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5549)\n",
      "18:14:57.891 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5549\n",
      "18:14:57.891 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5549\n",
      "18:14:57.891 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5828)\n",
      "18:14:57.891 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5828\n",
      "18:14:57.891 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5828\n",
      "18:14:57.891 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5671)\n",
      "18:14:57.891 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5671\n",
      "18:14:57.891 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5671\n",
      "18:14:57.891 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5676)\n",
      "18:14:57.892 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5676\n",
      "18:14:57.892 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5676\n",
      "18:14:57.892 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5493)\n",
      "18:14:57.892 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5493\n",
      "18:14:57.892 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5493\n",
      "18:14:57.892 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5321)\n",
      "18:14:57.892 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5321\n",
      "18:14:57.892 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5321\n",
      "18:14:57.892 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5510)\n",
      "18:14:57.894 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5510\n",
      "18:14:57.894 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5510\n",
      "18:14:57.894 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5664)\n",
      "18:14:57.894 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5664\n",
      "18:14:57.894 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5664\n",
      "18:14:57.894 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5404)\n",
      "18:14:57.894 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5404\n",
      "18:14:57.894 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5404\n",
      "18:14:57.894 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5513)\n",
      "18:14:57.918 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5513\n",
      "18:14:57.918 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5513\n",
      "18:14:57.918 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5817)\n",
      "18:14:57.918 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5817\n",
      "18:14:57.918 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5817\n",
      "18:14:57.918 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5292)\n",
      "18:14:57.918 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5292\n",
      "18:14:57.918 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5292\n",
      "18:14:57.918 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5745)\n",
      "18:14:57.918 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5745\n",
      "18:14:57.920 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5745\n",
      "18:14:57.920 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5429)\n",
      "18:14:57.920 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5429\n",
      "18:14:57.920 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5429\n",
      "18:14:57.920 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5610)\n",
      "18:14:57.920 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5610\n",
      "18:14:57.920 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5610\n",
      "18:14:57.920 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5483)\n",
      "18:14:57.920 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5483\n",
      "18:14:57.922 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5483\n",
      "18:14:57.922 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5832)\n",
      "18:14:57.922 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5832\n",
      "18:14:57.922 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5832\n",
      "18:14:57.922 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5490)\n",
      "18:14:57.922 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5490\n",
      "18:14:57.922 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5490\n",
      "18:14:57.922 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5310)\n",
      "18:14:57.922 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5310\n",
      "18:14:57.923 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5310\n",
      "18:14:57.924 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5479)\n",
      "18:14:57.924 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5479\n",
      "18:14:57.924 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5479\n",
      "18:14:57.924 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5301)\n",
      "18:14:57.924 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5301\n",
      "18:14:57.924 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5301\n",
      "18:14:57.924 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanShuffle(29)\n",
      "18:14:57.924 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning shuffle 29\n",
      "18:14:57.924 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned shuffle 29\n",
      "18:14:57.924 [block-manager-storage-async-thread-pool-375] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- removing shuffle 29\n",
      "18:14:57.926 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5389)\n",
      "18:14:57.926 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5389\n",
      "18:14:57.926 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5389\n",
      "18:14:57.926 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5786)\n",
      "18:14:57.926 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5786\n",
      "18:14:57.926 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5786\n",
      "18:14:57.926 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5798)\n",
      "18:14:57.926 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5798\n",
      "18:14:57.927 [block-manager-storage-async-thread-pool-377] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Done removing shuffle 29, response is true\n",
      "18:14:57.928 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5798\n",
      "18:14:57.928 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanBroadcast(100)\n",
      "18:14:57.928 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning broadcast 100\n",
      "18:14:57.928 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast -- Unpersisting TorrentBroadcast 100\n",
      "18:14:57.929 [block-manager-storage-async-thread-pool-377] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Sent response: true to 10.0.0.146:57282\n",
      "18:14:57.929 [block-manager-storage-async-thread-pool-378] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- removing broadcast 100\n",
      "18:14:57.929 [block-manager-storage-async-thread-pool-378] DEBUG org.apache.spark.storage.BlockManager -- Removing broadcast 100\n",
      "18:14:57.931 [block-manager-storage-async-thread-pool-378] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_100_piece0\n",
      "18:14:57.931 [block-manager-storage-async-thread-pool-378] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_100_piece0 of size 3933 dropped from memory (free 455169182)\n",
      "18:14:57.931 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_100_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:57.931 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Removed broadcast_100_piece0 on 10.0.0.146:57286 in memory (size: 3.8 KiB, free: 434.3 MiB)\n",
      "18:14:57.931 [block-manager-storage-async-thread-pool-378] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_100_piece0\n",
      "18:14:57.931 [block-manager-storage-async-thread-pool-378] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_100_piece0\n",
      "18:14:57.932 [block-manager-storage-async-thread-pool-378] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_100\n",
      "18:14:57.932 [block-manager-storage-async-thread-pool-378] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_100 of size 32792 dropped from memory (free 455201974)\n",
      "18:14:57.933 [block-manager-storage-async-thread-pool-380] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Done removing broadcast 100, response is 0\n",
      "18:14:57.933 [block-manager-storage-async-thread-pool-380] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Sent response: 0 to 10.0.0.146:57282\n",
      "18:14:57.933 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned broadcast 100\n",
      "18:14:57.933 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5715)\n",
      "18:14:57.933 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5715\n",
      "18:14:57.934 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5715\n",
      "18:14:57.934 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5507)\n",
      "18:14:57.934 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5507\n",
      "18:14:57.934 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5507\n",
      "18:14:57.934 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5584)\n",
      "18:14:57.934 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5584\n",
      "18:14:57.934 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5584\n",
      "18:14:57.934 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5804)\n",
      "18:14:57.934 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5804\n",
      "18:14:57.937 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5804\n",
      "18:14:57.937 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5920)\n",
      "18:14:57.937 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5920\n",
      "18:14:57.937 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5920\n",
      "18:14:57.937 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5752)\n",
      "18:14:57.937 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5752\n",
      "18:14:57.937 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5752\n",
      "18:14:57.937 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5534)\n",
      "18:14:57.937 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5534\n",
      "18:14:57.938 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5534\n",
      "18:14:57.939 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5428)\n",
      "18:14:57.939 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5428\n",
      "18:14:57.939 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5428\n",
      "18:14:57.939 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5815)\n",
      "18:14:57.939 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5815\n",
      "18:14:57.939 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5815\n",
      "18:14:57.939 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5730)\n",
      "18:14:57.939 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5730\n",
      "18:14:57.939 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5730\n",
      "18:14:57.941 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5400)\n",
      "18:14:57.941 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5400\n",
      "18:14:57.941 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5400\n",
      "18:14:57.941 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5841)\n",
      "18:14:57.941 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5841\n",
      "18:14:57.941 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5841\n",
      "18:14:57.941 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5659)\n",
      "18:14:57.941 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5659\n",
      "18:14:57.941 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5659\n",
      "18:14:57.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5741)\n",
      "18:14:57.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5741\n",
      "18:14:57.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5741\n",
      "18:14:57.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5742)\n",
      "18:14:57.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5742\n",
      "18:14:57.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5742\n",
      "18:14:57.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5482)\n",
      "18:14:57.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5482\n",
      "18:14:57.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5482\n",
      "18:14:57.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5855)\n",
      "18:14:57.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5855\n",
      "18:14:57.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5855\n",
      "18:14:57.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5670)\n",
      "18:14:57.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5670\n",
      "18:14:57.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5670\n",
      "18:14:57.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5738)\n",
      "18:14:57.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5738\n",
      "18:14:57.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5738\n",
      "18:14:57.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5781)\n",
      "18:14:57.946 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5781\n",
      "18:14:57.946 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5781\n",
      "18:14:57.946 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5562)\n",
      "18:14:57.946 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5562\n",
      "18:14:57.946 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5562\n",
      "18:14:57.946 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5682)\n",
      "18:14:57.946 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5682\n",
      "18:14:57.946 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5682\n",
      "18:14:57.946 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5444)\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5444\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5444\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5309)\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5309\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5309\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5796)\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5796\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5796\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5464)\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5464\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5464\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5775)\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5775\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5775\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5756)\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5756\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5756\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5337)\n",
      "18:14:57.947 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5337\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5337\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5683)\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5683\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5683\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5326)\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5326\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5326\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5821)\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5821\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5821\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5505)\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5505\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5505\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5274)\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5274\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5274\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5332)\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5332\n",
      "18:14:57.948 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5332\n",
      "18:14:57.949 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5316)\n",
      "18:14:57.949 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5316\n",
      "18:14:57.949 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5316\n",
      "18:14:57.949 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5582)\n",
      "18:14:57.949 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5582\n",
      "18:14:57.949 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5582\n",
      "18:14:57.949 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5478)\n",
      "18:14:57.949 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5478\n",
      "18:14:57.949 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5478\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5801)\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5801\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5801\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5833)\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5833\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5833\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5420)\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5420\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5420\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5720)\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5720\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5720\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5523)\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5523\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5523\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5511)\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5511\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5511\n",
      "18:14:57.951 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5861)\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5861\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5861\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5826)\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5826\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5826\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5426)\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5426\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5426\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5527)\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5527\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5527\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5535)\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5535\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5535\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5736)\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5736\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5736\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanBroadcast(96)\n",
      "18:14:57.952 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning broadcast 96\n",
      "18:14:57.953 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast -- Unpersisting TorrentBroadcast 96\n",
      "18:14:57.953 [block-manager-storage-async-thread-pool-381] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- removing broadcast 96\n",
      "18:14:57.953 [block-manager-storage-async-thread-pool-381] DEBUG org.apache.spark.storage.BlockManager -- Removing broadcast 96\n",
      "18:14:57.954 [block-manager-storage-async-thread-pool-381] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_96\n",
      "18:14:57.954 [block-manager-storage-async-thread-pool-381] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_96 of size 28224 dropped from memory (free 455230198)\n",
      "18:14:57.954 [block-manager-storage-async-thread-pool-381] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_96_piece0\n",
      "18:14:57.954 [block-manager-storage-async-thread-pool-381] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_96_piece0 of size 7619 dropped from memory (free 455237817)\n",
      "18:14:57.958 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_96_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:57.958 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Removed broadcast_96_piece0 on 10.0.0.146:57286 in memory (size: 7.4 KiB, free: 434.4 MiB)\n",
      "18:14:57.958 [block-manager-storage-async-thread-pool-381] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_96_piece0\n",
      "18:14:57.958 [block-manager-storage-async-thread-pool-381] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_96_piece0\n",
      "18:14:57.959 [block-manager-storage-async-thread-pool-383] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Done removing broadcast 96, response is 0\n",
      "18:14:57.959 [block-manager-storage-async-thread-pool-383] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Sent response: 0 to 10.0.0.146:57282\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned broadcast 96\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5661)\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5661\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5661\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5787)\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5787\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5787\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5373)\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5373\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5373\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5812)\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5812\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5812\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5831)\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5831\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5831\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5308)\n",
      "18:14:57.959 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5308\n",
      "18:14:57.960 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5308\n",
      "18:14:57.960 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5607)\n",
      "18:14:57.960 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5607\n",
      "18:14:57.960 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5607\n",
      "18:14:57.960 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5520)\n",
      "18:14:57.960 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5520\n",
      "18:14:57.960 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5520\n",
      "18:14:57.960 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5545)\n",
      "18:14:57.960 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5545\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5545\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5526)\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5526\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5526\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5524)\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5524\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5524\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5694)\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5694\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5694\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5491)\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5491\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5491\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5419)\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5419\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5419\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5604)\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5604\n",
      "18:14:57.961 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5604\n",
      "18:14:57.962 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5726)\n",
      "18:14:57.962 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5726\n",
      "18:14:57.962 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5726\n",
      "18:14:57.962 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5933)\n",
      "18:14:57.962 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5933\n",
      "18:14:57.962 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5933\n",
      "18:14:57.962 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5700)\n",
      "18:14:57.962 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5700\n",
      "18:14:57.962 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5700\n",
      "18:14:57.963 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5390)\n",
      "18:14:57.963 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5390\n",
      "18:14:57.963 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5390\n",
      "18:14:57.963 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5928)\n",
      "18:14:57.963 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5928\n",
      "18:14:57.963 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5928\n",
      "18:14:57.963 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5635)\n",
      "18:14:57.963 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5635\n",
      "18:14:57.963 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5635\n",
      "18:14:57.963 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5552)\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5552\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5552\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5772)\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5772\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5772\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5681)\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5681\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5681\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5639)\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5639\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5639\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5688)\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5688\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5688\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5318)\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5318\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5318\n",
      "18:14:57.964 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5427)\n",
      "18:14:57.966 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5427\n",
      "18:14:57.966 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5427\n",
      "18:14:57.966 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5768)\n",
      "18:14:57.966 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5768\n",
      "18:14:57.966 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5768\n",
      "18:14:57.966 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5328)\n",
      "18:14:57.966 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5328\n",
      "18:14:57.966 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5328\n",
      "18:14:57.966 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5836)\n",
      "18:14:57.966 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5836\n",
      "18:14:57.978 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5836\n",
      "18:14:57.978 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5839)\n",
      "18:14:57.978 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5839\n",
      "18:14:57.978 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5839\n",
      "18:14:57.978 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5613)\n",
      "18:14:57.978 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5613\n",
      "18:14:57.978 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5613\n",
      "18:14:57.978 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5381)\n",
      "18:14:57.978 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5381\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5381\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5453)\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5453\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5453\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5588)\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5588\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5588\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5375)\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5375\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5375\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5436)\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5436\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5436\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5750)\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5750\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5750\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5589)\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5589\n",
      "18:14:57.980 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5589\n",
      "18:14:57.981 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5615)\n",
      "18:14:57.981 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5615\n",
      "18:14:57.981 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5615\n",
      "18:14:57.981 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5470)\n",
      "18:14:57.981 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5470\n",
      "18:14:57.981 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5470\n",
      "18:14:57.981 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5388)\n",
      "18:14:57.981 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5388\n",
      "18:14:57.981 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5388\n",
      "18:14:57.982 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5565)\n",
      "18:14:57.982 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5565\n",
      "18:14:57.982 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5565\n",
      "18:14:57.982 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5620)\n",
      "18:14:57.982 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5620\n",
      "18:14:57.982 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5620\n",
      "18:14:57.982 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5666)\n",
      "18:14:57.982 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5666\n",
      "18:14:57.982 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5666\n",
      "18:14:57.982 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5711)\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5711\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5711\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5498)\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5498\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5498\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5540)\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5540\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5540\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5780)\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5780\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5780\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5509)\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5509\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5509\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5747)\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5747\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5747\n",
      "18:14:57.984 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5566)\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5566\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5566\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5574)\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5574\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5574\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5737)\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5737\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5737\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5608)\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5608\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5608\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5818)\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5818\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5818\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5678)\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5678\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5678\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5533)\n",
      "18:14:57.985 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5533\n",
      "18:14:57.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5533\n",
      "18:14:57.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5663)\n",
      "18:14:57.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5663\n",
      "18:14:57.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5663\n",
      "18:14:57.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5925)\n",
      "18:14:57.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5925\n",
      "18:14:57.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5925\n",
      "18:14:57.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5576)\n",
      "18:14:57.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5576\n",
      "18:14:57.986 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5576\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5624)\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5624\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5624\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5550)\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5550\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5550\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5601)\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5601\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5601\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5665)\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5665\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5665\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5697)\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5697\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5697\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5757)\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5757\n",
      "18:14:57.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5757\n",
      "18:14:57.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5813)\n",
      "18:14:57.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5813\n",
      "18:14:57.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5813\n",
      "18:14:57.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5581)\n",
      "18:14:57.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5581\n",
      "18:14:57.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5581\n",
      "18:14:57.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5430)\n",
      "18:14:57.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5430\n",
      "18:14:57.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5430\n",
      "18:14:57.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5908)\n",
      "18:14:57.991 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5908\n",
      "18:14:57.991 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5908\n",
      "18:14:57.991 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5916)\n",
      "18:14:57.991 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5916\n",
      "18:14:57.991 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5916\n",
      "18:14:57.991 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5638)\n",
      "18:14:57.991 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5638\n",
      "18:14:57.991 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5638\n",
      "18:14:57.991 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5634)\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5634\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5634\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5692)\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5692\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5692\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5289)\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5289\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5289\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5571)\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5571\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5571\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5650)\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5650\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5650\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5744)\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5744\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5744\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5276)\n",
      "18:14:57.992 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5276\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5276\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5542)\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5542\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5542\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5628)\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5628\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5628\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5850)\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5850\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5850\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5514)\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5514\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5514\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5630)\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5630\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5630\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5932)\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5932\n",
      "18:14:57.993 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5932\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5334)\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5334\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5334\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5762)\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5762\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5762\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5690)\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5690\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5690\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5854)\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5854\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5854\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5917)\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5917\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5917\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5784)\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5784\n",
      "18:14:57.994 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5784\n",
      "18:14:57.995 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5572)\n",
      "18:14:57.995 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5572\n",
      "18:14:57.995 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5572\n",
      "18:14:57.995 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5529)\n",
      "18:14:57.995 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5529\n",
      "18:14:57.995 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5529\n",
      "18:14:57.995 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5702)\n",
      "18:14:57.995 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5702\n",
      "18:14:57.995 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5702\n",
      "18:14:57.995 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5474)\n",
      "18:14:57.996 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5474\n",
      "18:14:57.996 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5474\n",
      "18:14:57.996 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5806)\n",
      "18:14:57.996 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5806\n",
      "18:14:57.996 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5806\n",
      "18:14:57.996 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5305)\n",
      "18:14:57.996 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5305\n",
      "18:14:57.996 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5305\n",
      "18:14:57.996 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5631)\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5631\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5631\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5847)\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5847\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5847\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5912)\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5912\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5912\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5403)\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5403\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5403\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5521)\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5521\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5521\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5808)\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5808\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5808\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5614)\n",
      "18:14:57.997 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5614\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5614\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5553)\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5553\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5553\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5746)\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5746\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5746\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5319)\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5319\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5319\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5322)\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5322\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5322\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5503)\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5503\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5503\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5525)\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5525\n",
      "18:14:57.998 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5525\n",
      "18:14:57.999 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5393)\n",
      "18:14:57.999 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5393\n",
      "18:14:57.999 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5393\n",
      "18:14:57.999 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5496)\n",
      "18:14:57.999 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5496\n",
      "18:14:57.999 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5496\n",
      "18:14:57.999 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5387)\n",
      "18:14:57.999 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5387\n",
      "18:14:57.999 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5387\n",
      "18:14:57.999 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanBroadcast(94)\n",
      "18:14:57.999 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning broadcast 94\n",
      "18:14:57.999 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast -- Unpersisting TorrentBroadcast 94\n",
      "18:14:58.000 [block-manager-storage-async-thread-pool-384] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- removing broadcast 94\n",
      "18:14:58.000 [block-manager-storage-async-thread-pool-384] DEBUG org.apache.spark.storage.BlockManager -- Removing broadcast 94\n",
      "18:14:58.000 [block-manager-storage-async-thread-pool-384] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_94_piece0\n",
      "18:14:58.000 [block-manager-storage-async-thread-pool-384] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_94_piece0 of size 3862 dropped from memory (free 455241679)\n",
      "18:14:58.000 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_94_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:58.000 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Removed broadcast_94_piece0 on 10.0.0.146:57286 in memory (size: 3.8 KiB, free: 434.4 MiB)\n",
      "18:14:58.000 [block-manager-storage-async-thread-pool-384] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_94_piece0\n",
      "18:14:58.000 [block-manager-storage-async-thread-pool-384] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_94_piece0\n",
      "18:14:58.000 [block-manager-storage-async-thread-pool-384] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_94\n",
      "18:14:58.000 [block-manager-storage-async-thread-pool-384] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_94 of size 32792 dropped from memory (free 455274471)\n",
      "18:14:58.001 [block-manager-storage-async-thread-pool-386] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Done removing broadcast 94, response is 0\n",
      "18:14:58.001 [block-manager-storage-async-thread-pool-386] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Sent response: 0 to 10.0.0.146:57282\n",
      "18:14:58.001 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned broadcast 94\n",
      "18:14:58.001 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5546)\n",
      "18:14:58.001 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5546\n",
      "18:14:58.001 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5546\n",
      "18:14:58.001 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5421)\n",
      "18:14:58.001 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5421\n",
      "18:14:58.001 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5421\n",
      "18:14:58.001 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5771)\n",
      "18:14:58.001 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5771\n",
      "18:14:58.001 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5771\n",
      "18:14:58.001 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5816)\n",
      "18:14:58.001 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5816\n",
      "18:14:58.001 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5816\n",
      "18:14:58.001 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5754)\n",
      "18:14:58.001 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5754\n",
      "18:14:58.001 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5754\n",
      "18:14:58.002 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5783)\n",
      "18:14:58.002 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5783\n",
      "18:14:58.002 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5783\n",
      "18:14:58.002 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5766)\n",
      "18:14:58.002 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5766\n",
      "18:14:58.002 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5766\n",
      "18:14:58.002 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5830)\n",
      "18:14:58.002 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5830\n",
      "18:14:58.002 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5830\n",
      "18:14:58.002 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5415)\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5415\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5415\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5905)\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5905\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5905\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5564)\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5564\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5564\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5497)\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5497\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5497\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5376)\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5376\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5376\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5840)\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5840\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5840\n",
      "18:14:58.003 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5936)\n",
      "18:14:58.004 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5936\n",
      "18:14:58.004 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5936\n",
      "18:14:58.004 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5569)\n",
      "18:14:58.004 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5569\n",
      "18:14:58.004 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5569\n",
      "18:14:58.004 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5913)\n",
      "18:14:58.004 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5913\n",
      "18:14:58.004 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5913\n",
      "18:14:58.004 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5311)\n",
      "18:14:58.004 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5311\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5311\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5811)\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5811\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5811\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5516)\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5516\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5516\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5472)\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5472\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5472\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5590)\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5590\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5590\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5541)\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5541\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5541\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5331)\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5331\n",
      "18:14:58.005 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5331\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5279)\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5279\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5279\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5760)\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5760\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5760\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5312)\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5312\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5312\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5411)\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5411\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5411\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5758)\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5758\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5758\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5779)\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5779\n",
      "18:14:58.006 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5779\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5394)\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5394\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5394\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5929)\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5929\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5929\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5856)\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5856\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5856\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5674)\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5674\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5674\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5686)\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5686\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5686\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5790)\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5790\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5790\n",
      "18:14:58.007 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5570)\n",
      "18:14:58.008 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5570\n",
      "18:14:58.008 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5570\n",
      "18:14:58.008 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5704)\n",
      "18:14:58.008 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5704\n",
      "18:14:58.008 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5704\n",
      "18:14:58.008 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5558)\n",
      "18:14:58.008 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5558\n",
      "18:14:58.008 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5558\n",
      "18:14:58.008 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5853)\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5853\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5853\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5755)\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5755\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5755\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5449)\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5449\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5449\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5433)\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5433\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5433\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5822)\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5822\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5822\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5284)\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5284\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5284\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5675)\n",
      "18:14:58.009 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5675\n",
      "18:14:58.010 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5675\n",
      "18:14:58.010 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5846)\n",
      "18:14:58.010 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5846\n",
      "18:14:58.010 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5846\n",
      "18:14:58.010 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5447)\n",
      "18:14:58.010 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5447\n",
      "18:14:58.010 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5447\n",
      "18:14:58.010 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanBroadcast(97)\n",
      "18:14:58.010 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning broadcast 97\n",
      "18:14:58.010 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast -- Unpersisting TorrentBroadcast 97\n",
      "18:14:58.011 [block-manager-storage-async-thread-pool-387] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- removing broadcast 97\n",
      "18:14:58.011 [block-manager-storage-async-thread-pool-387] DEBUG org.apache.spark.storage.BlockManager -- Removing broadcast 97\n",
      "18:14:58.011 [block-manager-storage-async-thread-pool-387] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_97\n",
      "18:14:58.011 [block-manager-storage-async-thread-pool-387] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_97 of size 12936 dropped from memory (free 455287407)\n",
      "18:14:58.011 [block-manager-storage-async-thread-pool-387] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_97_piece0\n",
      "18:14:58.011 [block-manager-storage-async-thread-pool-387] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_97_piece0 of size 6369 dropped from memory (free 455293776)\n",
      "18:14:58.011 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_97_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:58.011 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Removed broadcast_97_piece0 on 10.0.0.146:57286 in memory (size: 6.2 KiB, free: 434.4 MiB)\n",
      "18:14:58.011 [block-manager-storage-async-thread-pool-387] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_97_piece0\n",
      "18:14:58.011 [block-manager-storage-async-thread-pool-387] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_97_piece0\n",
      "18:14:58.011 [block-manager-storage-async-thread-pool-389] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Done removing broadcast 97, response is 0\n",
      "18:14:58.011 [block-manager-storage-async-thread-pool-389] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Sent response: 0 to 10.0.0.146:57282\n",
      "18:14:58.011 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned broadcast 97\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5685)\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5685\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5685\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5408)\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5408\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5408\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5410)\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5410\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5410\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5298)\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5298\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5298\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5773)\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5773\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5773\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5640)\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5640\n",
      "18:14:58.012 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5640\n",
      "18:14:58.013 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5304)\n",
      "18:14:58.013 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5304\n",
      "18:14:58.013 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5304\n",
      "18:14:58.013 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5382)\n",
      "18:14:58.013 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5382\n",
      "18:14:58.013 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5382\n",
      "18:14:58.013 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5488)\n",
      "18:14:58.013 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5488\n",
      "18:14:58.013 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5488\n",
      "18:14:58.013 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5277)\n",
      "18:14:58.013 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5277\n",
      "18:14:58.014 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5277\n",
      "18:14:58.014 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5622)\n",
      "18:14:58.014 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5622\n",
      "18:14:58.014 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5622\n",
      "18:14:58.014 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5468)\n",
      "18:14:58.014 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5468\n",
      "18:14:58.014 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5468\n",
      "18:14:58.014 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5626)\n",
      "18:14:58.014 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5626\n",
      "18:14:58.014 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5626\n",
      "18:14:58.014 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5799)\n",
      "18:14:58.014 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5799\n",
      "18:14:58.014 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5799\n",
      "18:14:58.014 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5462)\n",
      "18:14:58.014 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5462\n",
      "18:14:58.014 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5462\n",
      "18:14:58.014 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5561)\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5561\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5561\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5579)\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5579\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5579\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5501)\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5501\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5501\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5788)\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5788\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5788\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5800)\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5800\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5800\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5425)\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5425\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5425\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5636)\n",
      "18:14:58.015 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5636\n",
      "18:14:58.017 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5636\n",
      "18:14:58.017 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5629)\n",
      "18:14:58.017 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5629\n",
      "18:14:58.017 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5629\n",
      "18:14:58.017 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5689)\n",
      "18:14:58.017 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5689\n",
      "18:14:58.017 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5689\n",
      "18:14:58.017 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5330)\n",
      "18:14:58.017 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5330\n",
      "18:14:58.018 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5330\n",
      "18:14:58.018 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanBroadcast(101)\n",
      "18:14:58.018 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning broadcast 101\n",
      "18:14:58.018 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast -- Unpersisting TorrentBroadcast 101\n",
      "18:14:58.018 [block-manager-storage-async-thread-pool-390] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- removing broadcast 101\n",
      "18:14:58.018 [block-manager-storage-async-thread-pool-390] DEBUG org.apache.spark.storage.BlockManager -- Removing broadcast 101\n",
      "18:14:58.018 [block-manager-storage-async-thread-pool-390] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_101\n",
      "18:14:58.018 [block-manager-storage-async-thread-pool-390] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_101 of size 39256 dropped from memory (free 455333032)\n",
      "18:14:58.018 [block-manager-storage-async-thread-pool-390] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_101_piece0\n",
      "18:14:58.018 [block-manager-storage-async-thread-pool-390] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_101_piece0 of size 17358 dropped from memory (free 455350390)\n",
      "18:14:58.018 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_101_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:58.018 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Removed broadcast_101_piece0 on 10.0.0.146:57286 in memory (size: 17.0 KiB, free: 434.4 MiB)\n",
      "18:14:58.018 [block-manager-storage-async-thread-pool-390] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_101_piece0\n",
      "18:14:58.018 [block-manager-storage-async-thread-pool-390] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_101_piece0\n",
      "18:14:58.019 [block-manager-storage-async-thread-pool-392] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Done removing broadcast 101, response is 0\n",
      "18:14:58.019 [block-manager-storage-async-thread-pool-392] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Sent response: 0 to 10.0.0.146:57282\n",
      "18:14:58.019 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned broadcast 101\n",
      "18:14:58.019 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5329)\n",
      "18:14:58.019 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5329\n",
      "18:14:58.019 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5329\n",
      "18:14:58.019 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5691)\n",
      "18:14:58.019 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5691\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5691\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5616)\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5616\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5616\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5641)\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5641\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5641\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5434)\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5434\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5434\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5679)\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5679\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5679\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5306)\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5306\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5306\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5522)\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5522\n",
      "18:14:58.020 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5522\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5827)\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5827\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5827\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5320)\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5320\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5320\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5695)\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5695\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5695\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5599)\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5599\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5599\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5554)\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5554\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5554\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5467)\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5467\n",
      "18:14:58.021 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5467\n",
      "18:14:58.022 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5296)\n",
      "18:14:58.022 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5296\n",
      "18:14:58.022 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5296\n",
      "18:14:58.022 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5480)\n",
      "18:14:58.022 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5480\n",
      "18:14:58.022 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5480\n",
      "18:14:58.022 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5463)\n",
      "18:14:58.022 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5463\n",
      "18:14:58.022 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5463\n",
      "18:14:58.022 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5575)\n",
      "18:14:58.023 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5575\n",
      "18:14:58.023 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5575\n",
      "18:14:58.024 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5633)\n",
      "18:14:58.024 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5633\n",
      "18:14:58.024 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5633\n",
      "18:14:58.024 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5770)\n",
      "18:14:58.024 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5770\n",
      "18:14:58.024 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5770\n",
      "18:14:58.024 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5323)\n",
      "18:14:58.024 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5323\n",
      "18:14:58.024 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5323\n",
      "18:14:58.024 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5446)\n",
      "18:14:58.024 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5446\n",
      "18:14:58.024 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5446\n",
      "18:14:58.024 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5414)\n",
      "18:14:58.024 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5414\n",
      "18:14:58.024 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5414\n",
      "18:14:58.024 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5299)\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5299\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5299\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5705)\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5705\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5705\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5914)\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5914\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5914\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5417)\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5417\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5417\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5625)\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5625\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5625\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5794)\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5794\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5794\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5927)\n",
      "18:14:58.025 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5927\n",
      "18:14:58.026 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5927\n",
      "18:14:58.026 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5547)\n",
      "18:14:58.026 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5547\n",
      "18:14:58.026 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5547\n",
      "18:14:58.026 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5667)\n",
      "18:14:58.026 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5667\n",
      "18:14:58.026 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5667\n",
      "18:14:58.026 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5397)\n",
      "18:14:58.026 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5397\n",
      "18:14:58.026 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5397\n",
      "18:14:58.039 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5848)\n",
      "18:14:58.039 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5848\n",
      "18:14:58.039 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5848\n",
      "18:14:58.039 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5793)\n",
      "18:14:58.039 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5793\n",
      "18:14:58.039 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5793\n",
      "18:14:58.039 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5709)\n",
      "18:14:58.039 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5709\n",
      "18:14:58.039 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5709\n",
      "18:14:58.041 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5652)\n",
      "18:14:58.041 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5652\n",
      "18:14:58.041 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5652\n",
      "18:14:58.041 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5416)\n",
      "18:14:58.041 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5416\n",
      "18:14:58.041 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5416\n",
      "18:14:58.041 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5336)\n",
      "18:14:58.041 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5336\n",
      "18:14:58.041 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5336\n",
      "18:14:58.045 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5422)\n",
      "18:14:58.045 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5422\n",
      "18:14:58.045 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5422\n",
      "18:14:58.045 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5797)\n",
      "18:14:58.045 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5797\n",
      "18:14:58.045 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5797\n",
      "18:14:58.045 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5461)\n",
      "18:14:58.045 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5461\n",
      "18:14:58.045 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5461\n",
      "18:14:58.045 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5937)\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5937\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5937\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5295)\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5295\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5295\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5278)\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5278\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5278\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5432)\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5432\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5432\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5698)\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5698\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5698\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5938)\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5938\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5938\n",
      "18:14:58.049 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5719)\n",
      "18:14:58.050 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5719\n",
      "18:14:58.050 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5719\n",
      "18:14:58.050 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5413)\n",
      "18:14:58.050 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5413\n",
      "18:14:58.050 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5413\n",
      "18:14:58.050 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5543)\n",
      "18:14:58.050 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5543\n",
      "18:14:58.050 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5543\n",
      "18:14:58.050 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5293)\n",
      "18:14:58.050 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5293\n",
      "18:14:58.051 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5293\n",
      "18:14:58.051 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5915)\n",
      "18:14:58.051 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5915\n",
      "18:14:58.051 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5915\n",
      "18:14:58.051 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5495)\n",
      "18:14:58.051 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5495\n",
      "18:14:58.051 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5495\n",
      "18:14:58.051 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5439)\n",
      "18:14:58.051 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5439\n",
      "18:14:58.051 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5439\n",
      "18:14:58.051 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5443)\n",
      "18:14:58.051 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5443\n",
      "18:14:58.051 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5443\n",
      "18:14:58.051 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanBroadcast(95)\n",
      "18:14:58.051 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning broadcast 95\n",
      "18:14:58.051 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast -- Unpersisting TorrentBroadcast 95\n",
      "18:14:58.052 [block-manager-storage-async-thread-pool-393] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- removing broadcast 95\n",
      "18:14:58.052 [block-manager-storage-async-thread-pool-393] DEBUG org.apache.spark.storage.BlockManager -- Removing broadcast 95\n",
      "18:14:58.052 [block-manager-storage-async-thread-pool-393] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_95\n",
      "18:14:58.052 [block-manager-storage-async-thread-pool-393] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_95 of size 32792 dropped from memory (free 455383182)\n",
      "18:14:58.052 [block-manager-storage-async-thread-pool-393] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_95_piece0\n",
      "18:14:58.052 [block-manager-storage-async-thread-pool-393] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_95_piece0 of size 3862 dropped from memory (free 455387044)\n",
      "18:14:58.052 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_95_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:58.052 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Removed broadcast_95_piece0 on 10.0.0.146:57286 in memory (size: 3.8 KiB, free: 434.4 MiB)\n",
      "18:14:58.052 [block-manager-storage-async-thread-pool-393] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_95_piece0\n",
      "18:14:58.053 [block-manager-storage-async-thread-pool-393] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_95_piece0\n",
      "18:14:58.053 [block-manager-storage-async-thread-pool-395] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Done removing broadcast 95, response is 0\n",
      "18:14:58.053 [block-manager-storage-async-thread-pool-395] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Sent response: 0 to 10.0.0.146:57282\n",
      "18:14:58.053 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned broadcast 95\n",
      "18:14:58.053 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5586)\n",
      "18:14:58.053 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5586\n",
      "18:14:58.053 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5586\n",
      "18:14:58.053 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5500)\n",
      "18:14:58.053 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5500\n",
      "18:14:58.053 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5500\n",
      "18:14:58.053 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5573)\n",
      "18:14:58.053 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5573\n",
      "18:14:58.053 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5573\n",
      "18:14:58.053 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5378)\n",
      "18:14:58.053 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5378\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5378\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5906)\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5906\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5906\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5862)\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5862\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5862\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5791)\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5791\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5791\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5637)\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5637\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5637\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5687)\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5687\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5687\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5774)\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5774\n",
      "18:14:58.054 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5774\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5441)\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5441\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5441\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5654)\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5654\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5654\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5645)\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5645\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5645\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5724)\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5724\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5724\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5699)\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5699\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5699\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5651)\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5651\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5651\n",
      "18:14:58.055 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5442)\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5442\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5442\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5508)\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5508\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5508\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5910)\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5910\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5910\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5424)\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5424\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5424\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5837)\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5837\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5837\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5515)\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5515\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5515\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5602)\n",
      "18:14:58.056 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5602\n",
      "18:14:58.057 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5602\n",
      "18:14:58.057 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5486)\n",
      "18:14:58.057 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5486\n",
      "18:14:58.057 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5486\n",
      "18:14:58.057 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5578)\n",
      "18:14:58.057 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5578\n",
      "18:14:58.057 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5578\n",
      "18:14:58.057 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5669)\n",
      "18:14:58.057 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5669\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5669\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5829)\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5829\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5829\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5568)\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5568\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5568\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5706)\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5706\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5706\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5644)\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5644\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5644\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5911)\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5911\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5911\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5803)\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5803\n",
      "18:14:58.058 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5803\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5450)\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5450\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5450\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5759)\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5759\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5759\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5396)\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5396\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5396\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5580)\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5580\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5580\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5280)\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5280\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5280\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5673)\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5673\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5673\n",
      "18:14:58.059 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5437)\n",
      "18:14:58.060 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5437\n",
      "18:14:58.060 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5437\n",
      "18:14:58.060 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5653)\n",
      "18:14:58.060 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5653\n",
      "18:14:58.060 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5653\n",
      "18:14:58.060 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5776)\n",
      "18:14:58.060 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5776\n",
      "18:14:58.060 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5776\n",
      "18:14:58.060 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5385)\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5385\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5385\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5285)\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5285\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5285\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5668)\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5668\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5668\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5717)\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5717\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5717\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5600)\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5600\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5600\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5939)\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5939\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5939\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5592)\n",
      "18:14:58.061 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5592\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5592\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5769)\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5769\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5769\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5302)\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5302\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5302\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5860)\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5860\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5860\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5451)\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5451\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5451\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5412)\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5412\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5412\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5286)\n",
      "18:14:58.062 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5286\n",
      "18:14:58.063 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5286\n",
      "18:14:58.063 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5398)\n",
      "18:14:58.063 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5398\n",
      "18:14:58.063 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5398\n",
      "18:14:58.063 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5281)\n",
      "18:14:58.063 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5281\n",
      "18:14:58.063 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5281\n",
      "18:14:58.063 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5921)\n",
      "18:14:58.063 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5921\n",
      "18:14:58.063 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5921\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5931)\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5931\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5931\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5392)\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5392\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5392\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5477)\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5477\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5477\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5504)\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5504\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5504\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5605)\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5605\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5605\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5643)\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5643\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5643\n",
      "18:14:58.064 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5405)\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5405\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5405\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5728)\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5728\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5728\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5740)\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5740\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5740\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5452)\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5452\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5452\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5733)\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5733\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5733\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5619)\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5619\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5619\n",
      "18:14:58.065 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5557)\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5557\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5557\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5406)\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5406\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5406\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5314)\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5314\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5314\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5843)\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5843\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5843\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5703)\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5703\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5703\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5820)\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5820\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5820\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5618)\n",
      "18:14:58.066 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5618\n",
      "18:14:58.067 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5618\n",
      "18:14:58.067 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5722)\n",
      "18:14:58.067 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5722\n",
      "18:14:58.067 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5722\n",
      "18:14:58.067 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5834)\n",
      "18:14:58.067 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5834\n",
      "18:14:58.067 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5834\n",
      "18:14:58.067 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5809)\n",
      "18:14:58.067 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5809\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5809\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5481)\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5481\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5481\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5485)\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5485\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5485\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5283)\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5283\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5283\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5617)\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5617\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5617\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5489)\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5489\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5489\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5555)\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5555\n",
      "18:14:58.068 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5555\n",
      "18:14:58.069 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5761)\n",
      "18:14:58.069 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5761\n",
      "18:14:58.069 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5761\n",
      "18:14:58.069 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5585)\n",
      "18:14:58.069 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5585\n",
      "18:14:58.069 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5585\n",
      "18:14:58.069 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5749)\n",
      "18:14:58.069 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5749\n",
      "18:14:58.069 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5749\n",
      "18:14:58.069 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5621)\n",
      "18:14:58.070 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5621\n",
      "18:14:58.070 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5621\n",
      "18:14:58.070 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5926)\n",
      "18:14:58.070 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5926\n",
      "18:14:58.070 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5926\n",
      "18:14:58.070 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5729)\n",
      "18:14:58.070 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5729\n",
      "18:14:58.070 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5729\n",
      "18:14:58.070 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5658)\n",
      "18:14:58.071 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5658\n",
      "18:14:58.071 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5658\n",
      "18:14:58.071 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanBroadcast(93)\n",
      "18:14:58.071 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning broadcast 93\n",
      "18:14:58.071 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast -- Unpersisting TorrentBroadcast 93\n",
      "18:14:58.071 [block-manager-storage-async-thread-pool-396] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- removing broadcast 93\n",
      "18:14:58.071 [block-manager-storage-async-thread-pool-396] DEBUG org.apache.spark.storage.BlockManager -- Removing broadcast 93\n",
      "18:14:58.071 [block-manager-storage-async-thread-pool-396] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_93_piece0\n",
      "18:14:58.072 [block-manager-storage-async-thread-pool-396] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_93_piece0 of size 6508 dropped from memory (free 455393552)\n",
      "18:14:58.072 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_93_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:58.072 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Removed broadcast_93_piece0 on 10.0.0.146:57286 in memory (size: 6.4 KiB, free: 434.4 MiB)\n",
      "18:14:58.072 [block-manager-storage-async-thread-pool-396] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_93_piece0\n",
      "18:14:58.072 [block-manager-storage-async-thread-pool-396] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_93_piece0\n",
      "18:14:58.072 [block-manager-storage-async-thread-pool-396] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_93\n",
      "18:14:58.072 [block-manager-storage-async-thread-pool-396] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_93 of size 14024 dropped from memory (free 455407576)\n",
      "18:14:58.073 [block-manager-storage-async-thread-pool-398] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Done removing broadcast 93, response is 0\n",
      "18:14:58.073 [block-manager-storage-async-thread-pool-398] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Sent response: 0 to 10.0.0.146:57282\n",
      "18:14:58.073 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned broadcast 93\n",
      "18:14:58.073 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5454)\n",
      "18:14:58.073 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5454\n",
      "18:14:58.073 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5454\n",
      "18:14:58.073 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5810)\n",
      "18:14:58.073 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5810\n",
      "18:14:58.073 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5810\n",
      "18:14:58.073 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5684)\n",
      "18:14:58.073 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5684\n",
      "18:14:58.073 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5684\n",
      "18:14:58.073 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5655)\n",
      "18:14:58.073 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5655\n",
      "18:14:58.073 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5655\n",
      "18:14:58.073 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5587)\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5587\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5587\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5934)\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5934\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5934\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5458)\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5458\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5458\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5819)\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5819\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5819\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5459)\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5459\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5459\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5395)\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5395\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5395\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5440)\n",
      "18:14:58.074 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5440\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5440\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5778)\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5778\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5778\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5838)\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5838\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5838\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5767)\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5767\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5767\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5647)\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5647\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5647\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5612)\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5612\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5612\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5473)\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5473\n",
      "18:14:58.075 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5473\n",
      "18:14:58.076 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5907)\n",
      "18:14:58.076 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5907\n",
      "18:14:58.076 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5907\n",
      "18:14:58.076 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5303)\n",
      "18:14:58.076 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5303\n",
      "18:14:58.076 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5303\n",
      "18:14:58.076 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5502)\n",
      "18:14:58.076 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5502\n",
      "18:14:58.076 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5502\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5494)\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5494\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5494\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5294)\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5294\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5294\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5732)\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5732\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5732\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5399)\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5399\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5399\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5748)\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5748\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5748\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5487)\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5487\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5487\n",
      "18:14:58.077 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5789)\n",
      "18:14:58.091 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5789\n",
      "18:14:58.091 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5789\n",
      "18:14:58.091 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5457)\n",
      "18:14:58.091 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5457\n",
      "18:14:58.091 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5457\n",
      "18:14:58.091 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5609)\n",
      "18:14:58.091 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5609\n",
      "18:14:58.091 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5609\n",
      "18:14:58.091 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5290)\n",
      "18:14:58.094 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5290\n",
      "18:14:58.094 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5290\n",
      "18:14:58.095 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5471)\n",
      "18:14:58.095 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5471\n",
      "18:14:58.095 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5471\n",
      "18:14:58.095 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5627)\n",
      "18:14:58.095 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5627\n",
      "18:14:58.095 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5627\n",
      "18:14:58.095 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5528)\n",
      "18:14:58.095 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5528\n",
      "18:14:58.097 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5528\n",
      "18:14:58.097 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5764)\n",
      "18:14:58.097 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5764\n",
      "18:14:58.097 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5764\n",
      "18:14:58.097 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5551)\n",
      "18:14:58.097 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5551\n",
      "18:14:58.097 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5551\n",
      "18:14:58.097 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanBroadcast(98)\n",
      "18:14:58.097 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning broadcast 98\n",
      "18:14:58.099 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast -- Unpersisting TorrentBroadcast 98\n",
      "18:14:58.099 [block-manager-storage-async-thread-pool-399] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- removing broadcast 98\n",
      "18:14:58.099 [block-manager-storage-async-thread-pool-399] DEBUG org.apache.spark.storage.BlockManager -- Removing broadcast 98\n",
      "18:14:58.099 [block-manager-storage-async-thread-pool-399] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_98_piece0\n",
      "18:14:58.100 [block-manager-storage-async-thread-pool-399] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_98_piece0 of size 6506 dropped from memory (free 455414082)\n",
      "18:14:58.100 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_98_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:14:58.100 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Removed broadcast_98_piece0 on 10.0.0.146:57286 in memory (size: 6.4 KiB, free: 434.4 MiB)\n",
      "18:14:58.100 [block-manager-storage-async-thread-pool-399] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_98_piece0\n",
      "18:14:58.100 [block-manager-storage-async-thread-pool-399] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_98_piece0\n",
      "18:14:58.100 [block-manager-storage-async-thread-pool-399] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_98\n",
      "18:14:58.100 [block-manager-storage-async-thread-pool-399] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_98 of size 14024 dropped from memory (free 455428106)\n",
      "18:14:58.100 [block-manager-storage-async-thread-pool-401] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Done removing broadcast 98, response is 0\n",
      "18:14:58.100 [block-manager-storage-async-thread-pool-401] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Sent response: 0 to 10.0.0.146:57282\n",
      "18:14:58.101 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned broadcast 98\n",
      "18:14:58.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5918)\n",
      "18:14:58.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5918\n",
      "18:14:58.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5918\n",
      "18:14:58.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5598)\n",
      "18:14:58.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5598\n",
      "18:14:58.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5598\n",
      "18:14:58.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5402)\n",
      "18:14:58.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5402\n",
      "18:14:58.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5402\n",
      "18:14:58.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5518)\n",
      "18:14:58.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5518\n",
      "18:14:58.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5518\n",
      "18:14:58.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5723)\n",
      "18:14:58.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5723\n",
      "18:14:58.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5723\n",
      "18:14:58.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5506)\n",
      "18:14:58.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5506\n",
      "18:14:58.106 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5506\n",
      "18:14:58.106 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5649)\n",
      "18:14:58.106 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5649\n",
      "18:14:58.106 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5649\n",
      "18:14:58.106 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5731)\n",
      "18:14:58.106 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5731\n",
      "18:14:58.106 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5731\n",
      "18:14:58.106 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5282)\n",
      "18:14:58.106 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5282\n",
      "18:14:58.106 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5282\n",
      "18:14:58.111 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5857)\n",
      "18:14:58.111 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5857\n",
      "18:14:58.111 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5857\n",
      "18:14:58.111 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5713)\n",
      "18:14:58.111 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5713\n",
      "18:14:58.111 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5713\n",
      "18:14:58.111 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5275)\n",
      "18:14:58.111 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5275\n",
      "18:14:58.111 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5275\n",
      "18:15:05.965 [executor-heartbeater] DEBUG org.apache.spark.executor.ExecutorMetricsPoller -- removing (95, 0) from stageTCMP\n",
      "18:15:29.462 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5883)\n",
      "18:15:29.462 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5883\n",
      "18:15:29.462 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5883\n",
      "18:15:29.462 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5871)\n",
      "18:15:29.462 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5871\n",
      "18:15:29.462 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5871\n",
      "18:15:29.462 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5865)\n",
      "18:15:29.462 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5865\n",
      "18:15:29.462 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5865\n",
      "18:15:29.462 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanBroadcast(103)\n",
      "18:15:29.462 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning broadcast 103\n",
      "18:15:29.462 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast -- Unpersisting TorrentBroadcast 103\n",
      "18:15:29.463 [block-manager-storage-async-thread-pool-402] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- removing broadcast 103\n",
      "18:15:29.463 [block-manager-storage-async-thread-pool-402] DEBUG org.apache.spark.storage.BlockManager -- Removing broadcast 103\n",
      "18:15:29.464 [block-manager-storage-async-thread-pool-402] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_103_piece0\n",
      "18:15:29.464 [block-manager-storage-async-thread-pool-402] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_103_piece0 of size 3862 dropped from memory (free 455431968)\n",
      "18:15:29.464 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_103_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:15:29.465 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Removed broadcast_103_piece0 on 10.0.0.146:57286 in memory (size: 3.8 KiB, free: 434.4 MiB)\n",
      "18:15:29.466 [block-manager-storage-async-thread-pool-402] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_103_piece0\n",
      "18:15:29.466 [block-manager-storage-async-thread-pool-402] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_103_piece0\n",
      "18:15:29.466 [block-manager-storage-async-thread-pool-402] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_103\n",
      "18:15:29.466 [block-manager-storage-async-thread-pool-402] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_103 of size 32792 dropped from memory (free 455464760)\n",
      "18:15:29.466 [block-manager-storage-async-thread-pool-404] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Done removing broadcast 103, response is 0\n",
      "18:15:29.467 [block-manager-storage-async-thread-pool-404] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Sent response: 0 to 10.0.0.146:57282\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned broadcast 103\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5888)\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5888\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5888\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5894)\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5894\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5894\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5903)\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5903\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5903\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5890)\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5890\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5890\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5872)\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5872\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5872\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5886)\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5886\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5886\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5866)\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5866\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5866\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5891)\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5891\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5891\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5878)\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5878\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5878\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5874)\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5874\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5874\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5864)\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5864\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5864\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5889)\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5889\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5889\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5896)\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5896\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5896\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5904)\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5904\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5904\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanBroadcast(104)\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning broadcast 104\n",
      "18:15:29.467 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast -- Unpersisting TorrentBroadcast 104\n",
      "18:15:29.468 [block-manager-storage-async-thread-pool-405] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- removing broadcast 104\n",
      "18:15:29.468 [block-manager-storage-async-thread-pool-405] DEBUG org.apache.spark.storage.BlockManager -- Removing broadcast 104\n",
      "18:15:29.468 [block-manager-storage-async-thread-pool-405] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_104\n",
      "18:15:29.468 [block-manager-storage-async-thread-pool-405] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_104 of size 32792 dropped from memory (free 455497552)\n",
      "18:15:29.468 [block-manager-storage-async-thread-pool-405] DEBUG org.apache.spark.storage.BlockManager -- Removing block broadcast_104_piece0\n",
      "18:15:29.468 [block-manager-storage-async-thread-pool-405] DEBUG org.apache.spark.storage.memory.MemoryStore -- Block broadcast_104_piece0 of size 3862 dropped from memory (free 455501414)\n",
      "18:15:29.468 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint -- Updating block info on master broadcast_104_piece0 for BlockManagerId(driver, 10.0.0.146, 57286, None)\n",
      "18:15:29.468 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo -- Removed broadcast_104_piece0 on 10.0.0.146:57286 in memory (size: 3.8 KiB, free: 434.4 MiB)\n",
      "18:15:29.468 [block-manager-storage-async-thread-pool-405] DEBUG org.apache.spark.storage.BlockManagerMaster -- Updated info of block broadcast_104_piece0\n",
      "18:15:29.468 [block-manager-storage-async-thread-pool-405] DEBUG org.apache.spark.storage.BlockManager -- Told master about block broadcast_104_piece0\n",
      "18:15:29.469 [block-manager-storage-async-thread-pool-407] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Done removing broadcast 104, response is 0\n",
      "18:15:29.469 [block-manager-storage-async-thread-pool-407] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint -- Sent response: 0 to 10.0.0.146:57282\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned broadcast 104\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5892)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5892\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5892\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5900)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5900\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5900\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5873)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5873\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5873\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5867)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5867\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5867\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5902)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5902\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5902\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5877)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5877\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5877\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5885)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5885\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5885\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5895)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5895\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5895\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5887)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5887\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5887\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5876)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5876\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5876\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5880)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5880\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5880\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5870)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5870\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5870\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5884)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5884\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5884\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5901)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5901\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5901\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5897)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5897\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5897\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5882)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5882\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5882\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5875)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5875\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5875\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5893)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5893\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5893\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5899)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5899\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5899\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5898)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5898\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5898\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5863)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5863\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5863\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5881)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5881\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5881\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5879)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5879\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5879\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5868)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5868\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5868\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Got cleaning task CleanAccum(5869)\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaning accumulator 5869\n",
      "18:15:29.470 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner -- Cleaned accumulator 5869\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Suppress all logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# If you want to be specific, you can configure the py4j logger as well\n",
    "logging.getLogger('py4j').setLevel(logging.ERROR)\n",
    "devices = spark.sql(\"SELECT * FROM ncentral.devices\")\n",
    "devices.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82b552c-c870-4fa1-8b81-a3e25efe7654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
